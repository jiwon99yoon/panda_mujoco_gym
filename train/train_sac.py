#/home/dyros/panda_mujoco_gym/train/train_sac.py
#!/usr/bin/env python3
"""
SAC í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- ìˆœìˆ˜í•˜ê²Œ í•™ìŠµì—ë§Œ ì§‘ì¤‘
- ê²°ê³¼ëŠ” outputs í´ë”ì— ì €ì¥
- ë¹„ë””ì˜¤ ë…¹í™”ëŠ” ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ìˆ˜í–‰
"""

import os
import sys
import time
import argparse
import numpy as np
import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.logger import configure
from stable_baselines3.common.noise import NormalActionNoise

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.append(project_root)

# ì‚¬ìš©ì ì •ì˜ í™˜ê²½ ë“±ë¡
import panda_mujoco_gym

# Common ëª¨ë“ˆ ì„í¬íŠ¸
from train.common.config   import SACConfig
from train.common.wrappers import RewardScalingWrapper, SuccessTrackingWrapper
from train.common.callbacks import TrainingCallback
#from common import SACConfig, RewardScalingWrapper, SuccessTrackingWrapper, TrainingCallback

# í•œ ì—í”¼ì†Œë“œ ë‹¹ stepì„ ê¸°ë³¸ 50 -> 1,000ìœ¼ë¡œ wrappingí•˜ê¸° ìœ„í•´
from gymnasium.wrappers import TimeLimit

def create_env(env_name, render_mode=None, reward_scale=1.0):
    """í™˜ê²½ ìƒì„± (ë˜í¼ ì ìš©)"""
    raw = gym.make(env_name, render_mode=render_mode)
    env = TimeLimit(raw, max_episode_steps = 1000) #episodeì˜ timestep 1000ìœ¼ë¡œ í• ë‹¹
    env = Monitor(env)
    
    # ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ì ìš©
    if reward_scale != 1.0:
        env = RewardScalingWrapper(env, scale=reward_scale)
    
    # ì„±ê³µë¥  ì¶”ì  ì¶”ê°€
    env = SuccessTrackingWrapper(env)
    
    return env


# ê° ì›Œì»¤ì— ì„œë¡œ ë‹¤ë¥¸ ì‹œë“œë¥¼ í• ë‹¹
def create_vec_env(env_name, n_envs=1, normalize=True, reward_scale=1.0, seed=None):
    def make_env(rank):
        def _init():
            env = create_env(env_name, reward_scale=reward_scale)

            # ìˆ˜ì •: seed ì„¤ì • ì œê±° (ë§¤ë²ˆ ëœë¤í•˜ê²Œ)
            # seedê°€ ìˆìœ¼ë©´ action/observation spaceì˜ ìƒ˜í”Œë§ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©
            if seed is not None:
                env.action_space.seed(seed + rank)
                env.observation_space.seed(seed + rank)
            return env
        return _init

    # n_envsê°€ 1ì´ë©´ ë””ë²„ê¹… ìš©ì´í•œ DummyVecEnv ì‚¬ìš©
    if n_envs == 1:
        vec_env = DummyVecEnv([make_env(0)])
    else:
        env_fns = [make_env(i) for i in range(n_envs)]
        vec_env = SubprocVecEnv(env_fns, start_method='fork')    
    
    if normalize:
        vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)
 
    return vec_env
# def create_vec_env(env_name, n_envs=1, normalize=True, reward_scale=1.0):
#     """ë²¡í„°í™”ëœ í™˜ê²½ ìƒì„±"""
#     def make_env():
#         return create_env(env_name, reward_scale=reward_scale)
    
#     #vec_env = DummyVecEnv([make_env for _ in range(n_envs)])
#     # SubprocVecEnv: ì„œë¡œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë³‘ë ¬ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
#     vec_env = SubprocVecEnv([make_env for _ in range(n_envs)])
    
#     if normalize:
#         vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)
    
#     return vec_env


def create_sac_model(env, config):
    """SAC ëª¨ë¸ ìƒì„±"""
    n_actions = env.action_space.shape[-1]
    
    action_noise = NormalActionNoise(
        mean=np.zeros(n_actions), 
        sigma=config.action_noise_std * np.ones(n_actions)
    )
    
    model = SAC(
        policy="MultiInputPolicy",
        env=env,
        learning_rate=config.learning_rate,
        buffer_size=config.buffer_size,
        learning_starts=config.learning_starts,
        batch_size=config.batch_size,
        tau=config.tau,
        gamma=config.gamma,
        train_freq=config.train_freq,
        gradient_steps=config.gradient_steps,
        action_noise=action_noise,
        policy_kwargs=config.policy_kwargs,
        use_sde=config.use_sde,
        sde_sample_freq=config.sde_sample_freq,
        verbose=1,
        tensorboard_log=config.log_dir,
        device="auto"
    )
    
    return model


def train_sac(config: SACConfig):
    """SAC í•™ìŠµ ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ SAC í•™ìŠµ ì‹œì‘!")
    print(f"ğŸ¯ í™˜ê²½: {config.env_name}")
    print(f"ğŸ“Š ì´ í•™ìŠµ ìŠ¤í…: {config.total_timesteps:,}")
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {config.exp_dir}")
    print("-" * 60)
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    config.create_directories()
    
    # í™˜ê²½ ìƒì„±
    print("ğŸ—ï¸  í™˜ê²½ ìƒì„± ì¤‘...")
    env = create_vec_env(
        config.env_name,
        n_envs=config.n_envs, 
        normalize=config.normalize_env, 
        reward_scale=config.reward_scale,
        seed=config.seed
    )
    eval_env = create_vec_env(
        config.env_name,
        n_envs=1,  #ë‹¨ì¼í™˜ê²½ì—ì„œ í‰ê°€í•˜ê¸°
        normalize=config.normalize_env,
        reward_scale=config.reward_scale,
        seed=config.seed
    )
    
    # ë¡œê±° ì„¤ì •
    logger_path = os.path.join(config.log_dir, "tensorboard")
    new_logger = configure(logger_path, ["stdout", "csv", "tensorboard"])
    
    # ëª¨ë¸ ìƒì„±
    print(f"\nğŸ§  SAC ëª¨ë¸ ì´ˆê¸°í™”...")
    model = create_sac_model(env, config)
    model.set_logger(new_logger)
    
    # ì½œë°± ì„¤ì •
    callbacks = []
    
    # í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì½œë°±
    training_callback = TrainingCallback(config)
    callbacks.append(training_callback)
    
    # í‰ê°€ ì½œë°±
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=os.path.join(config.model_dir, "best_model"),
        log_path=os.path.join(config.log_dir, "eval"),
        eval_freq=config.eval_freq,
        n_eval_episodes=config.n_eval_episodes,
        deterministic=config.eval_deterministic,
        verbose=1
    )
    callbacks.append(eval_callback)
    
    # ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_callback = CheckpointCallback(
        save_freq=config.checkpoint_freq,
        save_path=config.checkpoint_dir,
        name_prefix=f"sac_{config.env_name}",
        save_replay_buffer=True,
        save_vecnormalize=True
    )
    callbacks.append(checkpoint_callback)
    
    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘!")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=config.total_timesteps,
            callback=callbacks,
            log_interval=10,
            progress_bar=True
        )
    except KeyboardInterrupt:
        print("\nâ¸ï¸  í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    end_time = time.time()
    training_time = end_time - start_time
    
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"â±ï¸  ì´ í•™ìŠµ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = os.path.join(config.model_dir, "final_model")
    model.save(final_model_path)
    env.save(os.path.join(config.model_dir, "vec_normalize.pkl"))
    print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
    
    # ìµœì¢… í‰ê°€
    print("\nğŸ” ìµœì¢… í‰ê°€...")
    mean_reward, std_reward = evaluate_policy(
        model, eval_env, 
        n_eval_episodes=50, #n_eval_episode : í‰ê°€ ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜(ê°œìˆ˜)
        deterministic=True
    )
    print(f"ğŸ† ìµœì¢… í‰ê°€ ê²°ê³¼: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    # í•™ìŠµ ìš”ì•½ ì €ì¥
    import json
    # config ì•ˆì— JSON ì§ë ¬í™” ë¶ˆê°€ ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    serializable_config = {}
    for k, v in config.__dict__.items():
        try:
            json.dumps({k: v})
            serializable_config[k] = v
        except TypeError:
            serializable_config[k] = repr(v)
    summary = {
        "experiment_name": config.experiment_name,
        "env_name": config.env_name,
        "algorithm": "SAC",
        "total_timesteps": config.total_timesteps,
        "training_time_hours": training_time / 3600,
        "final_mean_reward": float(mean_reward),
        "final_std_reward": float(std_reward),
        "best_reward": float(training_callback.best_reward),
        "final_success_rate": float(training_callback.recent_success_rate),
        "total_episodes": training_callback.episode_count,
        "config": serializable_config
    }
    
    summary_path = os.path.join(config.exp_dir, "training_summary.json")
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=4)
    
    print(f"\nğŸ“‹ í•™ìŠµ ìš”ì•½ ì €ì¥: {summary_path}")
    
    return model, env


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="SAC í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--env", type=str, default="FrankaSlideDense-v0", help="í™˜ê²½ ì´ë¦„")
    parser.add_argument("--timesteps", type=int, default=1_000_000, help="ì´ í•™ìŠµ ìŠ¤í…")
    parser.add_argument("--exp-name", type=str, default=None, help="ì‹¤í—˜ ì´ë¦„")
    parser.add_argument("--reward-scale", type=float, default=0.1, help="ë³´ìƒ ìŠ¤ì¼€ì¼")
    #main()ì— n_envs ì¸ì ì¶”ê°€
    parser.add_argument("--n-envs", type=int, default=4, help="ë³‘ë ¬ í™˜ê²½ ê°œìˆ˜")
    #seed ì¸ì ì¶”ê°€
    parser.add_argument("--seed", type=int, default=None, help="ë‚œìˆ˜ ì‹œë“œ (workerë§ˆë‹¤ seed+rank ì ìš©)")     

    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = SACConfig(
        env_name=args.env,
        total_timesteps=args.timesteps,
        experiment_name=args.exp_name,
        reward_scale=args.reward_scale,
        n_envs=args.n_envs,
        seed=args.seed,
    )
    
    # í•™ìŠµ ì‹¤í–‰
    model, env = train_sac(config)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {config.exp_dir}")
    print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   python evaluate/evaluate_with_video.py --exp-dir {config.exp_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()
