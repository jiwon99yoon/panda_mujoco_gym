#/home/minjun/panda_mujoco_gym/train_sac.py
# /home/dyros/panda_mujoco_gym/train_sac.py

#!/usr/bin/env python3
"""
ê°œì„ ëœ ì—í”¼ì†Œë“œ ê¸°ë°˜ ë¹„ë””ì˜¤ ë…¹í™” ì‹œìŠ¤í…œ
- ê° ì—í”¼ì†Œë“œë¥¼ ê°œë³„ mp4 íŒŒì¼ë¡œ ì €ì¥
- 6ë‹¨ê³„ í•™ìŠµ ì§„í–‰ë¥ ë³„ ì²´ê³„ì  ë…¹í™” (0%, 20%, 40%, 60%, 80%, 100%)
- ê° ë‹¨ê³„ë³„ 3ê°œ ì—í”¼ì†Œë“œ + ìµœê³  ë³´ìƒ í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤
- ì„±ê³µí•œ ì—í”¼ì†Œë“œ ìš°ì„  ë…¹í™”
"""

import os
import sys
import time
import numpy as np
import cv2
import json
import csv
from datetime import datetime
from typing import List, Dict, Optional
import gymnasium as gym
import torch
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback
from stable_baselines3.common.logger import configure
from stable_baselines3.common.noise import NormalActionNoise

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ì‚¬ìš©ì ì •ì˜ í™˜ê²½ ë“±ë¡
import panda_mujoco_gym

print("ğŸ¤– ê°œì„ ëœ ì—í”¼ì†Œë“œ ê¸°ë°˜ ë¹„ë””ì˜¤ ë…¹í™” ì‹œìŠ¤í…œ!")
print("=" * 60)

# ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ë˜í¼
class RewardScalingWrapper(gym.Wrapper):
    def __init__(self, env, scale=0.1):
        super().__init__(env)
        self.scale = scale
        self.episode_reward = 0
        
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        scaled_reward = reward * self.scale
        self.episode_reward += scaled_reward
        
        if terminated or truncated:
            info['original_reward'] = self.episode_reward / self.scale
            info['scaled_reward'] = self.episode_reward
            self.episode_reward = 0
            
        return obs, scaled_reward, terminated, truncated, info
    
    def reset(self, **kwargs):
        self.episode_reward = 0
        return self.env.reset(**kwargs)

# ì„±ê³µë¥  ì¶”ì  ë˜í¼
class SuccessTrackingWrapper(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)
        self.success_count = 0
        self.episode_count = 0
        
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        if terminated or truncated:
            self.episode_count += 1
            if info.get('is_success', False):
                self.success_count += 1
            
            info['success_rate'] = self.success_count / self.episode_count if self.episode_count > 0 else 0
            
        return obs, reward, terminated, truncated, info

# ì—í”¼ì†Œë“œë³„ ë¹„ë””ì˜¤ ë…¹í™” í´ë˜ìŠ¤
class EpisodeVideoRecorder:
    """
    ì—í”¼ì†Œë“œë³„ ê°œë³„ ë¹„ë””ì˜¤ ë…¹í™” ì‹œìŠ¤í…œ
    - ê° ì—í”¼ì†Œë“œ = í•˜ë‚˜ì˜ ì™„ì „í•œ mp4 íŒŒì¼
    - ì‹œì‘â†’ë ê³¼ì •ë§Œ í¬í•¨
    - ì„±ê³µ/ì‹¤íŒ¨ êµ¬ë¶„ ê°€ëŠ¥
    """
    
    def __init__(self, save_dir: str, fps: int = 30):
        self.save_dir = save_dir
        self.fps = fps
        self.current_episode_frames = []
        self.episode_metadata = []
        
        os.makedirs(save_dir, exist_ok=True)
    
    def start_episode_recording(self):
        """ìƒˆ ì—í”¼ì†Œë“œ ë…¹í™” ì‹œì‘"""
        self.current_episode_frames = []
        
    def add_frame(self, frame: np.ndarray):
        """í˜„ì¬ ì—í”¼ì†Œë“œì— í”„ë ˆì„ ì¶”ê°€"""
        if frame is not None:
            self.current_episode_frames.append(frame.copy())
    
    def end_episode_recording(self, episode_info: Dict) -> Optional[str]:
        """
        ì—í”¼ì†Œë“œ ë…¹í™” ì¢…ë£Œ ë° ë¹„ë””ì˜¤ ì €ì¥
        
        Args:
            episode_info: {
                'episode_id': int,
                'reward': float, 
                'length': int,
                'success': bool,
                'stage': str  # '0_random', '1_20percent', etc.
            }
        """
        if not self.current_episode_frames:
            return None
            
        # íŒŒì¼ëª… ìƒì„±
        stage = episode_info.get('stage', 'unknown')
        episode_id = episode_info.get('episode_id', 0)
        success_str = 'SUCCESS' if episode_info.get('success', False) else 'FAIL'
        reward = episode_info.get('reward', 0)
        
        filename = f"{stage}_ep{episode_id:03d}_{success_str}_reward{reward:.1f}.mp4"
        video_path = os.path.join(self.save_dir, filename)
        
        # ë¹„ë””ì˜¤ ì €ì¥
        success = self._save_video(video_path, self.current_episode_frames)
        
        if success:
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            episode_info['video_path'] = video_path
            episode_info['frame_count'] = len(self.current_episode_frames)
            self.episode_metadata.append(episode_info)
            
            print(f"âœ… ì—í”¼ì†Œë“œ ë¹„ë””ì˜¤ ì €ì¥: {filename}")
            print(f"   í”„ë ˆì„ ìˆ˜: {len(self.current_episode_frames)}, ì„±ê³µ: {success_str}, ë³´ìƒ: {reward:.2f}")
            
            return video_path
        
        return None
    
    def _save_video(self, video_path: str, frames: List[np.ndarray]) -> bool:
        """í”„ë ˆì„ë“¤ì„ ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥"""
        if not frames:
            return False
            
        try:
            height, width = frames[0].shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            writer = cv2.VideoWriter(video_path, fourcc, self.fps, (width, height))
            
            for frame in frames:
                # RGB â†’ BGR ë³€í™˜ (OpenCVìš©)
                if len(frame.shape) == 3 and frame.shape[2] == 3:
                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                else:
                    frame_bgr = frame
                writer.write(frame_bgr)
            
            writer.release()
            return True
            
        except Exception as e:
            print(f"âš ï¸ ë¹„ë””ì˜¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

# í•™ìŠµ ì§„í–‰ë¥ ë³„ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ
class StageBasedVideoSystem:
    """
    í•™ìŠµ ì§„í–‰ë¥ ë³„ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ
    - 6ë‹¨ê³„: 0%, 20%, 40%, 60%, 80%, 100%
    - ê° ë‹¨ê³„ë³„ 3ê°œ ì—í”¼ì†Œë“œ
    - ìµœê³  ë³´ìƒ í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤
    """
    
    def __init__(self, base_dir: str, total_timesteps: int):
        self.base_dir = base_dir
        self.total_timesteps = total_timesteps
        
        # 6ë‹¨ê³„ ì •ì˜
        self.stages = {
            '0_random': 0,                           # í•™ìŠµ ì „
            '1_20percent': total_timesteps // 5,     # 20%
            '2_40percent': total_timesteps * 2 // 5, # 40%
            '3_60percent': total_timesteps * 3 // 5, # 60% 
            '4_80percent': total_timesteps * 4 // 5, # 80%
            '5_100percent': total_timesteps          # 100%
        }
        
        self.completed_stages = set()
        self.stage_videos = {}  # stage_name -> [video_paths]
        self.best_episodes = []  # ìµœê³  ë³´ìƒ ì—í”¼ì†Œë“œë“¤
        
        # ê° ë‹¨ê³„ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        for stage_name in self.stages.keys():
            stage_dir = os.path.join(base_dir, stage_name)
            os.makedirs(stage_dir, exist_ok=True)
            self.stage_videos[stage_name] = []
    
    def should_record_stage(self, current_timestep: int) -> Optional[str]:
        """í˜„ì¬ timestepì—ì„œ ë…¹í™”í•  ë‹¨ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        for stage_name, target_step in self.stages.items():
            if (stage_name not in self.completed_stages and 
                current_timestep >= target_step):
                return stage_name
        return None
    
    def record_stage_episodes(self, stage_name: str, model, env, num_episodes: int = 3):
        """íŠ¹ì • ë‹¨ê³„ì˜ ì—í”¼ì†Œë“œë“¤ ë…¹í™”"""
        print(f"\nğŸ¬ [{stage_name}] ì—í”¼ì†Œë“œ ë…¹í™” ì‹œì‘!")
        
        stage_dir = os.path.join(self.base_dir, stage_name)
        recorder = EpisodeVideoRecorder(stage_dir)
        
        episode_results = []
        attempts = 0
        max_attempts = num_episodes * 5  # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ (ì„±ê³µ ë³´ì¥)
        
        for episode_idx in range(num_episodes):
            print(f"  ì—í”¼ì†Œë“œ {episode_idx + 1}/{num_episodes} ë…¹í™” ì¤‘...")
            
            # ì„±ê³µí•  ë•Œê¹Œì§€ ì‹œë„ (ë‹¨, ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì œí•œ)
            while attempts < max_attempts:
                attempts += 1
                result = self._record_single_episode(
                    recorder, model, env, episode_idx, stage_name
                )
                
                if result:
                    episode_results.append(result)
                    self.stage_videos[stage_name].append(result['video_path'])
                    
                    # ìµœê³  ë³´ìƒ ì¶”ì 
                    if not self.best_episodes or result['reward'] > min(ep['reward'] for ep in self.best_episodes):
                        self.best_episodes.append(result)
                        self.best_episodes.sort(key=lambda x: x['reward'], reverse=True)
                        self.best_episodes = self.best_episodes[:10]  # ìƒìœ„ 10ê°œë§Œ ìœ ì§€
                    
                    break  # ì„±ê³µí•˜ë©´ ë‹¤ìŒ ì—í”¼ì†Œë“œë¡œ
                
                # ì‹¤íŒ¨í•˜ë©´ ë‹¤ì‹œ ì‹œë„
                if attempts % 3 == 0:
                    print(f"    ì‹œë„ {attempts}: ì¬ì‹œë„ ì¤‘...")
        
        self.completed_stages.add(stage_name)
        print(f"âœ… [{stage_name}] ì™„ë£Œ! {len(episode_results)}ê°œ ì—í”¼ì†Œë“œ ë…¹í™”ë¨")
        
        return episode_results
    
    def _record_single_episode(self, recorder, model, env, episode_idx, stage_name):
        """ë‹¨ì¼ ì—í”¼ì†Œë“œ ë…¹í™”"""
        recorder.start_episode_recording()
        
        obs, _ = env.reset()
        total_reward = 0
        step_count = 0
        done = False
        
        # ì´ˆê¸° í”„ë ˆì„ ì¶”ê°€
        frame = env.render()
        recorder.add_frame(frame)
        
        while not done:
            # ì•¡ì…˜ ì˜ˆì¸¡
            if model and hasattr(model, 'predict'):
                action, _ = model.predict(obs, deterministic=True)
            else:
                action = env.action_space.sample()  # ëœë¤ ì•¡ì…˜
            
            # í™˜ê²½ ìŠ¤í…
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            total_reward += reward
            step_count += 1
            
            # í”„ë ˆì„ ì¶”ê°€
            frame = env.render()
            recorder.add_frame(frame)
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if step_count > 1000:
                break
        
        # ì—í”¼ì†Œë“œ ì •ë³´
        episode_info = {
            'episode_id': episode_idx,
            'reward': total_reward,
            'length': step_count,
            'success': info.get('is_success', False),
            'stage': stage_name
        }
        
        # ë¹„ë””ì˜¤ ì €ì¥
        video_path = recorder.end_episode_recording(episode_info)
        
        if video_path:
            episode_info['video_path'] = video_path
            return episode_info
        
        return None
    
    def create_highlight_video(self):
        """ìµœê³  ë³´ìƒ ì—í”¼ì†Œë“œë“¤ë¡œ í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ìƒì„±"""
        if not self.best_episodes:
            print("âŒ í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: ì—í”¼ì†Œë“œ ì—†ìŒ")
            return
        
        print(f"\nğŸŒŸ í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ìƒì„± ì¤‘... (ìƒìœ„ {len(self.best_episodes)}ê°œ ì—í”¼ì†Œë“œ)")
        
        highlight_dir = os.path.join(self.base_dir, "highlights")
        os.makedirs(highlight_dir, exist_ok=True)
        
        # ê° ìµœê³  ì—í”¼ì†Œë“œë¥¼ í•˜ì´ë¼ì´íŠ¸ í´ë”ì— ë³µì‚¬
        import shutil
        for i, episode in enumerate(self.best_episodes):
            original_path = episode['video_path']
            if os.path.exists(original_path):
                highlight_filename = f"highlight_{i+1:02d}_reward{episode['reward']:.1f}_{episode['stage']}.mp4"
                highlight_path = os.path.join(highlight_dir, highlight_filename)
                
                # íŒŒì¼ ë³µì‚¬
                shutil.copy2(original_path, highlight_path)
                print(f"  âœ¨ {highlight_filename}")
        
        print(f"âœ… í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ì™„ë£Œ! {highlight_dir}")

# ì„¤ì • í´ë˜ìŠ¤
class ImprovedConfig:
    # í™˜ê²½ ì„¤ì •
    env_name = "FrankaSlideDense-v0"
    algorithm = "SAC"
    total_timesteps = 1_000_000
    
    # SAC í•˜ì´í¼íŒŒë¼ë¯¸í„°
    learning_rate = 1e-4
    buffer_size = 1_000_000
    learning_starts = 10_000
    batch_size = 512
    tau = 0.01
    gamma = 0.98
    train_freq = 4
    gradient_steps = 4
    
    # ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
    policy_kwargs = {
        "net_arch": [256, 256, 128],
        "activation_fn": torch.nn.ReLU,
        "normalize_images": False
    }
    
    # íƒí—˜ ë…¸ì´ì¦ˆ
    action_noise_std = 0.2
    
    # í‰ê°€ ì„¤ì •
    eval_freq = 20_000
    n_eval_episodes = 20
    eval_deterministic = True
    
    # ì €ì¥ ì„¤ì •
    save_freq = 100_000
    
    # ğŸ¥ ê°œì„ ëœ ë¹„ë””ì˜¤ ì„¤ì •
    episodes_per_stage = 3         # ê° ë‹¨ê³„ë³„ ì—í”¼ì†Œë“œ ìˆ˜
    video_fps = 30                 # ë¹„ë””ì˜¤ FPS
    
    # í™˜ê²½ ê°œì„  ì„¤ì •
    normalize_env = True
    reward_scale = 0.1
    
    # ì¶”ê°€ í•™ìŠµ ê¸°ë²•
    use_sde = True
    sde_sample_freq = 4
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    base_dir = "improved_episode_videos"
    model_dir = os.path.join(base_dir, "models")
    results_dir = os.path.join(base_dir, "training_results")
    video_dir = os.path.join(base_dir, "videos")
    log_dir = os.path.join(base_dir, "logs")
    
    # ì‹¤í—˜ ì´ë¦„
    experiment_name = f"{env_name}_{algorithm}_episode_videos_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

config = ImprovedConfig()

def create_directories():
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    dirs_to_create = [
        config.base_dir,
        config.model_dir,
        config.results_dir,
        config.video_dir,
        config.log_dir
    ]
    
    for dir_path in dirs_to_create:
        os.makedirs(dir_path, exist_ok=True)
        print(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {dir_path}")

class ImprovedTrainingCallback(BaseCallback):
    """
    ê°œì„ ëœ í•™ìŠµ ì½œë°± - ì—í”¼ì†Œë“œë³„ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ
    """
    def __init__(self, log_dir, verbose=0):
        super(ImprovedTrainingCallback, self).__init__(verbose)
        self.log_dir = log_dir
        self.episode_rewards = []
        self.episode_lengths = []
        self.success_count = 0
        self.episode_count = 0
        self.csv_file = os.path.join(log_dir, f"training_log_{config.experiment_name}.csv")
        
        # ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.video_system = StageBasedVideoSystem(
            base_dir=config.video_dir,
            total_timesteps=config.total_timesteps
        )
        
        # ì„±ëŠ¥ ì¶”ì 
        self.best_reward = float('-inf')
        self.recent_rewards = []
        self.recent_success_rate = 0.0
        
        # CSV íŒŒì¼ ì´ˆê¸°í™”
        with open(self.csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Timestep', 'Episode', 'Reward', 'Length', 'Success', 'Success_Rate', 'Best_Reward'])
    
    def _on_step(self) -> bool:
        # ë‹¨ê³„ë³„ ë¹„ë””ì˜¤ ë…¹í™” ì²´í¬
        stage_to_record = self.video_system.should_record_stage(self.num_timesteps)
        if stage_to_record:
            print(f"\nğŸ¬ [{stage_to_record}] ë¹„ë””ì˜¤ ë…¹í™”! (Step {self.num_timesteps})")
            if self.recent_rewards:
                print(f"   í˜„ì¬ í‰ê·  ë³´ìƒ: {np.mean(self.recent_rewards[-50:]):.2f}")
                print(f"   í˜„ì¬ ì„±ê³µë¥ : {self.recent_success_rate:.3f}")
            
            self.record_stage_videos(stage_to_record)
        
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì‹œ ì²˜ë¦¬
        if len(self.locals.get('dones', [])) > 0 and self.locals.get('dones', [False])[0]:
            info = self.locals.get('infos', [{}])[0]
            
            if 'episode' in info:
                episode_reward = info['episode']['r']
                episode_length = info['episode']['l']
                is_success = info.get('is_success', False)
                
                self.episode_rewards.append(episode_reward)
                self.episode_lengths.append(episode_length)
                self.recent_rewards.append(episode_reward)
                self.episode_count += 1
                
                if is_success:
                    self.success_count += 1
                
                # ì„±ê³µë¥  ê³„ì‚°
                self.recent_success_rate = self.success_count / self.episode_count if self.episode_count > 0 else 0
                
                # ìµœê·¼ 100ê°œ ì—í”¼ì†Œë“œë§Œ ìœ ì§€
                if len(self.recent_rewards) > 100:
                    self.recent_rewards.pop(0)
                
                # ìµœê³  ë³´ìƒ ì—…ë°ì´íŠ¸
                if episode_reward > self.best_reward:
                    self.best_reward = episode_reward
                    print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ë³´ìƒ! {episode_reward:.2f} (ì„±ê³µë¥ : {self.recent_success_rate:.3f})")
                
                # ë¡œê¹…
                if self.episode_count % 10 == 0:
                    avg_reward = np.mean(self.recent_rewards[-50:]) if len(self.recent_rewards) >= 50 else np.mean(self.recent_rewards)
                    avg_length = np.mean(self.episode_lengths[-50:]) if len(self.episode_lengths) >= 50 else np.mean(self.episode_lengths)
                    
                    print(f"ğŸ“Š Episode {self.episode_count:4d} | "
                          f"Reward: {episode_reward:7.2f} | "
                          f"Avg: {avg_reward:7.2f} | "
                          f"Length: {episode_length:3.0f} | "
                          f"Success: {self.recent_success_rate:.3f}")
                
                # CSV ë¡œê·¸ ì €ì¥
                with open(self.csv_file, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        self.num_timesteps,
                        self.episode_count,
                        episode_reward,
                        episode_length,
                        is_success,
                        self.recent_success_rate,
                        self.best_reward
                    ])
        
        return True
    
    def record_stage_videos(self, stage_name):
        """ë‹¨ê³„ë³„ ë¹„ë””ì˜¤ ë…¹í™”"""
        try:
            # ë¹„ë””ì˜¤ í™˜ê²½ ìƒì„±
            video_env = create_env(config.env_name, render_mode="rgb_array")
            
            # ì—í”¼ì†Œë“œ ë…¹í™”
            results = self.video_system.record_stage_episodes(
                stage_name=stage_name,
                model=self.model if hasattr(self, 'model') else None,
                env=video_env,
                num_episodes=config.episodes_per_stage
            )
            
            print(f"âœ… [{stage_name}] ë¹„ë””ì˜¤ ë…¹í™” ì™„ë£Œ!")
            print(f"   ğŸ“Š ë…¹í™”ëœ ì—í”¼ì†Œë“œ: {len(results)}ê°œ")
            if results:
                success_rate = sum(1 for r in results if r['success']) / len(results)
                avg_reward = np.mean([r['reward'] for r in results])
                print(f"   ğŸ† ì„±ê³µë¥ : {success_rate:.3f}")
                print(f"   ğŸ’ í‰ê·  ë³´ìƒ: {avg_reward:.2f}")
            
        except Exception as e:
            print(f"âš ï¸ ë¹„ë””ì˜¤ ë…¹í™” ì˜¤ë¥˜: {e}")

def create_env(env_name, render_mode=None):
    """ê°œì„ ëœ í™˜ê²½ ìƒì„± (ë˜í¼ ì ìš©)"""
    env = gym.make(env_name, render_mode=render_mode)
    env = Monitor(env)
    
    # ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ì ìš©
    if config.reward_scale != 1.0:
        env = RewardScalingWrapper(env, scale=config.reward_scale)
    
    # ì„±ê³µë¥  ì¶”ì  ì¶”ê°€
    env = SuccessTrackingWrapper(env)
    
    return env

def create_vec_env(env_name, n_envs=1, normalize=True):
    """ë²¡í„°í™”ëœ í™˜ê²½ ìƒì„±"""
    def make_env():
        env = create_env(env_name)
        return env
    
    vec_env = DummyVecEnv([make_env for _ in range(n_envs)])
    
    if normalize:
        vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)
    
    return vec_env

def create_model(env):
    """SAC ëª¨ë¸ ìƒì„±"""
    n_actions = env.action_space.shape[-1]
    
    action_noise = NormalActionNoise(
        mean=np.zeros(n_actions), 
        sigma=config.action_noise_std * np.ones(n_actions)
    )
    
    model = SAC(
        policy="MultiInputPolicy",
        env=env,
        learning_rate=config.learning_rate,
        buffer_size=config.buffer_size,
        learning_starts=config.learning_starts,
        batch_size=config.batch_size,
        tau=config.tau,
        gamma=config.gamma,
        train_freq=config.train_freq,
        gradient_steps=config.gradient_steps,
        action_noise=action_noise,
        policy_kwargs=config.policy_kwargs,
        use_sde=config.use_sde,
        sde_sample_freq=config.sde_sample_freq,
        verbose=1,
        tensorboard_log=config.log_dir,
        device="auto"
    )
    
    return model

def compare_best_vs_final_models(final_model, training_callback):
    """Best Model vs Final Model ì„±ëŠ¥ ë¹„êµ"""
    print("\nğŸ” Best Model vs Final Model ì„±ëŠ¥ ë¹„êµ...")
    
    # Best model ë¡œë“œ
    best_model_path = None
    model_dir = config.model_dir
    
    # Best model íŒŒì¼ ì°¾ê¸°
    for root, dirs, files in os.walk(model_dir):
        if "best_model" in root:
            for file in files:
                if file.endswith('.zip'):
                    best_model_path = os.path.join(root, file)
                    break
            if best_model_path:
                break
    
    if not best_model_path or not os.path.exists(best_model_path):
        print("âŒ Best modelì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    
    try:
        print(f"ğŸ“ Best model ë¡œë“œ: {best_model_path}")
        best_model = SAC.load(best_model_path)
        
        # í‰ê°€ í™˜ê²½ ìƒì„±
        eval_env = create_env(config.env_name)
        
        print("ğŸ”„ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        
        # Best model í‰ê°€
        print("   Best Model í‰ê°€ ì¤‘...")
        best_rewards, best_lengths = evaluate_policy(
            best_model, eval_env, 
            n_eval_episodes=50, 
            deterministic=True, 
            return_episode_rewards=True
        )
        
        # Final model í‰ê°€  
        print("   Final Model í‰ê°€ ì¤‘...")
        final_rewards, final_lengths = evaluate_policy(
            final_model, eval_env,
            n_eval_episodes=50,
            deterministic=True,
            return_episode_rewards=True
        )
        
        # ì„±ê³µë¥  ê³„ì‚° (ë³´ìƒì´ -5ë³´ë‹¤ í¬ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼)
        success_threshold = -5  # í™˜ê²½ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥
        best_success_rate = np.mean(np.array(best_rewards) > success_threshold)
        final_success_rate = np.mean(np.array(final_rewards) > success_threshold)
        
        # ê²°ê³¼ ì •ë¦¬
        comparison_results = {
            'best_model': {
                'mean_reward': np.mean(best_rewards),
                'std_reward': np.std(best_rewards),
                'mean_length': np.mean(best_lengths),
                'success_rate': best_success_rate,
                'rewards': best_rewards
            },
            'final_model': {
                'mean_reward': np.mean(final_rewards),
                'std_reward': np.std(final_rewards),
                'mean_length': np.mean(final_lengths),
                'success_rate': final_success_rate,
                'rewards': final_rewards
            }
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ“Š BEST MODEL vs FINAL MODEL ë¹„êµ ê²°ê³¼")
        print("="*60)
        
        print(f"ğŸ† Best Model:")
        print(f"   í‰ê·  ë³´ìƒ: {comparison_results['best_model']['mean_reward']:.2f} Â± {comparison_results['best_model']['std_reward']:.2f}")
        print(f"   ì„±ê³µë¥ : {comparison_results['best_model']['success_rate']:.3f}")
        print(f"   í‰ê·  ê¸¸ì´: {comparison_results['best_model']['mean_length']:.1f}")
        
        print(f"\nğŸ¯ Final Model:")
        print(f"   í‰ê·  ë³´ìƒ: {comparison_results['final_model']['mean_reward']:.2f} Â± {comparison_results['final_model']['std_reward']:.2f}")
        print(f"   ì„±ê³µë¥ : {comparison_results['final_model']['success_rate']:.3f}")
        print(f"   í‰ê·  ê¸¸ì´: {comparison_results['final_model']['mean_length']:.1f}")
        
        # ìŠ¹ì íŒì •
        best_better_reward = comparison_results['best_model']['mean_reward'] > comparison_results['final_model']['mean_reward']
        best_better_success = comparison_results['best_model']['success_rate'] > comparison_results['final_model']['success_rate']
        
        print(f"\nğŸ… ì¢…í•© íŒì •:")
        if best_better_reward and best_better_success:
            print("   ğŸ¥‡ Best Modelì´ ë” ìš°ìˆ˜í•©ë‹ˆë‹¤!")
        elif not best_better_reward and not best_better_success:
            print("   ğŸ¥‡ Final Modelì´ ë” ìš°ìˆ˜í•©ë‹ˆë‹¤!")
        else:
            print("   ğŸ¤ ë‘ ëª¨ë¸ì´ ê°ê° ì¥ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤!")
        
        # ì‹œê°í™” ìƒì„±
        create_comparison_visualization(comparison_results)
        
        # ë¹„êµ ë¹„ë””ì˜¤ ìƒì„±
        create_comparison_videos(best_model, final_model, training_callback)
        
        print("="*60)
        
        return comparison_results
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def create_comparison_visualization(comparison_results):
    """Best vs Final ëª¨ë¸ ë¹„êµ ì‹œê°í™”"""
    try:
        import matplotlib.pyplot as plt
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        viz_dir = os.path.join(config.results_dir, "model_comparison")
        os.makedirs(viz_dir, exist_ok=True)
        
        # 1. ë³´ìƒ ë¶„í¬ ë¹„êµ (ë°•ìŠ¤í”Œë¡¯)
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # ë°•ìŠ¤í”Œë¡¯
        data_to_plot = [comparison_results['best_model']['rewards'], 
                       comparison_results['final_model']['rewards']]
        ax1.boxplot(data_to_plot, labels=['Best Model', 'Final Model'])
        ax1.set_title('ë³´ìƒ ë¶„í¬ ë¹„êµ')
        ax1.set_ylabel('ë³´ìƒ')
        ax1.grid(True, alpha=0.3)
        
        # íˆìŠ¤í† ê·¸ë¨
        ax2.hist(comparison_results['best_model']['rewards'], alpha=0.7, label='Best Model', bins=20)
        ax2.hist(comparison_results['final_model']['rewards'], alpha=0.7, label='Final Model', bins=20)
        ax2.set_title('ë³´ìƒ íˆìŠ¤í† ê·¸ë¨')
        ax2.set_xlabel('ë³´ìƒ')
        ax2.set_ylabel('ë¹ˆë„')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'reward_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. ì„±ëŠ¥ ìš”ì•½ ë§‰ëŒ€ ê·¸ë˜í”„
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        models = ['Best Model', 'Final Model']
        rewards = [comparison_results['best_model']['mean_reward'], 
                  comparison_results['final_model']['mean_reward']]
        success_rates = [comparison_results['best_model']['success_rate'], 
                        comparison_results['final_model']['success_rate']]
        
        ax1.bar(models, rewards, color=['gold', 'skyblue'])
        ax1.set_title('í‰ê·  ë³´ìƒ ë¹„êµ')
        ax1.set_ylabel('í‰ê·  ë³´ìƒ')
        for i, v in enumerate(rewards):
            ax1.text(i, v + 0.1, f'{v:.2f}', ha='center', va='bottom')
        
        ax2.bar(models, success_rates, color=['gold', 'skyblue'])
        ax2.set_title('ì„±ê³µë¥  ë¹„êµ')
        ax2.set_ylabel('ì„±ê³µë¥ ')
        ax2.set_ylim(0, 1.0)
        for i, v in enumerate(success_rates):
            ax2.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'performance_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {viz_dir}")
        
    except Exception as e:
        print(f"âš ï¸ ì‹œê°í™” ìƒì„± ì˜¤ë¥˜: {e}")

def create_comparison_videos(best_model, final_model, training_callback):
    """Best vs Final ëª¨ë¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„±"""
    try:
        print("ğŸ¥ Best vs Final ëª¨ë¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")
        
        # ë¹„êµ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬
        comparison_dir = os.path.join(config.video_dir, "model_comparison")
        os.makedirs(comparison_dir, exist_ok=True)
        
        # Best model ë¹„ë””ì˜¤
        best_recorder = EpisodeVideoRecorder(comparison_dir)
        env = create_env(config.env_name, render_mode="rgb_array")
        
        print("   Best Model ì—í”¼ì†Œë“œ ë…¹í™” ì¤‘...")
        for i in range(3):
            result = training_callback.video_system._record_single_episode(
                best_recorder, best_model, env, i, "best_model"
            )
        
        # Final model ë¹„ë””ì˜¤  
        print("   Final Model ì—í”¼ì†Œë“œ ë…¹í™” ì¤‘...")
        for i in range(3):
            result = training_callback.video_system._record_single_episode(
                best_recorder, final_model, env, i, "final_model"
            )
        
        print(f"âœ… ë¹„êµ ë¹„ë””ì˜¤ ì™„ë£Œ: {comparison_dir}")
        
    except Exception as e:
        print(f"âš ï¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± ì˜¤ë¥˜: {e}")

def record_final_videos(model, training_callback):
    """ìµœì¢… ë¹„ë””ì˜¤ ë…¹í™”"""
    print("\nğŸ¥ ìµœì¢… ì„±ëŠ¥ ë¹„ë””ì˜¤ ë…¹í™”...")
    
    # ìµœì¢… ë‹¨ê³„ ë…¹í™” (ë§Œì•½ ì•„ì§ ì•ˆ ëë‹¤ë©´)
    final_stage = '5_100percent'
    if final_stage not in training_callback.video_system.completed_stages:
        video_env = create_env(config.env_name, render_mode="rgb_array")
        training_callback.video_system.record_stage_episodes(
            stage_name=final_stage,
            model=model,
            env=video_env,
            num_episodes=5  # ìµœì¢…ì€ ë” ë§ì´
        )
    
    # í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ìƒì„±
    training_callback.video_system.create_highlight_video()
    
    # ğŸ†• Best vs Final ëª¨ë¸ ë¹„êµ
    comparison_results = compare_best_vs_final_models(model, training_callback)
    
    print(f"âœ… ìµœì¢… ë¹„ë””ì˜¤ ì™„ë£Œ!")

def train_model():
    """ê°œì„ ëœ ëª¨ë¸ í•™ìŠµ"""
    print(f"ğŸ¯ í™˜ê²½: {config.env_name}")
    print(f"ğŸ§  ì•Œê³ ë¦¬ì¦˜: {config.algorithm}")
    print(f"ğŸ“Š ì´ í•™ìŠµ ìŠ¤í…: {config.total_timesteps:,}")
    print(f"ğŸ¥ ì—í”¼ì†Œë“œë³„ ê°œë³„ ë¹„ë””ì˜¤: ê° ë‹¨ê³„ë³„ {config.episodes_per_stage}ê°œ")
    print(f"ğŸ”§ ë³´ìƒ ìŠ¤ì¼€ì¼ë§: {config.reward_scale}")
    print(f"ğŸ“ˆ 6ë‹¨ê³„ ì²´ê³„ì  ì‹œê°í™” (0%, 20%, 40%, 60%, 80%, 100%)")
    print("-" * 60)
    
    create_directories()
    
    # í™˜ê²½ ìƒì„±
    print("ğŸ—ï¸  ê°œì„ ëœ í™˜ê²½ ìƒì„± ì¤‘...")
    env = create_vec_env(config.env_name, normalize=config.normalize_env)
    eval_env = create_vec_env(config.env_name, normalize=config.normalize_env)
    
    # ë¡œê±° ì„¤ì •
    logger_path = os.path.join(config.log_dir, config.experiment_name)
    new_logger = configure(logger_path, ["stdout", "csv", "tensorboard"])
    
    # ëª¨ë¸ ìƒì„±
    print(f"\nğŸ§  {config.algorithm} ëª¨ë¸ ì´ˆê¸°í™”...")
    model = create_model(env)
    model.set_logger(new_logger)
    
    # ì½œë°± ì„¤ì •
    callbacks = []
    
    # ê°œì„ ëœ í•™ìŠµ ëª¨ë‹ˆí„°ë§
    training_callback = ImprovedTrainingCallback(config.log_dir)
    training_callback.model = model
    callbacks.append(training_callback)
    
    # í•™ìŠµ ì „ ëœë¤ ë¹„ë””ì˜¤
    print("\nğŸ¬ [í•™ìŠµ ì „] ë¬´ì‘ìœ„ í–‰ë™ ì—í”¼ì†Œë“œ ë…¹í™”!")
    training_callback.record_stage_videos("0_random")
    
    # í‰ê°€ ì½œë°±
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=os.path.join(config.model_dir, f"best_model_{config.experiment_name}"),
        log_path=os.path.join(config.log_dir, "eval"),
        eval_freq=config.eval_freq,
        n_eval_episodes=config.n_eval_episodes,
        deterministic=config.eval_deterministic,
        render=False,
        verbose=1
    )
    callbacks.append(eval_callback)
    
    # ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_callback = CheckpointCallback(
        save_freq=config.save_freq,
        save_path=os.path.join(config.model_dir, "checkpoints"),
        name_prefix=f"{config.algorithm}_episode_{config.experiment_name}"
    )
    callbacks.append(checkpoint_callback)
    
    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸš€ ê°œì„ ëœ {config.algorithm} í•™ìŠµ ì‹œì‘!")
    print("=" * 60)
    print("ğŸ’¡ 6ë‹¨ê³„ ìë™ ì—í”¼ì†Œë“œ ë¹„ë””ì˜¤!")
    print("ğŸ’¡ ê° ì—í”¼ì†Œë“œ = ê°œë³„ mp4 íŒŒì¼!")
    print("ğŸ’¡ ì„±ê³µ/ì‹¤íŒ¨ êµ¬ë¶„ ê°€ëŠ¥!")
    print("ğŸ’¡ ìµœê³  ë³´ìƒ í•˜ì´ë¼ì´íŠ¸ ìë™ ìƒì„±!")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=config.total_timesteps,
            callback=callbacks,
            log_interval=10,
            progress_bar=True
        )
    except KeyboardInterrupt:
        print("\nâ¸ï¸  í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    end_time = time.time()
    training_time = end_time - start_time
    
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"â±ï¸  ì´ í•™ìŠµ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = os.path.join(config.model_dir, f"{config.algorithm}_episode_{config.experiment_name}_final")
    model.save(final_model_path)
    print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
    
    # ìµœì¢… í‰ê°€
    print("\nğŸ” ìµœì¢… í‰ê°€...")
    mean_reward, std_reward = evaluate_policy(
        model, eval_env, 
        n_eval_episodes=50,
        deterministic=True
    )
    print(f"ğŸ† ìµœì¢… í‰ê°€ ê²°ê³¼: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    return model, training_callback

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print(f"ğŸš€ ê°œì„ ëœ ì—í”¼ì†Œë“œ ê¸°ë°˜ ë¹„ë””ì˜¤ í•™ìŠµ ì‹œì‘!")
    print(f"ğŸ¯ ëª©í‘œ: FrankaSlideDense íƒœìŠ¤í¬ ë§ˆìŠ¤í„°í•˜ê¸°")
    print(f"ğŸ’¡ ì£¼ìš” ê°œì„ ì‚¬í•­:")
    print(f"   â€¢ ì—í”¼ì†Œë“œë³„ ê°œë³„ mp4 íŒŒì¼")
    print(f"   â€¢ 6ë‹¨ê³„ ì²´ê³„ì  ë…¹í™” (0%, 20%, 40%, 60%, 80%, 100%)")
    print(f"   â€¢ ê° ë‹¨ê³„ë³„ {config.episodes_per_stage}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   â€¢ ì„±ê³µ/ì‹¤íŒ¨ íŒŒì¼ëª… êµ¬ë¶„")
    print(f"   â€¢ ìµœê³  ë³´ìƒ í•˜ì´ë¼ì´íŠ¸ ìë™ ìƒì„±")
    print(f"   â€¢ ë³´ìƒ ìŠ¤ì¼€ì¼ë§: {config.reward_scale} (ì•ˆì •ì  í•™ìŠµ)")
    print("=" * 60)
    
    # í•™ìŠµ ì‹¤í–‰
    model, training_callback = train_model()
    
    # ìµœì¢… ë¹„ë””ì˜¤ ë…¹í™”
    record_final_videos(model, training_callback)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ê°œì„ ëœ í•™ìŠµ ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… ì„±ê³µë¥ : {training_callback.recent_success_rate:.3f}")
    print(f"ğŸ† ìµœê³  ë³´ìƒ: {training_callback.best_reward:.2f}")
    print(f"ğŸ¥ ì—í”¼ì†Œë“œë³„ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print(f"ğŸ“ ë¹„ë””ì˜¤ ì €ì¥ ìœ„ì¹˜: {config.video_dir}")
    print("ğŸ“‹ ìƒì„±ëœ ê²°ê³¼:")
    print("   â€¢ 0_random/: í•™ìŠµ ì „ ë¬´ì‘ìœ„ í–‰ë™")
    print("   â€¢ 1_20percent/: 20% í•™ìŠµ ì§„í–‰ ì‹œì ")
    print("   â€¢ 2_40percent/: 40% í•™ìŠµ ì§„í–‰ ì‹œì ")
    print("   â€¢ 3_60percent/: 60% í•™ìŠµ ì§„í–‰ ì‹œì ") 
    print("   â€¢ 4_80percent/: 80% í•™ìŠµ ì§„í–‰ ì‹œì ")
    print("   â€¢ 5_100percent/: 100% í•™ìŠµ ì™„ë£Œ ì‹œì ")
    print("   â€¢ highlights/: ìµœê³  ë³´ìƒ ì—í”¼ì†Œë“œë“¤")
    print("   â€¢ model_comparison/: Best vs Final ëª¨ë¸ ë¹„êµ ë¹„ë””ì˜¤")
    print(f"ğŸ“Š ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„: {config.results_dir}/model_comparison/")
    print("=" * 60)
    
    return model, training_callback

if __name__ == "__main__":
    model, training_callback = main()