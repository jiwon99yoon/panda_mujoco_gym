# #/home/minjun/panda_mujoco_gym/train_sac_highlight.py
# # /home/dyros/panda_mujoco_gym/train_sac_highlight.py

#!/usr/bin/env python3
"""
ğŸ¯ ì™„ì „íˆ ìˆ˜ì •ëœ ì—í”¼ì†Œë“œ ê¸°ë°˜ ë¹„ë””ì˜¤ ë…¹í™” ì‹œìŠ¤í…œ
- í•™ìŠµ í™˜ê²½ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ë¹„ë””ì˜¤ í™˜ê²½ (VecNormalize í¬í•¨)
- ì‹¤ì œ info['is_success'] ê¸°ì¤€ ì‚¬ìš©
- Eval í™˜ê²½ê³¼ ë™ì¼í•œ ì¡°ê±´ìœ¼ë¡œ ë¹„ë””ì˜¤ ì œì‘
- Best Model ê¸°ë°˜ ë¹„êµ ë¹„ë””ì˜¤
- ì„±ê³µ ì—í”¼ì†Œë“œ ìš°ì„  ë…¹í™”
"""

import os
import sys
import time
import numpy as np
import cv2
import json
import csv
from datetime import datetime
from typing import List, Dict, Optional
import gymnasium as gym
import torch
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback
from stable_baselines3.common.logger import configure
from stable_baselines3.common.noise import NormalActionNoise
import pickle

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ì‚¬ìš©ì ì •ì˜ í™˜ê²½ ë“±ë¡
import panda_mujoco_gym

print("ğŸ¯ ì™„ì „íˆ ìˆ˜ì •ëœ ì—í”¼ì†Œë“œ ê¸°ë°˜ ë¹„ë””ì˜¤ ë…¹í™” ì‹œìŠ¤í…œ!")
print("=" * 60)

# ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ë˜í¼
class RewardScalingWrapper(gym.Wrapper):
    def __init__(self, env, scale=0.1):
        super().__init__(env)
        self.scale = scale
        self.episode_reward = 0
        
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        scaled_reward = reward * self.scale
        self.episode_reward += scaled_reward
        
        if terminated or truncated:
            info['original_reward'] = self.episode_reward / self.scale
            info['scaled_reward'] = self.episode_reward
            self.episode_reward = 0
            
        return obs, scaled_reward, terminated, truncated, info
    
    def reset(self, **kwargs):
        self.episode_reward = 0
        return self.env.reset(**kwargs)

# ì„±ê³µë¥  ì¶”ì  ë˜í¼
class SuccessTrackingWrapper(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)
        self.success_count = 0
        self.episode_count = 0
        
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        if terminated or truncated:
            self.episode_count += 1
            if info.get('is_success', False):
                self.success_count += 1
            
            info['success_rate'] = self.success_count / self.episode_count if self.episode_count > 0 else 0
            
        return obs, reward, terminated, truncated, info

# ğŸ¥ ê°œì„ ëœ ì—í”¼ì†Œë“œë³„ ë¹„ë””ì˜¤ ë…¹í™” í´ë˜ìŠ¤
class FixedEpisodeVideoRecorder:
    """
    ìˆ˜ì •ëœ ì—í”¼ì†Œë“œë³„ ê°œë³„ ë¹„ë””ì˜¤ ë…¹í™” ì‹œìŠ¤í…œ
    - VecNormalize ìƒíƒœ ë™ê¸°í™”
    - ì‹¤ì œ info['is_success'] ê¸°ì¤€ ì‚¬ìš©
    - ì„±ê³µ ì—í”¼ì†Œë“œ ìš°ì„  ë…¹í™”
    """
    
    def __init__(self, save_dir: str, fps: int = 30):
        self.save_dir = save_dir
        self.fps = fps
        self.current_episode_frames = []
        self.episode_metadata = []
        
        os.makedirs(save_dir, exist_ok=True)
    
    def start_episode_recording(self):
        """ìƒˆ ì—í”¼ì†Œë“œ ë…¹í™” ì‹œì‘"""
        self.current_episode_frames = []
        
    def add_frame(self, frame: np.ndarray):
        """í˜„ì¬ ì—í”¼ì†Œë“œì— í”„ë ˆì„ ì¶”ê°€"""
        if frame is not None:
            self.current_episode_frames.append(frame.copy())
    
    def end_episode_recording(self, episode_info: Dict) -> Optional[str]:
        """
        ì—í”¼ì†Œë“œ ë…¹í™” ì¢…ë£Œ ë° ë¹„ë””ì˜¤ ì €ì¥
        
        Args:
            episode_info: {
                'episode_id': int,
                'reward': float, 
                'length': int,
                'success': bool,  # âœ… ì‹¤ì œ info['is_success'] ì‚¬ìš©
                'stage': str
            }
        """
        if not self.current_episode_frames:
            return None
            
        # íŒŒì¼ëª… ìƒì„±
        stage = episode_info.get('stage', 'unknown')
        episode_id = episode_info.get('episode_id', 0)
        success_str = 'SUCCESS' if episode_info.get('success', False) else 'FAIL'
        reward = episode_info.get('reward', 0)
        
        filename = f"{stage}_ep{episode_id:03d}_{success_str}_reward{reward:.1f}.mp4"
        video_path = os.path.join(self.save_dir, filename)
        
        # ë¹„ë””ì˜¤ ì €ì¥
        success = self._save_video(video_path, self.current_episode_frames)
        
        if success:
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            episode_info['video_path'] = video_path
            episode_info['frame_count'] = len(self.current_episode_frames)
            self.episode_metadata.append(episode_info)
            
            print(f"âœ… ì—í”¼ì†Œë“œ ë¹„ë””ì˜¤ ì €ì¥: {filename}")
            print(f"   í”„ë ˆì„ ìˆ˜: {len(self.current_episode_frames)}, ì„±ê³µ: {success_str}, ë³´ìƒ: {reward:.2f}")
            
            return video_path
        
        return None
    
    def _save_video(self, video_path: str, frames: List[np.ndarray]) -> bool:
        """í”„ë ˆì„ë“¤ì„ ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥"""
        if not frames:
            return False
            
        try:
            height, width = frames[0].shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            writer = cv2.VideoWriter(video_path, fourcc, self.fps, (width, height))
            
            for frame in frames:
                # RGB â†’ BGR ë³€í™˜ (OpenCVìš©)
                if len(frame.shape) == 3 and frame.shape[2] == 3:
                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                else:
                    frame_bgr = frame
                writer.write(frame_bgr)
            
            writer.release()
            return True
            
        except Exception as e:
            print(f"âš ï¸ ë¹„ë””ì˜¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

# ğŸ¯ ìˆ˜ì •ëœ í•™ìŠµ ì§„í–‰ë¥ ë³„ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ
class FixedStageBasedVideoSystem:
    """
    ìˆ˜ì •ëœ í•™ìŠµ ì§„í–‰ë¥ ë³„ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ
    - í•™ìŠµ í™˜ê²½ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ë¹„ë””ì˜¤ í™˜ê²½
    - VecNormalize ìƒíƒœ ë™ê¸°í™”
    - ì‹¤ì œ ì„±ê³µ ê¸°ì¤€ ì‚¬ìš©
    - ì„±ê³µ ì—í”¼ì†Œë“œ ìš°ì„  ë…¹í™”
    """
    
    def __init__(self, base_dir: str, total_timesteps: int):
        self.base_dir = base_dir
        self.total_timesteps = total_timesteps
        
        # 6ë‹¨ê³„ ì •ì˜
        self.stages = {
            '0_random': 0,                           # í•™ìŠµ ì „
            '1_20percent': total_timesteps // 5,     # 20%
            '2_40percent': total_timesteps * 2 // 5, # 40%
            '3_60percent': total_timesteps * 3 // 5, # 60% 
            '4_80percent': total_timesteps * 4 // 5, # 80%
            '5_100percent': total_timesteps          # 100%
        }
        
        self.completed_stages = set()
        self.stage_videos = {}  # stage_name -> [video_paths]
        self.best_episodes = []  # ìµœê³  ë³´ìƒ ì—í”¼ì†Œë“œë“¤
        
        # ê° ë‹¨ê³„ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        for stage_name in self.stages.keys():
            stage_dir = os.path.join(base_dir, stage_name)
            os.makedirs(stage_dir, exist_ok=True)
            self.stage_videos[stage_name] = []
    
    def should_record_stage(self, current_timestep: int) -> Optional[str]:
        """í˜„ì¬ timestepì—ì„œ ë…¹í™”í•  ë‹¨ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        for stage_name, target_step in self.stages.items():
            if (stage_name not in self.completed_stages and 
                current_timestep >= target_step):
                return stage_name
        return None
    
    def record_stage_episodes(self, stage_name: str, model, eval_env_stats, num_episodes: int = 3):
        """
        ğŸ¯ ìˆ˜ì •ëœ ì—í”¼ì†Œë“œ ë…¹í™” - Eval í™˜ê²½ê³¼ ì™„ì „íˆ ì¼ì¹˜
        
        Args:
            stage_name: ë‹¨ê³„ ì´ë¦„
            model: í•™ìŠµëœ ëª¨ë¸
            eval_env_stats: Eval í™˜ê²½ì˜ VecNormalize í†µê³„
            num_episodes: ë…¹í™”í•  ì—í”¼ì†Œë“œ ìˆ˜
        """
        print(f"\nğŸ¬ [{stage_name}] ì—í”¼ì†Œë“œ ë…¹í™” ì‹œì‘! (Eval í™˜ê²½ ê¸°ì¤€)")
        
        stage_dir = os.path.join(self.base_dir, stage_name)
        recorder = FixedEpisodeVideoRecorder(stage_dir)
        
        # ğŸ”§ Eval í™˜ê²½ê³¼ ë™ì¼í•œ ë¹„ë””ì˜¤ í™˜ê²½ ìƒì„±
        video_env = self._create_eval_identical_env(eval_env_stats)
        
        episode_results = []
        attempts = 0
        max_attempts = num_episodes * 10  # ì„±ê³µ ì—í”¼ì†Œë“œë¥¼ ìœ„í•´ ë” ë§ì€ ì‹œë„
        
        for episode_idx in range(num_episodes):
            print(f"  ì—í”¼ì†Œë“œ {episode_idx + 1}/{num_episodes} ë…¹í™” ì¤‘...")
            
            # ğŸ¯ ì„±ê³µ ì—í”¼ì†Œë“œ ìš°ì„  ë…¹í™”
            episode_found = False
            while attempts < max_attempts and not episode_found:
                attempts += 1
                result = self._record_single_episode(
                    recorder, model, video_env, episode_idx, stage_name
                )
                
                if result:
                    episode_results.append(result)
                    self.stage_videos[stage_name].append(result['video_path'])
                    
                    # ìµœê³  ë³´ìƒ ì¶”ì 
                    if not self.best_episodes or result['reward'] > min(ep['reward'] for ep in self.best_episodes):
                        self.best_episodes.append(result)
                        self.best_episodes.sort(key=lambda x: x['reward'], reverse=True)
                        self.best_episodes = self.best_episodes[:10]  # ìƒìœ„ 10ê°œë§Œ ìœ ì§€
                    
                    episode_found = True  # ì—í”¼ì†Œë“œ ì™„ë£Œ
                
                # ì„±ê³µ ì—í”¼ì†Œë“œë¥¼ ì°¾ê¸° ìœ„í•´ ë” ë§ì´ ì‹œë„
                if attempts % 5 == 0:
                    print(f"    ì‹œë„ {attempts}: ì„±ê³µ ì—í”¼ì†Œë“œ íƒìƒ‰ ì¤‘...")
        
        self.completed_stages.add(stage_name)
        print(f"âœ… [{stage_name}] ì™„ë£Œ! {len(episode_results)}ê°œ ì—í”¼ì†Œë“œ ë…¹í™”ë¨")
        
        return episode_results
    
    def _create_eval_identical_env(self, eval_env_stats):
        """
        ğŸ¯ Eval í™˜ê²½ê³¼ ì™„ì „íˆ ë™ì¼í•œ ë¹„ë””ì˜¤ í™˜ê²½ ìƒì„±
        """
        # ê¸°ë³¸ í™˜ê²½ ìƒì„± (í•™ìŠµ í™˜ê²½ê³¼ ë™ì¼í•œ ë˜í¼ë“¤ ì ìš©)
        env = gym.make(config.env_name, render_mode="rgb_array")
        env = Monitor(env)
        
        # ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ì ìš©
        if config.reward_scale != 1.0:
            env = RewardScalingWrapper(env, scale=config.reward_scale)
        
        # ì„±ê³µë¥  ì¶”ì  ì¶”ê°€
        env = SuccessTrackingWrapper(env)
        
        # ë²¡í„°í™”
        vec_env = DummyVecEnv([lambda: env])
        
        # ğŸ”§ VecNormalize ì ìš© ë° í†µê³„ ë™ê¸°í™”
        if config.normalize_env and eval_env_stats is not None:
            vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)
            # Eval í™˜ê²½ì˜ ì •ê·œí™” í†µê³„ ë³µì‚¬
            vec_env.obs_rms = eval_env_stats['obs_rms']
            vec_env.ret_rms = eval_env_stats['ret_rms']
            vec_env.training = False  # í‰ê°€ ëª¨ë“œ
        
        return vec_env
    
    def _record_single_episode(self, recorder, model, video_env, episode_idx, stage_name):
        """ë‹¨ì¼ ì—í”¼ì†Œë“œ ë…¹í™”"""
        recorder.start_episode_recording()
        
        obs = video_env.reset()
        total_reward = 0
        step_count = 0
        done = False
        
        # ì´ˆê¸° í”„ë ˆì„ ì¶”ê°€ (ì˜¬ë°”ë¥¸ ì „ì²´ ì´ë¯¸ì§€)
        rendered = video_env.render()
        if rendered is not None:
            img = rendered[0] if isinstance(rendered, (list, tuple)) else rendered
            recorder.add_frame(img)

        while not done:
            # ì•¡ì…˜ ì˜ˆì¸¡ (deterministic=Trueë¡œ Evalê³¼ ë™ì¼)
            if model and hasattr(model, 'predict'):
                action, _ = model.predict(obs, deterministic=True)
            else:
                action = video_env.action_space.sample()  # ëœë¤ ì•¡ì…˜
            
            # í™˜ê²½ ìŠ¤í…
            obs, rewards, dones, infos = video_env.step(action)
            done = dones[0]
            total_reward += rewards[0]
            step_count += 1
            
            # í”„ë ˆì„ ì¶”ê°€ (ì˜¬ë°”ë¥¸ ì „ì²´ ì´ë¯¸ì§€)
            rendered = video_env.render()
            if rendered is not None:
                img = rendered[0] if isinstance(rendered, (list, tuple)) else rendered
                recorder.add_frame(img)
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if step_count > 1000:
                break
        
        # ğŸ¯ ì‹¤ì œ ì„±ê³µ ê¸°ì¤€ ì‚¬ìš©
        episode_info = {
            'episode_id': episode_idx,
            'reward': total_reward,
            'length': step_count,
            'success': infos[0].get('is_success', False),  # âœ… ì‹¤ì œ ì„±ê³µ ê¸°ì¤€
            'stage': stage_name
        }
        
        # ë¹„ë””ì˜¤ ì €ì¥
        video_path = recorder.end_episode_recording(episode_info)
        
        if video_path:
            episode_info['video_path'] = video_path
            return episode_info
        
        return None
    
    def create_highlight_video(self):
        """ìµœê³  ë³´ìƒ ì—í”¼ì†Œë“œë“¤ë¡œ í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ìƒì„±"""
        if not self.best_episodes:
            print("âŒ í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: ì—í”¼ì†Œë“œ ì—†ìŒ")
            return
        
        print(f"\nğŸŒŸ í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ìƒì„± ì¤‘... (ìƒìœ„ {len(self.best_episodes)}ê°œ ì—í”¼ì†Œë“œ)")
        
        highlight_dir = os.path.join(self.base_dir, "highlights")
        os.makedirs(highlight_dir, exist_ok=True)
        
        # ê° ìµœê³  ì—í”¼ì†Œë“œë¥¼ í•˜ì´ë¼ì´íŠ¸ í´ë”ì— ë³µì‚¬
        import shutil
        for i, episode in enumerate(self.best_episodes):
            original_path = episode['video_path']
            if os.path.exists(original_path):
                highlight_filename = f"highlight_{i+1:02d}_reward{episode['reward']:.1f}_{episode['stage']}.mp4"
                highlight_path = os.path.join(highlight_dir, highlight_filename)
                
                # íŒŒì¼ ë³µì‚¬
                shutil.copy2(original_path, highlight_path)
                print(f"  âœ¨ {highlight_filename}")
        
        print(f"âœ… í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ì™„ë£Œ! {highlight_dir}")

# ì„¤ì • í´ë˜ìŠ¤
class FixedConfig:
    # í™˜ê²½ ì„¤ì •
    env_name = "FrankaSlideDense-v0"
    algorithm = "SAC"
    total_timesteps = 1_000_000
    
    # SAC í•˜ì´í¼íŒŒë¼ë¯¸í„°
    learning_rate = 1e-4
    buffer_size = 1_000_000
    learning_starts = 10_000
    batch_size = 512
    tau = 0.01
    gamma = 0.98
    train_freq = 4
    gradient_steps = 4
    
    # ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
    policy_kwargs = {
        "net_arch": [256, 256, 128],
        "activation_fn": torch.nn.ReLU,
        "normalize_images": False
    }
    
    # íƒí—˜ ë…¸ì´ì¦ˆ
    action_noise_std = 0.2
    
    # í‰ê°€ ì„¤ì •
    eval_freq = 20_000
    n_eval_episodes = 20
    eval_deterministic = True
    
    # ì €ì¥ ì„¤ì •
    save_freq = 100_000
    
    # ğŸ¥ ìˆ˜ì •ëœ ë¹„ë””ì˜¤ ì„¤ì •
    episodes_per_stage = 3         # ê° ë‹¨ê³„ë³„ ì—í”¼ì†Œë“œ ìˆ˜
    video_fps = 30                 # ë¹„ë””ì˜¤ FPS
    
    # í™˜ê²½ ê°œì„  ì„¤ì •
    normalize_env = True
    reward_scale = 0.1
    
    # ì¶”ê°€ í•™ìŠµ ê¸°ë²•
    use_sde = True
    sde_sample_freq = 4
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    base_dir = "fixed_episode_videos"
    model_dir = os.path.join(base_dir, "models")
    results_dir = os.path.join(base_dir, "training_results")
    video_dir = os.path.join(base_dir, "videos")
    log_dir = os.path.join(base_dir, "logs")
    
    # ì‹¤í—˜ ì´ë¦„
    experiment_name = f"{env_name}_{algorithm}_fixed_videos_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

config = FixedConfig()

def create_directories():
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    dirs_to_create = [
        config.base_dir,
        config.model_dir,
        config.results_dir,
        config.video_dir,
        config.log_dir
    ]
    
    for dir_path in dirs_to_create:
        os.makedirs(dir_path, exist_ok=True)
        print(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {dir_path}")

# ğŸ¯ ìˆ˜ì •ëœ í•™ìŠµ ì½œë°±
class FixedTrainingCallback(BaseCallback):
    """
    ìˆ˜ì •ëœ í•™ìŠµ ì½œë°± - Eval í™˜ê²½ê³¼ ì¼ì¹˜í•˜ëŠ” ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ
    """
    def __init__(self, log_dir, verbose=0):
        super(FixedTrainingCallback, self).__init__(verbose)
        self.log_dir = log_dir
        self.episode_rewards = []
        self.episode_lengths = []
        self.success_count = 0
        self.episode_count = 0
        self.csv_file = os.path.join(log_dir, f"training_log_{config.experiment_name}.csv")
        
        # ìˆ˜ì •ëœ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.video_system = FixedStageBasedVideoSystem(
            base_dir=config.video_dir,
            total_timesteps=config.total_timesteps
        )
        
        # ì„±ëŠ¥ ì¶”ì 
        self.best_reward = float('-inf')
        self.recent_rewards = []
        self.recent_success_rate = 0.0
        
        # ğŸ”§ Eval í™˜ê²½ í†µê³„ ì €ì¥ìš©
        self.eval_env_stats = None
        
        # CSV íŒŒì¼ ì´ˆê¸°í™”
        with open(self.csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Timestep', 'Episode', 'Reward', 'Length', 'Success', 'Success_Rate', 'Best_Reward'])
    
    def set_eval_env_stats(self, eval_env):
        """Eval í™˜ê²½ì˜ ì •ê·œí™” í†µê³„ ì €ì¥"""
        if hasattr(eval_env, 'obs_rms') and hasattr(eval_env, 'ret_rms'):
            self.eval_env_stats = {
                'obs_rms': eval_env.obs_rms,
                'ret_rms': eval_env.ret_rms
            }
            #print("âœ… Eval í™˜ê²½ í†µê³„ ì €ì¥ ì™„ë£Œ!")
    
    def _on_step(self) -> bool:
        # ë‹¨ê³„ë³„ ë¹„ë””ì˜¤ ë…¹í™” ì²´í¬
        stage_to_record = self.video_system.should_record_stage(self.num_timesteps)
        if stage_to_record:
            print(f"\nğŸ¬ [{stage_to_record}] ìˆ˜ì •ëœ ë¹„ë””ì˜¤ ë…¹í™”! (Step {self.num_timesteps})")
            if self.recent_rewards:
                print(f"   í˜„ì¬ í‰ê·  ë³´ìƒ: {np.mean(self.recent_rewards[-50:]):.2f}")
                print(f"   í˜„ì¬ ì„±ê³µë¥ : {self.recent_success_rate:.3f}")
            
            self.record_stage_videos(stage_to_record)
        
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì‹œ ì²˜ë¦¬
        if len(self.locals.get('dones', [])) > 0 and self.locals.get('dones', [False])[0]:
            info = self.locals.get('infos', [{}])[0]
            
            if 'episode' in info:
                episode_reward = info['episode']['r']
                episode_length = info['episode']['l']
                is_success = info.get('is_success', False)
                
                self.episode_rewards.append(episode_reward)
                self.episode_lengths.append(episode_length)
                self.recent_rewards.append(episode_reward)
                self.episode_count += 1
                
                if is_success:
                    self.success_count += 1
                
                # ì„±ê³µë¥  ê³„ì‚°
                self.recent_success_rate = self.success_count / self.episode_count if self.episode_count > 0 else 0
                
                # ìµœê·¼ 100ê°œ ì—í”¼ì†Œë“œë§Œ ìœ ì§€
                if len(self.recent_rewards) > 100:
                    self.recent_rewards.pop(0)
                
                # ìµœê³  ë³´ìƒ ì—…ë°ì´íŠ¸
                if episode_reward > self.best_reward:
                    self.best_reward = episode_reward
                    print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ë³´ìƒ! {episode_reward:.2f} (ì„±ê³µë¥ : {self.recent_success_rate:.3f})")
                
                # ë¡œê¹…
                if self.episode_count % 10 == 0:
                    avg_reward = np.mean(self.recent_rewards[-50:]) if len(self.recent_rewards) >= 50 else np.mean(self.recent_rewards)
                    avg_length = np.mean(self.episode_lengths[-50:]) if len(self.episode_lengths) >= 50 else np.mean(self.episode_lengths)
                    
                    print(f"ğŸ“Š Episode {self.episode_count:4d} | "
                          f"Reward: {episode_reward:7.2f} | "
                          f"Avg: {avg_reward:7.2f} | "
                          f"Length: {episode_length:3.0f} | "
                          f"Success: {self.recent_success_rate:.3f}")
                
                # CSV ë¡œê·¸ ì €ì¥
                with open(self.csv_file, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        self.num_timesteps,
                        self.episode_count,
                        episode_reward,
                        episode_length,
                        is_success,
                        self.recent_success_rate,
                        self.best_reward
                    ])
        
        return True
    
    def record_stage_videos(self, stage_name):
        """ğŸ¯ ìˆ˜ì •ëœ ë‹¨ê³„ë³„ ë¹„ë””ì˜¤ ë…¹í™”"""
        try:
            # ğŸ”§ Eval í™˜ê²½ í†µê³„ì™€ í•¨ê»˜ ë¹„ë””ì˜¤ ë…¹í™”
            results = self.video_system.record_stage_episodes(
                stage_name=stage_name,
                model=self.model if hasattr(self, 'model') else None,
                eval_env_stats=self.eval_env_stats,
                num_episodes=config.episodes_per_stage
            )
            
            print(f"âœ… [{stage_name}] ìˆ˜ì •ëœ ë¹„ë””ì˜¤ ë…¹í™” ì™„ë£Œ!")
            print(f"   ğŸ“Š ë…¹í™”ëœ ì—í”¼ì†Œë“œ: {len(results)}ê°œ")
            if results:
                success_rate = sum(1 for r in results if r['success']) / len(results)
                avg_reward = np.mean([r['reward'] for r in results])
                print(f"   ğŸ† ì„±ê³µë¥ : {success_rate:.3f}")
                print(f"   ğŸ’ í‰ê·  ë³´ìƒ: {avg_reward:.2f}")
            
        except Exception as e:
            print(f"âš ï¸ ë¹„ë””ì˜¤ ë…¹í™” ì˜¤ë¥˜: {e}")

def create_env(env_name, render_mode=None):
    """ê°œì„ ëœ í™˜ê²½ ìƒì„± (ë˜í¼ ì ìš©)"""
    env = gym.make(env_name, render_mode=render_mode)
    env = Monitor(env)
    
    # ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ì ìš©
    if config.reward_scale != 1.0:
        env = RewardScalingWrapper(env, scale=config.reward_scale)
    
    # ì„±ê³µë¥  ì¶”ì  ì¶”ê°€
    env = SuccessTrackingWrapper(env)
    
    return env

def create_vec_env(env_name, n_envs=1, normalize=True):
    """ë²¡í„°í™”ëœ í™˜ê²½ ìƒì„±"""
    def make_env():
        env = create_env(env_name)
        return env
    
    vec_env = DummyVecEnv([make_env for _ in range(n_envs)])
    
    if normalize:
        vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)
    
    return vec_env

def create_model(env):
    """SAC ëª¨ë¸ ìƒì„±"""
    n_actions = env.action_space.shape[-1]
    
    action_noise = NormalActionNoise(
        mean=np.zeros(n_actions), 
        sigma=config.action_noise_std * np.ones(n_actions)
    )
    
    model = SAC(
        policy="MultiInputPolicy",
        env=env,
        learning_rate=config.learning_rate,
        buffer_size=config.buffer_size,
        learning_starts=config.learning_starts,
        batch_size=config.batch_size,
        tau=config.tau,
        gamma=config.gamma,
        train_freq=config.train_freq,
        gradient_steps=config.gradient_steps,
        action_noise=action_noise,
        policy_kwargs=config.policy_kwargs,
        use_sde=config.use_sde,
        sde_sample_freq=config.sde_sample_freq,
        verbose=1,
        tensorboard_log=config.log_dir,
        device="auto"
    )
    
    return model

def fixed_compare_best_vs_final_models(final_model, training_callback):
    print("\nğŸ” ì‹œì ë³„ Best Model vs Final Model ì„±ëŠ¥ ë¹„êµ...")

    comparison_results = {}

    # 1) model_dir ì—ì„œ best_model_* í´ë”ë§Œ ê³¨ë¼ì„œ ìˆœíšŒ
    for sub in sorted(os.listdir(config.model_dir)):
        if not sub.startswith("best_model_"):
            continue
        stage = sub.replace("best_model_", "")
        model_zip = os.path.join(config.model_dir, sub, "best_model.zip")
        if not os.path.exists(model_zip):
            continue

        print(f"\nâ–¶ Evaluate Stage {stage}")
        bm = SAC.load(model_zip)
        eval_env = create_env(config.env_name)

        # ë³´ìƒ & ê¸¸ì´ í‰ê°€
        rewards, lengths = evaluate_policy(
            bm, eval_env, 
            n_eval_episodes=20, 
            deterministic=True, 
            return_episode_rewards=True
        )
        # ì„±ê³µë¥  ê³„ì‚°
        succ = 0
        for _ in range(20):
            obs, _ = eval_env.reset()
            done = False
            while not done:
                a, _ = bm.predict(obs, deterministic=True)
                obs, _, term, trunc, info = eval_env.step(a)
                done = term or trunc
            if info.get('is_success', False):
                succ += 1

        comparison_results[stage] = {
            'mean_reward': np.mean(rewards),
            'std_reward' : np.std(rewards),
            'mean_length': np.mean(lengths),
            'success_rate': succ / 20,
            'rewards': rewards
        }

    # 2) Final Model í‰ê°€
    print("\nâ–¶ Evaluate Final Model")
    eval_env = create_env(config.env_name)
    fr, fl = evaluate_policy(
        final_model, eval_env,
        n_eval_episodes=20,
        deterministic=True,
        return_episode_rewards=True
    )
    fs = 0
    for _ in range(20):
        obs, _ = eval_env.reset()
        done = False
        while not done:
            a, _ = final_model.predict(obs, deterministic=True)
            obs, _, term, trunc, info = eval_env.step(a)
            done = term or trunc
        if info.get('is_success', False):
            fs += 1

    comparison_results['final'] = {
        'mean_reward': np.mean(fr),
        'std_reward' : np.std(fr),
        'mean_length': np.mean(fl),
        'success_rate': fs / 20,
        'rewards': fr
    }

    return comparison_results

# def fixed_compare_best_vs_final_models(final_model, training_callback):
#     """ğŸ¯ ìˆ˜ì •ëœ Best Model vs Final Model ì„±ëŠ¥ ë¹„êµ"""
#     print("\nğŸ” ìˆ˜ì •ëœ Best Model vs Final Model ì„±ëŠ¥ ë¹„êµ...")
    
#     # Best model ë¡œë“œ
#     best_model_path = None
#     model_dir = config.model_dir
    
#     # Best model íŒŒì¼ ì°¾ê¸°
#     for root, dirs, files in os.walk(model_dir):
#         if "best_model" in root:
#             for file in files:
#                 if file.endswith('.zip'):
#                     best_model_path = os.path.join(root, file)
#                     break
#             if best_model_path:
#                 break
    
#     if not best_model_path or not os.path.exists(best_model_path):
#         print("âŒ Best modelì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
#         return None
    
#     try:
#         print(f"ğŸ“ Best model ë¡œë“œ: {best_model_path}")
#         best_model = SAC.load(best_model_path)
        
#         # ğŸ”§ Evalê³¼ ë™ì¼í•œ í™˜ê²½ ìƒì„±
#         eval_env = create_env(config.env_name)
        
#         print("ğŸ”„ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        
#         # Best model í‰ê°€
#         print("   Best Model í‰ê°€ ì¤‘...")
#         best_rewards, best_lengths = evaluate_policy(
#             best_model, eval_env, 
#             n_eval_episodes=50, 
#             deterministic=True, 
#             return_episode_rewards=True
#         )
        
#         # Final model í‰ê°€  
#         print("   Final Model í‰ê°€ ì¤‘...")
#         final_rewards, final_lengths = evaluate_policy(
#             final_model, eval_env,
#             n_eval_episodes=50,
#             deterministic=True,
#             return_episode_rewards=True
#         )
        
#         # ğŸ¯ ì‹¤ì œ ì„±ê³µë¥  ê³„ì‚° (evaluate_policyì˜ infoë¡œë¶€í„°)
#         # ì‹¤ì œ ì„±ê³µ ì—í”¼ì†Œë“œ ì¹´ìš´íŠ¸ë¥¼ ìœ„í•œ ì¶”ê°€ í‰ê°€
#         best_success_count = 0
#         final_success_count = 0
        
#         # Best model ì„±ê³µë¥  ì¸¡ì •
#         for _ in range(50):
#             obs, _ = eval_env.reset()
#             done = False
#             while not done:
#                 action, _ = best_model.predict(obs, deterministic=True)
#                 obs, reward, terminated, truncated, info = eval_env.step(action)
#                 done = terminated or truncated
#                 if done and info.get('is_success', False):
#                     best_success_count += 1
#                     break
        
#         # Final model ì„±ê³µë¥  ì¸¡ì •
#         for _ in range(50):
#             obs, _ = eval_env.reset()
#             done = False
#             while not done:
#                 action, _ = final_model.predict(obs, deterministic=True)
#                 obs, reward, terminated, truncated, info = eval_env.step(action)
#                 done = terminated or truncated
#                 if done and info.get('is_success', False):
#                     final_success_count += 1
#                     break
        
#         best_success_rate = best_success_count / 50
#         final_success_rate = final_success_count / 50
        
#         # ê²°ê³¼ ì •ë¦¬
#         comparison_results = {
#             'best_model': {
#                 'mean_reward': np.mean(best_rewards),
#                 'std_reward': np.std(best_rewards),
#                 'mean_length': np.mean(best_lengths),
#                 'success_rate': best_success_rate,  # âœ… ì‹¤ì œ ì„±ê³µë¥ 
#                 'rewards': best_rewards
#             },
#             'final_model': {
#                 'mean_reward': np.mean(final_rewards),
#                 'std_reward': np.std(final_rewards),
#                 'mean_length': np.mean(final_lengths),
#                 'success_rate': final_success_rate,  # âœ… ì‹¤ì œ ì„±ê³µë¥ 
#                 'rewards': final_rewards
#             }
#         }
        
#         # ê²°ê³¼ ì¶œë ¥
#         print("\n" + "="*60)
#         print("ğŸ“Š ìˆ˜ì •ëœ BEST MODEL vs FINAL MODEL ë¹„êµ ê²°ê³¼")
#         print("="*60)
        
#         print(f"ğŸ† Best Model:")
#         print(f"   í‰ê·  ë³´ìƒ: {comparison_results['best_model']['mean_reward']:.2f} Â± {comparison_results['best_model']['std_reward']:.2f}")
#         print(f"   ì‹¤ì œ ì„±ê³µë¥ : {comparison_results['best_model']['success_rate']:.3f}")
#         print(f"   í‰ê·  ê¸¸ì´: {comparison_results['best_model']['mean_length']:.1f}")
        
#         print(f"\nğŸ¯ Final Model:")
#         print(f"   í‰ê·  ë³´ìƒ: {comparison_results['final_model']['mean_reward']:.2f} Â± {comparison_results['final_model']['std_reward']:.2f}")
#         print(f"   ì‹¤ì œ ì„±ê³µë¥ : {comparison_results['final_model']['success_rate']:.3f}")
#         print(f"   í‰ê·  ê¸¸ì´: {comparison_results['final_model']['mean_length']:.1f}")
        
#         # ìŠ¹ì íŒì •
#         best_better_reward = comparison_results['best_model']['mean_reward'] > comparison_results['final_model']['mean_reward']
#         best_better_success = comparison_results['best_model']['success_rate'] > comparison_results['final_model']['success_rate']
        
#         print(f"\nğŸ… ì¢…í•© íŒì •:")
#         if best_better_reward and best_better_success:
#             print("   ğŸ¥‡ Best Modelì´ ë” ìš°ìˆ˜í•©ë‹ˆë‹¤!")
#         elif not best_better_reward and not best_better_success:
#             print("   ğŸ¥‡ Final Modelì´ ë” ìš°ìˆ˜í•©ë‹ˆë‹¤!")
#         else:
#             print("   ğŸ¤ ë‘ ëª¨ë¸ì´ ê°ê° ì¥ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤!")
        
#         # ì‹œê°í™” ìƒì„±
#         create_comparison_visualization(comparison_results)
        
#         # ğŸ¯ ìˆ˜ì •ëœ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„±
#         create_fixed_comparison_videos(best_model, final_model, training_callback)
        
#         print("="*60)
        
#         return comparison_results
        
    # except Exception as e:
    #     print(f"âŒ ëª¨ë¸ ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    #     return None

def create_comparison_visualization(comparison_results):
    """Best vs Final ëª¨ë¸ ë¹„êµ ì‹œê°í™”"""
    try:
        import matplotlib.pyplot as plt
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        viz_dir = os.path.join(config.results_dir, "model_comparison")
        os.makedirs(viz_dir, exist_ok=True)
        
        # 1. ë³´ìƒ ë¶„í¬ ë¹„êµ (ë°•ìŠ¤í”Œë¡¯)
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # ë°•ìŠ¤í”Œë¡¯
        data_to_plot = [comparison_results['best_model']['rewards'], 
                       comparison_results['final_model']['rewards']]
        ax1.boxplot(data_to_plot, labels=['Best Model', 'Final Model'])
        ax1.set_title('ë³´ìƒ ë¶„í¬ ë¹„êµ')
        ax1.set_ylabel('ë³´ìƒ')
        ax1.grid(True, alpha=0.3)
        
        # íˆìŠ¤í† ê·¸ë¨
        ax2.hist(comparison_results['best_model']['rewards'], alpha=0.7, label='Best Model', bins=20)
        ax2.hist(comparison_results['final_model']['rewards'], alpha=0.7, label='Final Model', bins=20)
        ax2.set_title('ë³´ìƒ íˆìŠ¤í† ê·¸ë¨')
        ax2.set_xlabel('ë³´ìƒ')
        ax2.set_ylabel('ë¹ˆë„')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'reward_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. ì„±ëŠ¥ ìš”ì•½ ë§‰ëŒ€ ê·¸ë˜í”„
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        models = ['Best Model', 'Final Model']
        rewards = [comparison_results['best_model']['mean_reward'], 
                  comparison_results['final_model']['mean_reward']]
        success_rates = [comparison_results['best_model']['success_rate'], 
                        comparison_results['final_model']['success_rate']]
        
        ax1.bar(models, rewards, color=['gold', 'skyblue'])
        ax1.set_title('í‰ê·  ë³´ìƒ ë¹„êµ')
        ax1.set_ylabel('í‰ê·  ë³´ìƒ')
        for i, v in enumerate(rewards):
            ax1.text(i, v + 0.1, f'{v:.2f}', ha='center', va='bottom')
        
        ax2.bar(models, success_rates, color=['gold', 'skyblue'])
        ax2.set_title('ì‹¤ì œ ì„±ê³µë¥  ë¹„êµ')
        ax2.set_ylabel('ì„±ê³µë¥ ')
        ax2.set_ylim(0, 1.0)
        for i, v in enumerate(success_rates):
            ax2.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'performance_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {viz_dir}")
        
    except Exception as e:
        print(f"âš ï¸ ì‹œê°í™” ìƒì„± ì˜¤ë¥˜: {e}")

# def create_fixed_comparison_videos(best_model, final_model, training_callback):
#     """ğŸ¯ ìˆ˜ì •ëœ Best vs Final ëª¨ë¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„±"""
#     try:
#         print("ğŸ¥ ìˆ˜ì •ëœ Best vs Final ëª¨ë¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")
        
#         # ë¹„êµ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬
#         comparison_dir = os.path.join(config.video_dir, "model_comparison")
#         os.makedirs(comparison_dir, exist_ok=True)
        
#         # ğŸ”§ ìˆ˜ì •ëœ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ ì‚¬ìš©
#         fixed_video_system = FixedStageBasedVideoSystem(
#             base_dir=comparison_dir,
#             total_timesteps=config.total_timesteps
#         )
        
#         print("   Best Model ì—í”¼ì†Œë“œ ë…¹í™” ì¤‘...")
#         fixed_video_system.record_stage_episodes(
#             stage_name="best_model",
#             model=best_model,
#             eval_env_stats=training_callback.eval_env_stats,
#             num_episodes=3
#         )
        
#         print("   Final Model ì—í”¼ì†Œë“œ ë…¹í™” ì¤‘...")
#         fixed_video_system.record_stage_episodes(
#             stage_name="final_model", 
#             model=final_model,
#             eval_env_stats=training_callback.eval_env_stats,
#             num_episodes=3
#         )
        
#         print(f"âœ… ìˆ˜ì •ëœ ë¹„êµ ë¹„ë””ì˜¤ ì™„ë£Œ: {comparison_dir}")
        
#     except Exception as e:
#         print(f"âš ï¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± ì˜¤ë¥˜: {e}")

def create_fixed_comparison_videos(best_model, final_model, training_callback):
    print("ğŸ¥ ì‹œì ë³„ Best vs Final ëª¨ë¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")
    comparison_dir = os.path.join(config.video_dir, "model_comparison")
    os.makedirs(comparison_dir, exist_ok=True)

    video_sys = FixedStageBasedVideoSystem(
        base_dir=comparison_dir,
        total_timesteps=config.total_timesteps
    )

    # 1) best_model_* ìˆœíšŒ
    for sub in sorted(os.listdir(config.model_dir)):
        if not sub.startswith("best_model_"):
            continue
        stage = sub.replace("best_model_", "")
        print(f"   â–¶ Best Model [{stage}] ë…¹í™”")
        model_path = os.path.join(config.model_dir, sub, "best_model.zip")
        bm = SAC.load(model_path)
        video_sys.record_stage_episodes(
            stage_name=f"best_{stage}",
            model=bm,
            eval_env_stats=training_callback.eval_env_stats,
            num_episodes=config.episodes_per_stage
        )

    # 2) final ëª¨ë¸ë„ ë™ì¼í•˜ê²Œ ë…¹í™”
    print("   â–¶ Final Model ë…¹í™”")
    video_sys.record_stage_episodes(
        stage_name="final",
        model=final_model,
        eval_env_stats=training_callback.eval_env_stats,
        num_episodes=config.episodes_per_stage
    )

    print(f"âœ… ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {comparison_dir}")


def record_final_videos(model, training_callback):
    """ğŸ¯ ìˆ˜ì •ëœ ìµœì¢… ë¹„ë””ì˜¤ ë…¹í™”"""
    print("\nğŸ¥ ìˆ˜ì •ëœ ìµœì¢… ì„±ëŠ¥ ë¹„ë””ì˜¤ ë…¹í™”...")
    
    # ìµœì¢… ë‹¨ê³„ ë…¹í™” (ë§Œì•½ ì•„ì§ ì•ˆ ëë‹¤ë©´)
    final_stage = '5_100percent'
    if final_stage not in training_callback.video_system.completed_stages:
        training_callback.video_system.record_stage_episodes(
            stage_name=final_stage,
            model=model,
            eval_env_stats=training_callback.eval_env_stats,
            num_episodes=5  # ìµœì¢…ì€ ë” ë§ì´
        )
    
    # í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ìƒì„±
    training_callback.video_system.create_highlight_video()
    
    # ğŸ¯ ìˆ˜ì •ëœ Best vs Final ëª¨ë¸ ë¹„êµ
    comparison_results = fixed_compare_best_vs_final_models(model, training_callback)
    
    print(f"âœ… ìˆ˜ì •ëœ ìµœì¢… ë¹„ë””ì˜¤ ì™„ë£Œ!")

def train_model():
    """ğŸ¯ ìˆ˜ì •ëœ ëª¨ë¸ í•™ìŠµ"""
    print(f"ğŸ¯ í™˜ê²½: {config.env_name}")
    print(f"ğŸ§  ì•Œê³ ë¦¬ì¦˜: {config.algorithm}")
    print(f"ğŸ“Š ì´ í•™ìŠµ ìŠ¤í…: {config.total_timesteps:,}")
    print(f"ğŸ¥ ìˆ˜ì •ëœ ì—í”¼ì†Œë“œë³„ ë¹„ë””ì˜¤: ê° ë‹¨ê³„ë³„ {config.episodes_per_stage}ê°œ")
    print(f"ğŸ”§ ë³´ìƒ ìŠ¤ì¼€ì¼ë§: {config.reward_scale}")
    print(f"ğŸ“ˆ 6ë‹¨ê³„ ì²´ê³„ì  ì‹œê°í™” (0%, 20%, 40%, 60%, 80%, 100%)")
    print(f"âœ… Eval í™˜ê²½ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ë¹„ë””ì˜¤")
    print(f"âœ… ì‹¤ì œ info['is_success'] ê¸°ì¤€ ì‚¬ìš©")
    print("-" * 60)
    
    create_directories()
    
    # í™˜ê²½ ìƒì„±
    print("ğŸ—ï¸  ê°œì„ ëœ í™˜ê²½ ìƒì„± ì¤‘...")
    env = create_vec_env(config.env_name, normalize=config.normalize_env)
    eval_env = create_vec_env(config.env_name, normalize=config.normalize_env)
    
    # ë¡œê±° ì„¤ì •
    logger_path = os.path.join(config.log_dir, config.experiment_name)
    new_logger = configure(logger_path, ["stdout", "csv", "tensorboard"])
    
    # ëª¨ë¸ ìƒì„±
    print(f"\nğŸ§  {config.algorithm} ëª¨ë¸ ì´ˆê¸°í™”...")
    model = create_model(env)
    model.set_logger(new_logger)
    
    # ì½œë°± ì„¤ì •
    callbacks = []
    
    # ğŸ¯ ìˆ˜ì •ëœ í•™ìŠµ ëª¨ë‹ˆí„°ë§
    training_callback = FixedTrainingCallback(config.log_dir)
    training_callback.model = model
    callbacks.append(training_callback)
    
    # ğŸ”§ ìˆ˜ì •ëœ í‰ê°€ ì½œë°±
    class FixedEvalCallback(EvalCallback):
        def __init__(self, *args, **kwargs):
            self.training_callback = kwargs.pop('training_callback', None)
            super().__init__(*args, **kwargs)
        
        def _on_step(self) -> bool:
            result = super()._on_step()
            # Eval í™˜ê²½ í†µê³„ë¥¼ training_callbackì— ì „ë‹¬
            if self.training_callback and hasattr(self.eval_env, 'obs_rms'):
                self.training_callback.set_eval_env_stats(self.eval_env)

            # 3) í˜„ì¬ timestep ê¸°ì¤€ ìŠ¤í…Œì´ì§€ ê³„ì‚°
            current_stage = self.training_callback.video_system.should_record_stage(self.num_timesteps)
            if current_stage:
                # ë””ë ‰í† ë¦¬ ì˜ˆ: models/best_model_1_20percent/best_model.zip
                save_dir = os.path.join(config.model_dir, f"best_model_{current_stage}")
                os.makedirs(save_dir, exist_ok=True)
                # EvalCallback ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²½ë¡œë¥¼ ë™ì ìœ¼ë¡œ êµì²´
                self.best_model_save_path = save_dir

            return result
    
    eval_callback = FixedEvalCallback(
        eval_env,
        training_callback=training_callback,
        best_model_save_path=os.path.join(config.model_dir, "best_model"),  # prefix ìš©ìœ¼ë¡œë§Œ ë‚¨ê¹€
        log_path=os.path.join(config.log_dir, "eval"),
        eval_freq=config.eval_freq,
        n_eval_episodes=config.n_eval_episodes,
        deterministic=config.eval_deterministic,
        verbose=1
    )


    callbacks.append(eval_callback)
    
    # ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_callback = CheckpointCallback(
        save_freq=config.save_freq,
        save_path=os.path.join(config.model_dir, "checkpoints"),
        name_prefix=f"{config.algorithm}_fixed_{config.experiment_name}"
    )
    callbacks.append(checkpoint_callback)
    
    # í•™ìŠµ ì „ ëœë¤ ë¹„ë””ì˜¤
    print("\nğŸ¬ [í•™ìŠµ ì „] ìˆ˜ì •ëœ ë¬´ì‘ìœ„ í–‰ë™ ì—í”¼ì†Œë“œ ë…¹í™”!")
    training_callback.record_stage_videos("0_random")
    
    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸš€ ìˆ˜ì •ëœ {config.algorithm} í•™ìŠµ ì‹œì‘!")
    print("=" * 60)
    print("ğŸ’¡ Eval í™˜ê²½ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ë¹„ë””ì˜¤!")
    print("ğŸ’¡ VecNormalize í†µê³„ ë™ê¸°í™”!")
    print("ğŸ’¡ ì‹¤ì œ info['is_success'] ê¸°ì¤€ ì‚¬ìš©!")
    print("ğŸ’¡ ì„±ê³µ ì—í”¼ì†Œë“œ ìš°ì„  ë…¹í™”!")
    print("ğŸ’¡ Best Model ê¸°ë°˜ ë¹„êµ!")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=config.total_timesteps,
            callback=callbacks,
            log_interval=10,
            progress_bar=True
        )
    except KeyboardInterrupt:
        print("\nâ¸ï¸  í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    end_time = time.time()
    training_time = end_time - start_time
    
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"â±ï¸  ì´ í•™ìŠµ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = os.path.join(config.model_dir, f"{config.algorithm}_fixed_{config.experiment_name}_final")
    model.save(final_model_path)
    print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
    
    # ìµœì¢… í‰ê°€
    print("\nğŸ” ìµœì¢… í‰ê°€...")
    mean_reward, std_reward = evaluate_policy(
        model, eval_env, 
        n_eval_episodes=50,
        deterministic=True
    )
    print(f"ğŸ† ìµœì¢… í‰ê°€ ê²°ê³¼: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    return model, training_callback

def main():
    """ğŸ¯ ìˆ˜ì •ëœ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print(f"ğŸš€ ì™„ì „íˆ ìˆ˜ì •ëœ ì—í”¼ì†Œë“œ ê¸°ë°˜ ë¹„ë””ì˜¤ í•™ìŠµ ì‹œì‘!")
    print(f"ğŸ¯ ëª©í‘œ: FrankaSlideDense íƒœìŠ¤í¬ ë§ˆìŠ¤í„°í•˜ê¸°")
    print(f"ğŸ”§ ì£¼ìš” ìˆ˜ì •ì‚¬í•­:")
    print(f"   âœ… Eval í™˜ê²½ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ë¹„ë””ì˜¤ í™˜ê²½")
    print(f"   âœ… VecNormalize í†µê³„ ë™ê¸°í™”")
    print(f"   âœ… ì‹¤ì œ info['is_success'] ê¸°ì¤€ ì‚¬ìš©")
    print(f"   âœ… ì„±ê³µ ì—í”¼ì†Œë“œ ìš°ì„  ë…¹í™”")
    print(f"   âœ… Best Model ê¸°ë°˜ ë¹„êµ ì‹œìŠ¤í…œ")
    print(f"   âœ… ê° ë‹¨ê³„ë³„ {config.episodes_per_stage}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   âœ… ë³´ìƒ ìŠ¤ì¼€ì¼ë§: {config.reward_scale} (ì•ˆì •ì  í•™ìŠµ)")
    print("=" * 60)
    
    # í•™ìŠµ ì‹¤í–‰
    model, training_callback = train_model()
    
    # ìµœì¢… ë¹„ë””ì˜¤ ë…¹í™”
    record_final_videos(model, training_callback)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ì™„ì „íˆ ìˆ˜ì •ëœ í•™ìŠµ ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… ì„±ê³µë¥ : {training_callback.recent_success_rate:.3f}")
    print(f"ğŸ† ìµœê³  ë³´ìƒ: {training_callback.best_reward:.2f}")
    print(f"ğŸ¥ ìˆ˜ì •ëœ ì—í”¼ì†Œë“œë³„ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print(f"ğŸ“ ë¹„ë””ì˜¤ ì €ì¥ ìœ„ì¹˜: {config.video_dir}")
    print("ğŸ“‹ ìƒì„±ëœ ê²°ê³¼:")
    print("   âœ… 0_random/: í•™ìŠµ ì „ ë¬´ì‘ìœ„ í–‰ë™ (ìˆ˜ì •ë¨)")
    print("   âœ… 1_20percent/: 20% í•™ìŠµ ì§„í–‰ ì‹œì  (Eval ì¼ì¹˜)")
    print("   âœ… 2_40percent/: 40% í•™ìŠµ ì§„í–‰ ì‹œì  (Eval ì¼ì¹˜)")
    print("   âœ… 3_60percent/: 60% í•™ìŠµ ì§„í–‰ ì‹œì  (Eval ì¼ì¹˜)") 
    print("   âœ… 4_80percent/: 80% í•™ìŠµ ì§„í–‰ ì‹œì  (Eval ì¼ì¹˜)")
    print("   âœ… 5_100percent/: 100% í•™ìŠµ ì™„ë£Œ ì‹œì  (Eval ì¼ì¹˜)")
    print("   âœ… highlights/: ìµœê³  ë³´ìƒ ì—í”¼ì†Œë“œë“¤ (ì‹¤ì œ ì„±ê³µ ê¸°ì¤€)")
    print("   âœ… model_comparison/: Best vs Final ëª¨ë¸ ë¹„êµ (ìˆ˜ì •ë¨)")
    print(f"ğŸ“Š ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„: {config.results_dir}/model_comparison/")
    print("ğŸ¯ ì´ì œ ë¹„ë””ì˜¤ê°€ ì‹¤ì œ í•™ìŠµ ì„±ëŠ¥ì„ ì •í™•íˆ ë°˜ì˜í•©ë‹ˆë‹¤!")
    print("=" * 60)
    
    return model, training_callback

if __name__ == "__main__":
    model, training_callback = main()
