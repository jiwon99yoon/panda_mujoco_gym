# train ì½”ë“œ ìˆ˜ì • í•„ìš” : ë³‘ë ¬ í™˜ê²½ì— ë§ê²Œë” ì¡°ì • í•„ìš” : í•™ìŠµ ì˜ ì•ˆë¨ (í˜„ì¬)
# # #/home/minjun/panda_mujoco_gym/train_sac_highlight4.py
# # /home/dyros/panda_mujoco_gym/train_sac_highlight4.py

#!/usr/bin/env python3
"""
ğŸ¯ ì™„ì „íˆ ìˆ˜ì •ëœ ì—í”¼ì†Œë“œ ê¸°ë°˜ ë¹„ë””ì˜¤ ë…¹í™” ì‹œìŠ¤í…œ
- í•™ìŠµ í™˜ê²½ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ë¹„ë””ì˜¤ í™˜ê²½ (VecNormalize í¬í•¨)
- ì‹¤ì œ info['is_success'] ê¸°ì¤€ ì‚¬ìš©
- Eval í™˜ê²½ê³¼ ë™ì¼í•œ ì¡°ê±´ìœ¼ë¡œ ë¹„ë””ì˜¤ ì œì‘
- Best Model ê¸°ë°˜ ë¹„êµ ë¹„ë””ì˜¤
- ì„±ê³µ ì—í”¼ì†Œë“œ ìš°ì„  ë…¹í™”
"""

import os
import sys
import time
import numpy as np
import cv2
import json
import csv
from datetime import datetime
from typing import List, Dict, Optional
import gymnasium as gym
import torch
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback
from stable_baselines3.common.logger import configure
from stable_baselines3.common.noise import NormalActionNoise

import pickle

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ì‚¬ìš©ì ì •ì˜ í™˜ê²½ ë“±ë¡
import panda_mujoco_gym

print("ğŸ¯ ì™„ì „íˆ ìˆ˜ì •ëœ ì—í”¼ì†Œë“œ ê¸°ë°˜ ë¹„ë””ì˜¤ ë…¹í™” ì‹œìŠ¤í…œ!")
print("=" * 60)

# ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ë˜í¼
class RewardScalingWrapper(gym.Wrapper):
    def __init__(self, env, scale=0.1):
        super().__init__(env)
        self.scale = scale
        self.episode_reward = 0
        
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        scaled_reward = reward * self.scale
        self.episode_reward += scaled_reward
        
        if terminated or truncated:
            info['original_reward'] = self.episode_reward / self.scale
            info['scaled_reward'] = self.episode_reward
            self.episode_reward = 0
            
        return obs, scaled_reward, terminated, truncated, info
    
    def reset(self, **kwargs):
        self.episode_reward = 0
        return self.env.reset(**kwargs)

# ì„±ê³µë¥  ì¶”ì  ë˜í¼
class SuccessTrackingWrapper(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)
        self.success_count = 0
        self.episode_count = 0
        
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        if terminated or truncated:
            self.episode_count += 1
            if info.get('is_success', False):
                self.success_count += 1
            
            info['success_rate'] = self.success_count / self.episode_count if self.episode_count > 0 else 0
            
        return obs, reward, terminated, truncated, info

# ğŸ¥ ê°œì„ ëœ ì—í”¼ì†Œë“œë³„ ë¹„ë””ì˜¤ ë…¹í™” í´ë˜ìŠ¤
class FixedEpisodeVideoRecorder:
    """
    ìˆ˜ì •ëœ ì—í”¼ì†Œë“œë³„ ê°œë³„ ë¹„ë””ì˜¤ ë…¹í™” ì‹œìŠ¤í…œ
    - VecNormalize ìƒíƒœ ë™ê¸°í™”
    - ì‹¤ì œ info['is_success'] ê¸°ì¤€ ì‚¬ìš©
    - ì„±ê³µ ì—í”¼ì†Œë“œ ìš°ì„  ë…¹í™”
    """
    
    def __init__(self, save_dir: str, fps: int = 30):
        self.save_dir = save_dir
        self.fps = fps
        self.current_episode_frames = []
        self.episode_metadata = []
        
        os.makedirs(save_dir, exist_ok=True)
    
    def start_episode_recording(self):
        """ìƒˆ ì—í”¼ì†Œë“œ ë…¹í™” ì‹œì‘"""
        self.current_episode_frames = []
        
    def add_frame(self, frame: np.ndarray):
        """í˜„ì¬ ì—í”¼ì†Œë“œì— í”„ë ˆì„ ì¶”ê°€"""
        if frame is not None:
            self.current_episode_frames.append(frame.copy())

    # 3. FixedEpisodeVideoRecorderì˜ end_episode_recording ë©”ì„œë“œ ìˆ˜ì •
    def end_episode_recording(self, episode_info: Dict) -> Optional[str]:
        """ì—í”¼ì†Œë“œ ë…¹í™” ì¢…ë£Œ ë° ë¹„ë””ì˜¤ ì €ì¥ - ê°œì„ ëœ ë©”íƒ€ë°ì´í„°"""
        if not self.current_episode_frames:
            return None
            
        # íŒŒì¼ëª… ìƒì„±
        stage = episode_info.get('stage', 'unknown')
        episode_id = episode_info.get('episode_id', 0)
        success_str = 'SUCCESS' if episode_info.get('success', False) else 'FAIL'
        task_str = '_TASK_COMPLETE' if episode_info.get('task_completed', False) else ''
        reward = episode_info.get('reward', 0)
        
        base_filename = f"{stage}_ep{episode_id:03d}_{success_str}{task_str}_reward{reward:.1f}"
        
        # ë®ì–´ì“°ê¸° ë°©ì§€
        version = 1
        while True:
            if version == 1:
                filename = f"{base_filename}.mp4"
            else:
                filename = f"{base_filename}_v{version}.mp4"
            
            video_path = os.path.join(self.save_dir, filename)
            
            if not os.path.exists(video_path):
                break
            
            version += 1
            if version > 100:
                print(f"âš ï¸ ê²½ê³ : {base_filename}ì˜ ë²„ì „ì´ 100ê°œë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤!")
                break

        # ë¹„ë””ì˜¤ ì €ì¥
        success = self._save_video(video_path, self.current_episode_frames)
        
        if success:
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            episode_info['video_path'] = video_path
            episode_info['frame_count'] = len(self.current_episode_frames)
            episode_info['fps'] = self.fps
            episode_info['duration'] = len(self.current_episode_frames) / self.fps
            episode_info['version'] = version
            self.episode_metadata.append(episode_info)
            
            print(f"âœ… ì—í”¼ì†Œë“œ ë¹„ë””ì˜¤ ì €ì¥: {filename}")
            print(f"   í”„ë ˆì„ ìˆ˜: {len(self.current_episode_frames)}, ì„±ê³µ: {success_str}, ë³´ìƒ: {reward:.2f}")
            print(f"   ì¬ìƒ ì‹œê°„: {episode_info['duration']:.1f}ì´ˆ, íƒœìŠ¤í¬ ì™„ë£Œ: {episode_info.get('task_completed', False)}")
            if 'total_frames' in episode_info:
                print(f"   ì›ë³¸ í”„ë ˆì„: {episode_info['total_frames']}, ì €ì¥ í”„ë ˆì„: {len(self.current_episode_frames)}")
            if version > 1:
                print(f"   ğŸ“Œ ë²„ì „: v{version} (ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸° ë°©ì§€)")
            
            return video_path
        
        return None
    
    def _save_video(self, video_path: str, frames: List[np.ndarray]) -> bool:
        """í”„ë ˆì„ë“¤ì„ ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥ - ê°œì„ ëœ ì¸ì½”ë”©"""
        if not frames:
            return False
            
        try:
            height, width = frames[0].shape[:2]
            
            # ğŸ¯ ë” ë‚˜ì€ ì½”ë± ì‚¬ìš© (H.264)
            fourcc = cv2.VideoWriter_fourcc(*'H264')
            # ëŒ€ì•ˆ: fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            
            writer = cv2.VideoWriter(video_path, fourcc, self.fps, (width, height))
            
            if not writer.isOpened():
                # H264 ì‹¤íŒ¨ ì‹œ mp4vë¡œ fallback
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                writer = cv2.VideoWriter(video_path, fourcc, self.fps, (width, height))
            
            # í”„ë ˆì„ ì €ì¥ with progress
            total_frames = len(frames)
            for i, frame in enumerate(frames):
                # RGB â†’ BGR ë³€í™˜
                if len(frame.shape) == 3 and frame.shape[2] == 3:
                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                else:
                    frame_bgr = frame
                
                writer.write(frame_bgr)
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ (ê¸´ ë¹„ë””ì˜¤ì˜ ê²½ìš°)
                if total_frames > 300 and i % 100 == 0:
                    progress = (i / total_frames) * 100
                    print(f"     ë¹„ë””ì˜¤ ì¸ì½”ë”© ì§„í–‰: {progress:.1f}%", end='\r')
            
            writer.release()
            
            # íŒŒì¼ ìƒì„± í™•ì¸
            if os.path.exists(video_path) and os.path.getsize(video_path) > 0:
                print(f"     ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {total_frames} í”„ë ˆì„")
                return True
            else:
                print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {video_path}")
                return False
            
        except Exception as e:
            print(f"âŒ ë¹„ë””ì˜¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

# ğŸ¯ ìˆ˜ì •ëœ í•™ìŠµ ì§„í–‰ë¥ ë³„ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ
class FixedStageBasedVideoSystem:
    """
    ìˆ˜ì •ëœ í•™ìŠµ ì§„í–‰ë¥ ë³„ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ
    - í•™ìŠµ í™˜ê²½ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ë¹„ë””ì˜¤ í™˜ê²½
    - VecNormalize ìƒíƒœ ë™ê¸°í™”
    - ì‹¤ì œ ì„±ê³µ ê¸°ì¤€ ì‚¬ìš©
    - ì„±ê³µ ì—í”¼ì†Œë“œ ìš°ì„  ë…¹í™”
    """
    
    def __init__(self, base_dir: str, total_timesteps: int):
        self.base_dir = base_dir
        self.total_timesteps = total_timesteps
        
        # 6ë‹¨ê³„ ì •ì˜
        self.stages = {
            '0_random': 0,                           # í•™ìŠµ ì „
            '1_20percent': total_timesteps // 5,     # 20%
            '2_40percent': total_timesteps * 2 // 5, # 40%
            '3_60percent': total_timesteps * 3 // 5, # 60% 
            '4_80percent': total_timesteps * 4 // 5, # 80%
            '5_100percent': total_timesteps          # 100%
        }
        
        self.completed_stages = set()
        self.stage_videos = {}  # stage_name -> [video_paths]
        self.best_episodes = []  # ìµœê³  ë³´ìƒ ì—í”¼ì†Œë“œë“¤
        
        # ê° ë‹¨ê³„ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        for stage_name in self.stages.keys():
            stage_dir = os.path.join(base_dir, stage_name)
            os.makedirs(stage_dir, exist_ok=True)
            self.stage_videos[stage_name] = []
    
    def should_record_stage(self, current_timestep: int) -> Optional[str]:
        """í˜„ì¬ timestepì—ì„œ ë…¹í™”í•  ë‹¨ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        for stage_name, target_step in self.stages.items():
            if (stage_name not in self.completed_stages and 
                current_timestep >= target_step):
                return stage_name
        return None

    # 4. FixedStageBasedVideoSystemì˜ record_stage_episodes ë©”ì„œë“œ ìˆ˜ì •
    def record_stage_episodes(self, stage_name: str, model, eval_env_stats, num_episodes: int = 3):
        """
        ğŸ¯ ìˆ˜ì •ëœ ì—í”¼ì†Œë“œ ë…¹í™” - Eval í™˜ê²½ê³¼ ì™„ì „íˆ ì¼ì¹˜
        
        Args:
            stage_name: ë‹¨ê³„ ì´ë¦„
            model: í•™ìŠµëœ ëª¨ë¸
            eval_env_stats: Eval í™˜ê²½ì˜ VecNormalize í†µê³„
            num_episodes: ë…¹í™”í•  ì—í”¼ì†Œë“œ ìˆ˜
        """
        print(f"\nğŸ¬ [{stage_name}] ì—í”¼ì†Œë“œ ë…¹í™” ì‹œì‘! (Eval í™˜ê²½ ê¸°ì¤€)")
        
        stage_dir = os.path.join(self.base_dir, stage_name)
        recorder = FixedEpisodeVideoRecorder(stage_dir)
        
        # ğŸ”§ Eval í™˜ê²½ê³¼ ë™ì¼í•œ ë¹„ë””ì˜¤ í™˜ê²½ ìƒì„±
        video_env = self._create_eval_identical_env(eval_env_stats)
        
        episode_results = []
        successful_episodes = []
        failed_episodes = []
        attempts = 0
        max_attempts = num_episodes * 15  # ì„±ê³µ ì—í”¼ì†Œë“œë¥¼ ìœ„í•´ ë” ë§ì€ ì‹œë„ (10->15)


        # ------------------------------- ì¶”ê°€ ---------------------  #
        print(f"  ëª©í‘œ: ì„±ê³µ ì—í”¼ì†Œë“œ {num_episodes}ê°œ ìˆ˜ì§‘ (ìµœëŒ€ {max_attempts}íšŒ ì‹œë„)")
        
        while len(episode_results) < num_episodes and attempts < max_attempts:
            attempts += 1
            
            result = self._record_single_episode(
                recorder, model, video_env, attempts - 1, stage_name
            )
            
            if result:
                if result['task_completed']:
                    successful_episodes.append(result)
                    print(f"    âœ… íƒœìŠ¤í¬ ì™„ë£Œ ì—í”¼ì†Œë“œ ìˆ˜ì§‘! (ì„±ê³µ: {len(successful_episodes)}ê°œ)")
                else:
                    failed_episodes.append(result)
                    print(f"    âŒ íƒœìŠ¤í¬ ì‹¤íŒ¨ ì—í”¼ì†Œë“œ (ì‹¤íŒ¨: {len(failed_episodes)}ê°œ)")
                
                # ì„±ê³µ ì—í”¼ì†Œë“œ ìš°ì„ , ë¶€ì¡±í•˜ë©´ ì‹¤íŒ¨ ì—í”¼ì†Œë“œë¡œ ì±„ì›€
                if len(successful_episodes) >= num_episodes:
                    episode_results = successful_episodes[:num_episodes]
                    break
                elif len(successful_episodes) + len(failed_episodes) >= num_episodes:
                    episode_results = successful_episodes + failed_episodes[:num_episodes - len(successful_episodes)]
                    break
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if attempts % 5 == 0:
                print(f"    ì‹œë„ {attempts}/{max_attempts}: ì„±ê³µ {len(successful_episodes)}ê°œ, ì‹¤íŒ¨ {len(failed_episodes)}ê°œ")
        
        # ìµœì¢… ì—í”¼ì†Œë“œ ê²°ê³¼ ì²˜ë¦¬
        for result in episode_results:
            self.stage_videos[stage_name].append(result['video_path'])
            
            # ìµœê³  ë³´ìƒ ì¶”ì 
            if not self.best_episodes or result['reward'] > min(ep['reward'] for ep in self.best_episodes):
                self.best_episodes.append(result)
                self.best_episodes.sort(key=lambda x: x['reward'], reverse=True)
                self.best_episodes = self.best_episodes[:10]  # ìƒìœ„ 10ê°œë§Œ ìœ ì§€
        
        self.completed_stages.add(stage_name)
        
        # ê²°ê³¼ ìš”ì•½
        task_completed_count = sum(1 for r in episode_results if r.get('task_completed', False))
        print(f"âœ… [{stage_name}] ì™„ë£Œ! {len(episode_results)}ê°œ ì—í”¼ì†Œë“œ ë…¹í™”ë¨")
        print(f"   íƒœìŠ¤í¬ ì™„ë£Œ: {task_completed_count}/{len(episode_results)}")
        if episode_results:
            avg_reward = sum(r['reward'] for r in episode_results) / len(episode_results)
            avg_duration = sum(r.get('frame_count', 0) / config.video_fps for r in episode_results) / len(episode_results)
            print(f"   í‰ê·  ë³´ìƒ: {avg_reward:.2f}")
            print(f"   í‰ê·  ì¬ìƒ ì‹œê°„: {avg_duration:.1f}ì´ˆ")
        
        return episode_results
    
    def _create_eval_identical_env(self, eval_env_stats):
        """
        ğŸ¯ Eval í™˜ê²½ê³¼ ì™„ì „íˆ ë™ì¼í•œ ë¹„ë””ì˜¤ í™˜ê²½ ìƒì„±
        """
        # ê¸°ë³¸ í™˜ê²½ ìƒì„± (í•™ìŠµ í™˜ê²½ê³¼ ë™ì¼í•œ ë˜í¼ë“¤ ì ìš©)
        env = gym.make(config.env_name, render_mode="rgb_array")
        env = Monitor(env)
        
        # ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ì ìš©
        if config.reward_scale != 1.0:
            env = RewardScalingWrapper(env, scale=config.reward_scale)
        
        # ì„±ê³µë¥  ì¶”ì  ì¶”ê°€
        env = SuccessTrackingWrapper(env)
        
        # ë²¡í„°í™”
        vec_env = DummyVecEnv([lambda: env])
        
        # ğŸ”§ VecNormalize ì ìš© ë° í†µê³„ ë™ê¸°í™”
        if config.normalize_env and eval_env_stats is not None:
            vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)
            # Eval í™˜ê²½ì˜ ì •ê·œí™” í†µê³„ ë³µì‚¬
            vec_env.obs_rms = eval_env_stats['obs_rms']
            vec_env.ret_rms = eval_env_stats['ret_rms']
            vec_env.training = False  # í‰ê°€ ëª¨ë“œ
        
        return vec_env

# 2. FixedStageBasedVideoSystemì˜ _record_single_episode ë©”ì„œë“œ ê°œì„ 
    def _record_single_episode(self, recorder, model, video_env, episode_idx, stage_name):
        """ë‹¨ì¼ ì—í”¼ì†Œë“œ ë…¹í™” - ê°œì„ ëœ í”„ë ˆì„ ìº¡ì²˜"""
        recorder.start_episode_recording()
        
        obs = video_env.reset()
        total_reward = 0
        step_count = 0
        done = False
        task_completed = False
        
        # í”„ë ˆì„ ìº¡ì²˜ë¥¼ ìœ„í•œ ë³€ìˆ˜
        frame_buffer = []  # ëª¨ë“  í”„ë ˆì„ì„ ë²„í¼ì— ì €ì¥
        action_history = []  # ì•¡ì…˜ íˆìŠ¤í† ë¦¬ ì €ì¥ (ë””ë²„ê¹…ìš©)
        
        # ì´ˆê¸° ìƒíƒœ ìº¡ì²˜ (ì—¬ëŸ¬ í”„ë ˆì„)
        for _ in range(5):  # ì´ˆê¸° 0.25ì´ˆ ì •ë„ ìº¡ì²˜
            rendered = video_env.render()
            if rendered is not None:
                img = rendered[0] if isinstance(rendered, (list, tuple)) else rendered
                frame_buffer.append(img)
        
        while not done:
            # ì•¡ì…˜ ì˜ˆì¸¡
            if model and hasattr(model, 'predict'):
                action, _ = model.predict(obs, deterministic=True)
            else:
                action = video_env.action_space.sample()
            
            action_history.append(action)
            
            # í™˜ê²½ ìŠ¤í…
            obs, rewards, dones, infos = video_env.step(action)
            done = dones[0]
            total_reward += rewards[0]
            step_count += 1
            
            # ğŸ¯ ë§¤ ìŠ¤í…ë§ˆë‹¤ í”„ë ˆì„ ìº¡ì²˜ (í”„ë ˆì„ ëˆ„ë½ ë°©ì§€)
            rendered = video_env.render()
            if rendered is not None:
                img = rendered[0] if isinstance(rendered, (list, tuple)) else rendered
                frame_buffer.append(img)
            
            # íƒœìŠ¤í¬ ì„±ê³µ ì²´í¬
            if infos[0].get('is_success', False):
                task_completed = True
                
                # ğŸ¯ ì„±ê³µ í›„ ì§§ì€ ì¶”ê°€ ë…¹í™” (0.5ì´ˆ)
                for _ in range(config.success_extra_frames):
                    rendered = video_env.render()
                    if rendered is not None:
                        img = rendered[0] if isinstance(rendered, (list, tuple)) else rendered
                        frame_buffer.append(img)
                done = True
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if step_count > config.max_episode_steps:
                break
        
        # ğŸ¯ í”„ë ˆì„ ì„œë¸Œìƒ˜í”Œë§ (í•„ìš”í•œ ê²½ìš°)
        # ë„ˆë¬´ ë§ì€ í”„ë ˆì„ì´ë©´ ì ì ˆíˆ ìƒ˜í”Œë§
        if len(frame_buffer) > 600:  # 20ì´ˆ ì´ìƒì´ë©´
            # ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§í•˜ë˜, ì¤‘ìš”í•œ ìˆœê°„ì€ ë³´ì¡´
            sampled_frames = self._smart_frame_sampling(frame_buffer, target_frames=600)
            recorder.current_episode_frames = sampled_frames
        else:
            # ëª¨ë“  í”„ë ˆì„ ì‚¬ìš©
            recorder.current_episode_frames = frame_buffer
        
        # ì—í”¼ì†Œë“œ ì •ë³´
        episode_info = {
            'episode_id': episode_idx,
            'reward': total_reward,
            'length': step_count,
            'success': task_completed,
            'stage': stage_name,
            'task_completed': task_completed,
            'total_frames': len(frame_buffer),
            'action_count': len(action_history)
        }
        
        # ë¹„ë””ì˜¤ ì €ì¥
        video_path = recorder.end_episode_recording(episode_info)
        
        if video_path:
            episode_info['video_path'] = video_path
            return episode_info
        
        return None
    
    def _smart_frame_sampling(self, frames, target_frames=600):
        """ìŠ¤ë§ˆíŠ¸ í”„ë ˆì„ ìƒ˜í”Œë§ - ì¤‘ìš”í•œ ìˆœê°„ ë³´ì¡´"""
        if len(frames) <= target_frames:
            return frames
        
        # ì‹œì‘ê³¼ ë ë¶€ë¶„ì€ ë” ë§ì´ ë³´ì¡´
        start_frames = frames[:60]  # ì²˜ìŒ 2ì´ˆ
        end_frames = frames[-60:]   # ë§ˆì§€ë§‰ 2ì´ˆ
        
        # ì¤‘ê°„ ë¶€ë¶„ì€ ê· ë“± ìƒ˜í”Œë§
        middle_frames = frames[60:-60]
        middle_target = target_frames - 120
        
        if len(middle_frames) > middle_target:
            # ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
            indices = np.linspace(0, len(middle_frames)-1, middle_target, dtype=int)
            middle_sampled = [middle_frames[i] for i in indices]
        else:
            middle_sampled = middle_frames
        
        return start_frames + middle_sampled + end_frames
    
    def create_highlight_video(self):
        """ìµœê³  ë³´ìƒ ì—í”¼ì†Œë“œë“¤ë¡œ í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ìƒì„± (ë®ì–´ì“°ê¸° ë°©ì§€ ì¶”ê°€)"""
        if not self.best_episodes:
            print("âŒ í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: ì—í”¼ì†Œë“œ ì—†ìŒ")
            return
        
        print(f"\nğŸŒŸ í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ìƒì„± ì¤‘... (ìƒìœ„ {len(self.best_episodes)}ê°œ ì—í”¼ì†Œë“œ)")
        
        highlight_dir = os.path.join(self.base_dir, "highlights")
        os.makedirs(highlight_dir, exist_ok=True)
        
        # ê° ìµœê³  ì—í”¼ì†Œë“œë¥¼ í•˜ì´ë¼ì´íŠ¸ í´ë”ì— ë³µì‚¬
        import shutil
        for i, episode in enumerate(self.best_episodes):
            original_path = episode['video_path']
            if os.path.exists(original_path):
                base_filename = f"highlight_{i+1:02d}_reward{episode['reward']:.1f}_{episode['stage']}"
                
                # ğŸ”§ ë®ì–´ì“°ê¸° ë°©ì§€: íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë²„ì „ ë²ˆí˜¸ ì¶”ê°€
                version = 1
                while True:
                    if version == 1:
                        highlight_filename = f"{base_filename}.mp4"
                    else:
                        highlight_filename = f"{base_filename}_v{version}.mp4"
                    
                    highlight_path = os.path.join(highlight_dir, highlight_filename)
                    
                    # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì´ ì´ë¦„ìœ¼ë¡œ ì €ì¥
                    if not os.path.exists(highlight_path):
                        break
                    
                    version += 1
                    
                    # ì•ˆì „ì¥ì¹˜
                    if version > 100:
                        print(f"âš ï¸ ê²½ê³ : {base_filename}ì˜ ë²„ì „ì´ 100ê°œë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤!")
                        break
                
                # íŒŒì¼ ë³µì‚¬
                shutil.copy2(original_path, highlight_path)
                print(f"  âœ¨ {highlight_filename}")
                if version > 1:
                    print(f"     ğŸ“Œ ë²„ì „: v{version}")
        
        print(f"âœ… í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ì™„ë£Œ! {highlight_dir}")    

# ì„¤ì • í´ë˜ìŠ¤
class FixedConfig:
    # í™˜ê²½ ì„¤ì •
    env_name = "FrankaSlideDense-v0"
    algorithm = "SAC"
    total_timesteps = 1_000_000
    
    # SAC í•˜ì´í¼íŒŒë¼ë¯¸í„°
    learning_rate = 1e-4
    buffer_size = 1_000_000
    learning_starts = 10_000
    batch_size = 512
    tau = 0.01
    gamma = 0.98
    train_freq = 4
    gradient_steps = 4
    #gpu ì‘ì—…ëŸ‰ ì¦ê°€
    # train_freq, gradient_steps ì¦ê°€ ê°ê° 8, 16
    
    # ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
    policy_kwargs = {
        "net_arch": [256, 256, 128],
        "activation_fn": torch.nn.ReLU,
        "normalize_images": False
    }
    
    # íƒí—˜ ë…¸ì´ì¦ˆ
    action_noise_std = 0.2
    
    # í‰ê°€ ì„¤ì •
    eval_freq = 20_000
    n_eval_episodes = 20
    eval_deterministic = True
    
    # ì €ì¥ ì„¤ì •
    save_freq = 100_000
    
    # ğŸ¥ ìˆ˜ì •ëœ ë¹„ë””ì˜¤ ì„¤ì •
    episodes_per_stage = 3         # ê° ë‹¨ê³„ë³„ ì—í”¼ì†Œë“œ ìˆ˜
    video_fps = 100                 # ë¹„ë””ì˜¤ FPS -> MUJOCOê°€ 0.01 dtë¡œ ì‹¤í–‰ë˜ëŠ” ê²ƒ ë°˜ì˜í•˜ì—¬ 100ìœ¼ë¡œ ì¼ë‹¨ ì„¤ì •í•´ë´„
    recording_fps = 120            # ë…¹í™” ì‹œ í”„ë ˆì„ ìº¡ì²˜ ë¹ˆë„ (ë” ë§ì€ í”„ë ˆì„ ìº¡ì²˜)
    max_episode_steps = 5000       # ì—í”¼ì†Œë“œ ìµœëŒ€ ìŠ¤í… ìˆ˜ ì¦ê°€ (1000 â†’ 2000)
    success_extra_frames = 100      # ì„±ê³µ í›„ ì¶”ê°€ í”„ë ˆì„ ìˆ˜ (0.5ì´ˆ @ 20fps)
    # í™˜ê²½ ê°œì„  ì„¤ì •
    normalize_env = True
    reward_scale = 0.1
    
    # ì¶”ê°€ í•™ìŠµ ê¸°ë²•
    use_sde = True
    sde_sample_freq = 4
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    base_dir = "fixed_episode_videos"
    model_dir = os.path.join(base_dir, "models")
    results_dir = os.path.join(base_dir, "training_results")
    video_dir = os.path.join(base_dir, "videos")
    log_dir = os.path.join(base_dir, "logs")
    
    # ì‹¤í—˜ ì´ë¦„
    experiment_name = f"{env_name}_{algorithm}_fixed_videos_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

config = FixedConfig()

def create_directories():
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    dirs_to_create = [
        config.base_dir,
        config.model_dir,
        config.results_dir,
        config.video_dir,
        config.log_dir
    ]
    
    for dir_path in dirs_to_create:
        os.makedirs(dir_path, exist_ok=True)
        print(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {dir_path}")

# ğŸ¯ ìˆ˜ì •ëœ í•™ìŠµ ì½œë°±
class FixedTrainingCallback(BaseCallback):
    """
    ìˆ˜ì •ëœ í•™ìŠµ ì½œë°± - Eval í™˜ê²½ê³¼ ì¼ì¹˜í•˜ëŠ” ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ
    """
    def __init__(self, log_dir, verbose=0):
        super(FixedTrainingCallback, self).__init__(verbose)
        self.log_dir = log_dir
        self.episode_rewards = []
        self.episode_lengths = []
        self.success_count = 0
        self.episode_count = 0
        self.csv_file = os.path.join(log_dir, f"training_log_{config.experiment_name}.csv")
        
        # ìˆ˜ì •ëœ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.video_system = FixedStageBasedVideoSystem(
            base_dir=config.video_dir,
            total_timesteps=config.total_timesteps
        )
        
        # ì„±ëŠ¥ ì¶”ì 
        self.best_reward = float('-inf')
        self.recent_rewards = []
        self.recent_success_rate = 0.0
        
        # ğŸ”§ Eval í™˜ê²½ í†µê³„ ì €ì¥ìš©
        self.eval_env_stats = None
        
        # CSV íŒŒì¼ ì´ˆê¸°í™”
        with open(self.csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Timestep', 'Episode', 'Reward', 'Length', 'Success', 'Success_Rate', 'Best_Reward'])
    
    def set_eval_env_stats(self, eval_env):
        """Eval í™˜ê²½ì˜ ì •ê·œí™” í†µê³„ ì €ì¥"""
        if hasattr(eval_env, 'obs_rms') and hasattr(eval_env, 'ret_rms'):
            self.eval_env_stats = {
                'obs_rms': eval_env.obs_rms,
                'ret_rms': eval_env.ret_rms
            }
            #print("âœ… Eval í™˜ê²½ í†µê³„ ì €ì¥ ì™„ë£Œ!")
    
    def _on_step(self) -> bool:
        # ë‹¨ê³„ë³„ ë¹„ë””ì˜¤ ë…¹í™” ì²´í¬
        stage_to_record = self.video_system.should_record_stage(self.num_timesteps)
        if stage_to_record:
            print(f"\nğŸ¬ [{stage_to_record}] ìˆ˜ì •ëœ ë¹„ë””ì˜¤ ë…¹í™”! (Step {self.num_timesteps})")
            if self.recent_rewards:
                print(f"   í˜„ì¬ í‰ê·  ë³´ìƒ: {np.mean(self.recent_rewards[-50:]):.2f}")
                print(f"   í˜„ì¬ ì„±ê³µë¥ : {self.recent_success_rate:.3f}")
            
            self.record_stage_videos(stage_to_record)
        
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì‹œ ì²˜ë¦¬
        if len(self.locals.get('dones', [])) > 0 and self.locals.get('dones', [False])[0]:
            info = self.locals.get('infos', [{}])[0]
            
            if 'episode' in info:
                episode_reward = info['episode']['r']
                episode_length = info['episode']['l']
                is_success = info.get('is_success', False)
                
                self.episode_rewards.append(episode_reward)
                self.episode_lengths.append(episode_length)
                self.recent_rewards.append(episode_reward)
                self.episode_count += 1
                
                if is_success:
                    self.success_count += 1
                
                # ì„±ê³µë¥  ê³„ì‚°
                self.recent_success_rate = self.success_count / self.episode_count if self.episode_count > 0 else 0
                
                # ìµœê·¼ 100ê°œ ì—í”¼ì†Œë“œë§Œ ìœ ì§€
                if len(self.recent_rewards) > 100:
                    self.recent_rewards.pop(0)
                
                # ìµœê³  ë³´ìƒ ì—…ë°ì´íŠ¸
                if episode_reward > self.best_reward:
                    self.best_reward = episode_reward
                    print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ë³´ìƒ! {episode_reward:.2f} (ì„±ê³µë¥ : {self.recent_success_rate:.3f})")
                
                # ë¡œê¹…
                if self.episode_count % 10 == 0:
                    avg_reward = np.mean(self.recent_rewards[-50:]) if len(self.recent_rewards) >= 50 else np.mean(self.recent_rewards)
                    avg_length = np.mean(self.episode_lengths[-50:]) if len(self.episode_lengths) >= 50 else np.mean(self.episode_lengths)
                    
                    print(f"ğŸ“Š Episode {self.episode_count:4d} | "
                          f"Reward: {episode_reward:7.2f} | "
                          f"Avg: {avg_reward:7.2f} | "
                          f"Length: {episode_length:3.0f} | "
                          f"Success: {self.recent_success_rate:.3f}")
                
                # CSV ë¡œê·¸ ì €ì¥
                with open(self.csv_file, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        self.num_timesteps,
                        self.episode_count,
                        episode_reward,
                        episode_length,
                        is_success,
                        self.recent_success_rate,
                        self.best_reward
                    ])
        
        return True
    
    def record_stage_videos(self, stage_name):
        """ğŸ¯ ìˆ˜ì •ëœ ë‹¨ê³„ë³„ ë¹„ë””ì˜¤ ë…¹í™”"""
        try:
            # ğŸ”§ Eval í™˜ê²½ í†µê³„ì™€ í•¨ê»˜ ë¹„ë””ì˜¤ ë…¹í™”
            results = self.video_system.record_stage_episodes(
                stage_name=stage_name,
                model=self.model if hasattr(self, 'model') else None,
                eval_env_stats=self.eval_env_stats,
                num_episodes=config.episodes_per_stage
            )
            
            print(f"âœ… [{stage_name}] ìˆ˜ì •ëœ ë¹„ë””ì˜¤ ë…¹í™” ì™„ë£Œ!")
            print(f"   ğŸ“Š ë…¹í™”ëœ ì—í”¼ì†Œë“œ: {len(results)}ê°œ")
            if results:
                success_rate = sum(1 for r in results if r['success']) / len(results)
                avg_reward = np.mean([r['reward'] for r in results])
                print(f"   ğŸ† ì„±ê³µë¥ : {success_rate:.3f}")
                print(f"   ğŸ’ í‰ê·  ë³´ìƒ: {avg_reward:.2f}")
            
        except Exception as e:
            print(f"âš ï¸ ë¹„ë””ì˜¤ ë…¹í™” ì˜¤ë¥˜: {e}")

def create_env(env_name, render_mode=None):
    """ê°œì„ ëœ í™˜ê²½ ìƒì„± (ë˜í¼ ì ìš©)"""
    env = gym.make(env_name, render_mode=render_mode)
    env = Monitor(env)
    
    # ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ì ìš©
    if config.reward_scale != 1.0:
        env = RewardScalingWrapper(env, scale=config.reward_scale)
    
    # ì„±ê³µë¥  ì¶”ì  ì¶”ê°€
    env = SuccessTrackingWrapper(env)
    
    return env

# def create_vec_env(env_name, n_envs=1, normalize=True):
#     """ë²¡í„°í™”ëœ í™˜ê²½ ìƒì„±"""
#     def make_env():
#         env = create_env(env_name)
#         return env
    
#     vec_env = DummyVecEnv([make_env for _ in range(n_envs)])
    
#     if normalize:
#         vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)
    
#     return vec_env

#ë³‘ë ¬ í™˜ê²½ìœ¼ë¡œ êµì²´ - n_envs=4ë¡œ ì„¤ì •, 
# export ì¶”ê°€ from stable_baselines3.common.vec_env import SubprocVecEnv
# def create_vec_env(env_name, n_envs=4, normalize=True):
#     def make_env():
#         return lambda: create_env(env_name)

#     vec_env = SubprocVecEnv([make_env() for _ in range(n_envs)])

#     if normalize:
#         vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)

#     return vec_env
def create_vec_env(env_name: str, normalize: bool = True, num_envs: int = 4):
    def make_env_fn(rank):
        def _init():
            env = gym.make(env_name)  # âœ… ì´ë¯¸ ë“±ë¡ëœ í™˜ê²½ ì‚¬ìš©
            return env
        return _init

    env_fns = [make_env_fn(i) for i in range(num_envs)]
    vec_env = SubprocVecEnv(env_fns)

    if normalize:
        vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)
        vec_env.reset()  # í•„ìˆ˜! ì´ˆê¸°í™” ì•ˆ í•˜ë©´ ì—ëŸ¬ ë‚  ìˆ˜ ìˆìŒ

    return vec_env


def create_model(env):
    """SAC ëª¨ë¸ ìƒì„±"""
    n_actions = env.action_space.shape[-1]
    
    action_noise = NormalActionNoise(
        mean=np.zeros(n_actions), 
        sigma=config.action_noise_std * np.ones(n_actions)
    )
    
    model = SAC(
        policy="MultiInputPolicy",
        env=env,
        learning_rate=config.learning_rate,
        buffer_size=config.buffer_size,
        learning_starts=config.learning_starts,
        batch_size=config.batch_size,
        tau=config.tau,
        gamma=config.gamma,
        train_freq=config.train_freq,
        gradient_steps=config.gradient_steps,
        action_noise=action_noise,
        policy_kwargs=config.policy_kwargs,
        use_sde=config.use_sde,
        sde_sample_freq=config.sde_sample_freq,
        verbose=1,
        tensorboard_log=config.log_dir,
        device="auto"
    )
    
    return model

def fixed_compare_best_vs_final_models(final_model, training_callback):
    print("\nğŸ” ì‹œì ë³„ Best Model vs Final Model ì„±ëŠ¥ ë¹„êµ...")

    comparison_results = {}

    # 1) model_dir ì—ì„œ best_model_* í´ë”ë§Œ ê³¨ë¼ì„œ ìˆœíšŒ
    for sub in sorted(os.listdir(config.model_dir)):
        if not sub.startswith("best_model_"):
            continue
        stage = sub.replace("best_model_", "")
        model_zip = os.path.join(config.model_dir, sub, "best_model.zip")
        if not os.path.exists(model_zip):
            continue

        print(f"\nâ–¶ Evaluate Stage {stage}")
        bm = SAC.load(model_zip)
        eval_env = create_env(config.env_name)

        # ë³´ìƒ & ê¸¸ì´ í‰ê°€
        rewards, lengths = evaluate_policy(
            bm, eval_env, 
            n_eval_episodes=20, 
            deterministic=True, 
            return_episode_rewards=True
        )
        # ì„±ê³µë¥  ê³„ì‚°
        succ = 0
        for _ in range(20):
            obs, _ = eval_env.reset()
            done = False
            while not done:
                a, _ = bm.predict(obs, deterministic=True)
                obs, _, term, trunc, info = eval_env.step(a)
                done = term or trunc
            if info.get('is_success', False):
                succ += 1

        comparison_results[stage] = {
            'mean_reward': np.mean(rewards),
            'std_reward' : np.std(rewards),
            'mean_length': np.mean(lengths),
            'success_rate': succ / 20,
            'rewards': rewards
        }

    # 2) Final Model í‰ê°€
    print("\nâ–¶ Evaluate Final Model")
    eval_env = create_env(config.env_name)
    fr, fl = evaluate_policy(
        final_model, eval_env,
        n_eval_episodes=20,
        deterministic=True,
        return_episode_rewards=True
    )
    fs = 0
    for _ in range(20):
        obs, _ = eval_env.reset()
        done = False
        while not done:
            a, _ = final_model.predict(obs, deterministic=True)
            obs, _, term, trunc, info = eval_env.step(a)
            done = term or trunc
        if info.get('is_success', False):
            fs += 1

    comparison_results['final'] = {
        'mean_reward': np.mean(fr),
        'std_reward' : np.std(fr),
        'mean_length': np.mean(fl),
        'success_rate': fs / 20,
        'rewards': fr
    }

    return comparison_results

def create_comparison_visualization(comparison_results):
    """Best vs Final ëª¨ë¸ ë¹„êµ ì‹œê°í™”"""
    try:
        import matplotlib.pyplot as plt
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        viz_dir = os.path.join(config.results_dir, "model_comparison")
        os.makedirs(viz_dir, exist_ok=True)
        
        # 1. ë³´ìƒ ë¶„í¬ ë¹„êµ (ë°•ìŠ¤í”Œë¡¯)
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # ë°•ìŠ¤í”Œë¡¯
        data_to_plot = [comparison_results['best_model']['rewards'], 
                       comparison_results['final_model']['rewards']]
        ax1.boxplot(data_to_plot, labels=['Best Model', 'Final Model'])
        ax1.set_title('ë³´ìƒ ë¶„í¬ ë¹„êµ')
        ax1.set_ylabel('ë³´ìƒ')
        ax1.grid(True, alpha=0.3)
        
        # íˆìŠ¤í† ê·¸ë¨
        ax2.hist(comparison_results['best_model']['rewards'], alpha=0.7, label='Best Model', bins=20)
        ax2.hist(comparison_results['final_model']['rewards'], alpha=0.7, label='Final Model', bins=20)
        ax2.set_title('ë³´ìƒ íˆìŠ¤í† ê·¸ë¨')
        ax2.set_xlabel('ë³´ìƒ')
        ax2.set_ylabel('ë¹ˆë„')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'reward_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. ì„±ëŠ¥ ìš”ì•½ ë§‰ëŒ€ ê·¸ë˜í”„
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        models = ['Best Model', 'Final Model']
        rewards = [comparison_results['best_model']['mean_reward'], 
                  comparison_results['final_model']['mean_reward']]
        success_rates = [comparison_results['best_model']['success_rate'], 
                        comparison_results['final_model']['success_rate']]
        
        ax1.bar(models, rewards, color=['gold', 'skyblue'])
        ax1.set_title('í‰ê·  ë³´ìƒ ë¹„êµ')
        ax1.set_ylabel('í‰ê·  ë³´ìƒ')
        for i, v in enumerate(rewards):
            ax1.text(i, v + 0.1, f'{v:.2f}', ha='center', va='bottom')
        
        ax2.bar(models, success_rates, color=['gold', 'skyblue'])
        ax2.set_title('ì‹¤ì œ ì„±ê³µë¥  ë¹„êµ')
        ax2.set_ylabel('ì„±ê³µë¥ ')
        ax2.set_ylim(0, 1.0)
        for i, v in enumerate(success_rates):
            ax2.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'performance_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {viz_dir}")
        
    except Exception as e:
        print(f"âš ï¸ ì‹œê°í™” ìƒì„± ì˜¤ë¥˜: {e}")

def create_fixed_comparison_videos(best_model, final_model, training_callback):
    print("ğŸ¥ ì‹œì ë³„ Best vs Final ëª¨ë¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")
    comparison_dir = os.path.join(config.video_dir, "model_comparison")
    os.makedirs(comparison_dir, exist_ok=True)

    video_sys = FixedStageBasedVideoSystem(
        base_dir=comparison_dir,
        total_timesteps=config.total_timesteps
    )

    # 1) best_model_* ìˆœíšŒ
    for sub in sorted(os.listdir(config.model_dir)):
        if not sub.startswith("best_model_"):
            continue
        stage = sub.replace("best_model_", "")
        print(f"   â–¶ Best Model [{stage}] ë…¹í™”")
        model_path = os.path.join(config.model_dir, sub, "best_model.zip")
        bm = SAC.load(model_path)
        video_sys.record_stage_episodes(
            stage_name=f"best_{stage}",
            model=bm,
            eval_env_stats=training_callback.eval_env_stats,
            num_episodes=config.episodes_per_stage
        )

    # 2) final ëª¨ë¸ë„ ë™ì¼í•˜ê²Œ ë…¹í™”
    print("   â–¶ Final Model ë…¹í™”")
    video_sys.record_stage_episodes(
        stage_name="final",
        model=final_model,
        eval_env_stats=training_callback.eval_env_stats,
        num_episodes=config.episodes_per_stage
    )

    print(f"âœ… ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {comparison_dir}")


def record_final_videos(model, training_callback):
    """ğŸ¯ ìˆ˜ì •ëœ ìµœì¢… ë¹„ë””ì˜¤ ë…¹í™”"""
    print("\nğŸ¥ ìˆ˜ì •ëœ ìµœì¢… ì„±ëŠ¥ ë¹„ë””ì˜¤ ë…¹í™”...")
    
    # ìµœì¢… ë‹¨ê³„ ë…¹í™” (ë§Œì•½ ì•„ì§ ì•ˆ ëë‹¤ë©´)
    final_stage = '5_100percent'
    if final_stage not in training_callback.video_system.completed_stages:
        training_callback.video_system.record_stage_episodes(
            stage_name=final_stage,
            model=model,
            eval_env_stats=training_callback.eval_env_stats,
            num_episodes=5  # ìµœì¢…ì€ ë” ë§ì´
        )
    
    # í•˜ì´ë¼ì´íŠ¸ ë¹„ë””ì˜¤ ìƒì„±
    training_callback.video_system.create_highlight_video()
    
    # ğŸ¯ ìˆ˜ì •ëœ Best vs Final ëª¨ë¸ ë¹„êµ
    comparison_results = fixed_compare_best_vs_final_models(model, training_callback)
    
    print(f"âœ… ìˆ˜ì •ëœ ìµœì¢… ë¹„ë””ì˜¤ ì™„ë£Œ!")

def train_model():
    """ğŸ¯ ìˆ˜ì •ëœ ëª¨ë¸ í•™ìŠµ"""
    print(f"ğŸ¯ í™˜ê²½: {config.env_name}")
    print(f"ğŸ§  ì•Œê³ ë¦¬ì¦˜: {config.algorithm}")
    print(f"ğŸ“Š ì´ í•™ìŠµ ìŠ¤í…: {config.total_timesteps:,}")
    print(f"ğŸ¥ ìˆ˜ì •ëœ ì—í”¼ì†Œë“œë³„ ë¹„ë””ì˜¤: ê° ë‹¨ê³„ë³„ {config.episodes_per_stage}ê°œ")
    print(f"ğŸ”§ ë³´ìƒ ìŠ¤ì¼€ì¼ë§: {config.reward_scale}")
    print(f"ğŸ“ˆ 6ë‹¨ê³„ ì²´ê³„ì  ì‹œê°í™” (0%, 20%, 40%, 60%, 80%, 100%)")
    print(f"âœ… Eval í™˜ê²½ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ë¹„ë””ì˜¤")
    print(f"âœ… ì‹¤ì œ info['is_success'] ê¸°ì¤€ ì‚¬ìš©")
    print("-" * 60)
    
    create_directories()
    
    # í™˜ê²½ ìƒì„±
    print("ğŸ—ï¸  ê°œì„ ëœ í™˜ê²½ ìƒì„± ì¤‘...")
    env = create_vec_env(config.env_name, normalize=config.normalize_env)
    eval_env = create_vec_env(config.env_name, normalize=config.normalize_env)
    
    # ë¡œê±° ì„¤ì •
    logger_path = os.path.join(config.log_dir, config.experiment_name)
    new_logger = configure(logger_path, ["stdout", "csv", "tensorboard"])
    
    # ëª¨ë¸ ìƒì„±
    print(f"\nğŸ§  {config.algorithm} ëª¨ë¸ ì´ˆê¸°í™”...")
    model = create_model(env)
    model.set_logger(new_logger)
    
    # ì½œë°± ì„¤ì •
    callbacks = []
    
    # ğŸ¯ ìˆ˜ì •ëœ í•™ìŠµ ëª¨ë‹ˆí„°ë§
    training_callback = FixedTrainingCallback(config.log_dir)
    training_callback.model = model
    callbacks.append(training_callback)
    
    # ğŸ”§ ìˆ˜ì •ëœ í‰ê°€ ì½œë°±
    class FixedEvalCallback(EvalCallback):
        def __init__(self, *args, **kwargs):
            self.training_callback = kwargs.pop('training_callback', None)
            super().__init__(*args, **kwargs)
        
        def _on_step(self) -> bool:
            result = super()._on_step()
            # Eval í™˜ê²½ í†µê³„ë¥¼ training_callbackì— ì „ë‹¬
            if self.training_callback and hasattr(self.eval_env, 'obs_rms'):
                self.training_callback.set_eval_env_stats(self.eval_env)

            # 3) í˜„ì¬ timestep ê¸°ì¤€ ìŠ¤í…Œì´ì§€ ê³„ì‚°
            current_stage = self.training_callback.video_system.should_record_stage(self.num_timesteps)
            if current_stage:
                # ë””ë ‰í† ë¦¬ ì˜ˆ: models/best_model_1_20percent/best_model.zip
                save_dir = os.path.join(config.model_dir, f"best_model_{current_stage}")
                os.makedirs(save_dir, exist_ok=True)
                # EvalCallback ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²½ë¡œë¥¼ ë™ì ìœ¼ë¡œ êµì²´
                self.best_model_save_path = save_dir

            return result
    
    eval_callback = FixedEvalCallback(
        eval_env,
        training_callback=training_callback,
        best_model_save_path=os.path.join(config.model_dir, "best_model"),  # prefix ìš©ìœ¼ë¡œë§Œ ë‚¨ê¹€
        log_path=os.path.join(config.log_dir, "eval"),
        eval_freq=config.eval_freq,
        n_eval_episodes=config.n_eval_episodes,
        deterministic=config.eval_deterministic,
        verbose=1
    )


    callbacks.append(eval_callback)
    
    # ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_callback = CheckpointCallback(
        save_freq=config.save_freq,
        save_path=os.path.join(config.model_dir, "checkpoints"),
        name_prefix=f"{config.algorithm}_fixed_{config.experiment_name}"
    )
    callbacks.append(checkpoint_callback)
    
    # í•™ìŠµ ì „ ëœë¤ ë¹„ë””ì˜¤
    print("\nğŸ¬ [í•™ìŠµ ì „] ìˆ˜ì •ëœ ë¬´ì‘ìœ„ í–‰ë™ ì—í”¼ì†Œë“œ ë…¹í™”!")
    training_callback.record_stage_videos("0_random")
    
    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸš€ ìˆ˜ì •ëœ {config.algorithm} í•™ìŠµ ì‹œì‘!")
    print("=" * 60)
    print("ğŸ’¡ Eval í™˜ê²½ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ë¹„ë””ì˜¤!")
    print("ğŸ’¡ VecNormalize í†µê³„ ë™ê¸°í™”!")
    print("ğŸ’¡ ì‹¤ì œ info['is_success'] ê¸°ì¤€ ì‚¬ìš©!")
    print("ğŸ’¡ ì„±ê³µ ì—í”¼ì†Œë“œ ìš°ì„  ë…¹í™”!")
    print("ğŸ’¡ Best Model ê¸°ë°˜ ë¹„êµ!")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=config.total_timesteps,
            callback=callbacks,
            log_interval=10,
            progress_bar=True
        )
    except KeyboardInterrupt:
        print("\nâ¸ï¸  í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    end_time = time.time()
    training_time = end_time - start_time
    
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"â±ï¸  ì´ í•™ìŠµ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = os.path.join(config.model_dir, f"{config.algorithm}_fixed_{config.experiment_name}_final")
    model.save(final_model_path)
    print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
    
    # ìµœì¢… í‰ê°€
    print("\nğŸ” ìµœì¢… í‰ê°€...")
    mean_reward, std_reward = evaluate_policy(
        model, eval_env, 
        n_eval_episodes=50,
        deterministic=True
    )
    print(f"ğŸ† ìµœì¢… í‰ê°€ ê²°ê³¼: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    return model, training_callback

def main():
    """ğŸ¯ ìˆ˜ì •ëœ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print(f"ğŸš€ ì™„ì „íˆ ìˆ˜ì •ëœ ì—í”¼ì†Œë“œ ê¸°ë°˜ ë¹„ë””ì˜¤ í•™ìŠµ ì‹œì‘!")
    print(f"ğŸ¯ ëª©í‘œ: FrankaSlideDense íƒœìŠ¤í¬ ë§ˆìŠ¤í„°í•˜ê¸°")
    print(f"ğŸ”§ ì£¼ìš” ìˆ˜ì •ì‚¬í•­:")
    print(f"   âœ… Eval í™˜ê²½ê³¼ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ë¹„ë””ì˜¤ í™˜ê²½")
    print(f"   âœ… VecNormalize í†µê³„ ë™ê¸°í™”")
    print(f"   âœ… ì‹¤ì œ info['is_success'] ê¸°ì¤€ ì‚¬ìš©")
    print(f"   âœ… ì„±ê³µ ì—í”¼ì†Œë“œ ìš°ì„  ë…¹í™”")
    print(f"   âœ… Best Model ê¸°ë°˜ ë¹„êµ ì‹œìŠ¤í…œ")
    print(f"   âœ… ê° ë‹¨ê³„ë³„ {config.episodes_per_stage}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   âœ… ë³´ìƒ ìŠ¤ì¼€ì¼ë§: {config.reward_scale} (ì•ˆì •ì  í•™ìŠµ)")
    print("=" * 60)
    
    # í•™ìŠµ ì‹¤í–‰
    model, training_callback = train_model()
    
    # ìµœì¢… ë¹„ë””ì˜¤ ë…¹í™”
    record_final_videos(model, training_callback)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ì™„ì „íˆ ìˆ˜ì •ëœ í•™ìŠµ ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… ì„±ê³µë¥ : {training_callback.recent_success_rate:.3f}")
    print(f"ğŸ† ìµœê³  ë³´ìƒ: {training_callback.best_reward:.2f}")
    print(f"ğŸ¥ ìˆ˜ì •ëœ ì—í”¼ì†Œë“œë³„ ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print(f"ğŸ“ ë¹„ë””ì˜¤ ì €ì¥ ìœ„ì¹˜: {config.video_dir}")
    print("ğŸ“‹ ìƒì„±ëœ ê²°ê³¼:")
    print("   âœ… 0_random/: í•™ìŠµ ì „ ë¬´ì‘ìœ„ í–‰ë™ (ìˆ˜ì •ë¨)")
    print("   âœ… 1_20percent/: 20% í•™ìŠµ ì§„í–‰ ì‹œì  (Eval ì¼ì¹˜)")
    print("   âœ… 2_40percent/: 40% í•™ìŠµ ì§„í–‰ ì‹œì  (Eval ì¼ì¹˜)")
    print("   âœ… 3_60percent/: 60% í•™ìŠµ ì§„í–‰ ì‹œì  (Eval ì¼ì¹˜)") 
    print("   âœ… 4_80percent/: 80% í•™ìŠµ ì§„í–‰ ì‹œì  (Eval ì¼ì¹˜)")
    print("   âœ… 5_100percent/: 100% í•™ìŠµ ì™„ë£Œ ì‹œì  (Eval ì¼ì¹˜)")
    print("   âœ… highlights/: ìµœê³  ë³´ìƒ ì—í”¼ì†Œë“œë“¤ (ì‹¤ì œ ì„±ê³µ ê¸°ì¤€)")
    print("   âœ… model_comparison/: Best vs Final ëª¨ë¸ ë¹„êµ (ìˆ˜ì •ë¨)")
    print(f"ğŸ“Š ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„: {config.results_dir}/model_comparison/")
    print("ğŸ¯ ì´ì œ ë¹„ë””ì˜¤ê°€ ì‹¤ì œ í•™ìŠµ ì„±ëŠ¥ì„ ì •í™•íˆ ë°˜ì˜í•©ë‹ˆë‹¤!")
    print("=" * 60)
    
    return model, training_callback

if __name__ == "__main__":
    model, training_callback = main()
