#/home/minjun/panda_mujoco_gym/train_smart_video_big_network.py
#!/usr/bin/env python3
"""
ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë…¹í™” ì‹œìŠ¤í…œ
- ì—í”¼ì†Œë“œ ê¸°ë°˜ ë…¹í™” (ì •ì§€ í™”ë©´ ìµœì†Œí™”)
- 1ì´ˆ ëŒ€ê¸° í›„ ìë™ ì¢…ë£Œ
- 6ë‹¨ê³„ ì‹œê°í™” (ì ì ˆí•œ ë¹ˆë„)
- ì˜ë¯¸ ìˆëŠ” trajectoryë§Œ ìº¡ì²˜
"""

import os
import sys
import time
import numpy as np
import matplotlib.pyplot as plt
import json
import csv
from datetime import datetime
import gymnasium as gym
import torch  # ğŸ”§ torch ì¶”ê°€ (activation_fn ìš©)
from stable_baselines3 import SAC, TD3, PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder, VecNormalize
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback
from stable_baselines3.common.logger import configure
from stable_baselines3.common.noise import NormalActionNoise

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ì‚¬ìš©ì ì •ì˜ í™˜ê²½ ë“±ë¡
import panda_mujoco_gym

print("ğŸ¤– ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë…¹í™” ê¸°ë°˜ Panda ë¡œë´‡ í•™ìŠµ!")
print("=" * 60)

# ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ë˜í¼ (ê¸°ì¡´ê³¼ ë™ì¼)
class RewardScalingWrapper(gym.Wrapper):
    def __init__(self, env, scale=0.1):
        super().__init__(env)
        self.scale = scale
        self.episode_reward = 0
        
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        scaled_reward = reward * self.scale
        self.episode_reward += scaled_reward
        
        if terminated or truncated:
            info['original_reward'] = self.episode_reward / self.scale
            info['scaled_reward'] = self.episode_reward
            self.episode_reward = 0
            
        return obs, scaled_reward, terminated, truncated, info
    
    def reset(self, **kwargs):
        self.episode_reward = 0
        return self.env.reset(**kwargs)

# ì„±ê³µë¥  ì¶”ì  ë˜í¼ (ê¸°ì¡´ê³¼ ë™ì¼)
class SuccessTrackingWrapper(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)
        self.success_count = 0
        self.episode_count = 0
        
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        if terminated or truncated:
            self.episode_count += 1
            if info.get('is_success', False):
                self.success_count += 1
            
            info['success_rate'] = self.success_count / self.episode_count if self.episode_count > 0 else 0
            
        return obs, reward, terminated, truncated, info

# ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë…¹í™” í´ë˜ìŠ¤
class SmartVideoRecorder:
    def __init__(self, env, video_path, max_episodes=5, wait_time=1.0):
        """
        ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë…¹í™”ê¸°
        
        Args:
            env: í™˜ê²½
            video_path: ë¹„ë””ì˜¤ ì €ì¥ ê²½ë¡œ
            max_episodes: ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜
            wait_time: ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        self.env = env
        self.video_path = video_path
        self.max_episodes = max_episodes
        self.wait_time = wait_time
        self.wait_frames = int(wait_time * 50)  # 50 FPS ê¸°ì¤€
        
        # ë¹„ë””ì˜¤ ë…¹í™” í™˜ê²½ ì„¤ì •
        os.makedirs(os.path.dirname(video_path), exist_ok=True)
        
    def record_episodes(self, model=None, deterministic=True):
        """ì—í”¼ì†Œë“œ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë…¹í™”"""
        print(f"ğŸ¥ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë…¹í™”: ìµœëŒ€ {self.max_episodes} ì—í”¼ì†Œë“œ")
        
        # ì„ì‹œ ë¹„ë””ì˜¤ í™˜ê²½
        video_env = DummyVecEnv([lambda: self.env])
        video_env = VecVideoRecorder(
            video_env,
            self.video_path,
            record_video_trigger=lambda x: x == 0,
            video_length=10000,  # ì¶©ë¶„íˆ í° ê°’ (ì‹¤ì œë¡œëŠ” ì—í”¼ì†Œë“œë¡œ ì œì–´)
            name_prefix="smart_recording"
        )
        
        obs = video_env.reset()
        episode_count = 0
        step_count = 0
        total_steps = 0
        successes = 0
        episode_rewards = []
        
        current_episode_reward = 0
        wait_counter = 0
        in_wait_phase = False
        
        while episode_count < self.max_episodes:
            if not in_wait_phase:
                # ì •ìƒ í–‰ë™ ë‹¨ê³„
                if model is not None:
                    action, _ = model.predict(obs, deterministic=deterministic)
                else:
                    action = [video_env.action_space.sample()]
                
                obs, rewards, dones, infos = video_env.step(action)
                current_episode_reward += rewards[0]
                step_count += 1
                total_steps += 1
                
                if dones[0]:
                    # ì—í”¼ì†Œë“œ ì™„ë£Œ
                    episode_count += 1
                    success = infos[0].get('is_success', False)
                    if success:
                        successes += 1
                    
                    episode_rewards.append(current_episode_reward)
                    print(f"   Episode {episode_count}: {step_count} steps, "
                          f"reward: {current_episode_reward:.2f}, success: {success}")
                    
                    # ëŒ€ê¸° ë‹¨ê³„ ì‹œì‘
                    in_wait_phase = True
                    wait_counter = 0
                    step_count = 0
                    current_episode_reward = 0
            
            else:
                # ëŒ€ê¸° ë‹¨ê³„ (1ì´ˆ ëŒ€ê¸°)
                wait_counter += 1
                total_steps += 1
                
                # ëŒ€ê¸° ì¤‘ì—ëŠ” no-op action
                action = [np.zeros(video_env.action_space.shape[0])]
                obs, rewards, dones, infos = video_env.step(action)
                
                if wait_counter >= self.wait_frames:
                    # ëŒ€ê¸° ì™„ë£Œ, ë‹¤ìŒ ì—í”¼ì†Œë“œ ì‹œì‘
                    in_wait_phase = False
                    if episode_count < self.max_episodes:
                        obs = video_env.reset()
        
        video_env.close()
        
        success_rate = successes / episode_count if episode_count > 0 else 0
        avg_reward = np.mean(episode_rewards) if episode_rewards else 0
        
        print(f"âœ… ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ì™„ë£Œ!")
        print(f"   ğŸ“Š ì´ ì—í”¼ì†Œë“œ: {episode_count}")
        print(f"   ğŸ† ì„±ê³µë¥ : {success_rate:.3f}")
        print(f"   ğŸ’ í‰ê·  ë³´ìƒ: {avg_reward:.2f}")
        print(f"   ğŸ“¹ ì´ í”„ë ˆì„: {total_steps} (íš¨ìœ¨ì !)")
        
        return {
            'episodes': episode_count,
            'success_rate': success_rate,
            'avg_reward': avg_reward,
            'total_frames': total_steps
        }

# ìŠ¤ë§ˆíŠ¸ ì„¤ì • í´ë˜ìŠ¤
class SmartConfig:
    # í™˜ê²½ ì„¤ì •
    env_name = "FrankaSlideDense-v0"
    algorithm = "SAC"
    total_timesteps = 1_000_000       # ë” ë§ì€ í•™ìŠµ ìœ ì§€
    
    # ğŸ”¥ FrankaSlideDense íŠ¹í™” ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    learning_rate = 1e-4              # ë” ì‘ì€ í•™ìŠµë¥ ë¡œ ì•ˆì •ì  í•™ìŠµ (í° ë„¤íŠ¸ì›Œí¬ì— ì í•©)
    buffer_size = 1_000_000           # ë” í° ë²„í¼ (ë³µì¡í•œ í•™ìŠµì— í•„ìš”)
    learning_starts = 10_000          # ë” ë§ì€ ì´ˆê¸° ê²½í—˜ ìˆ˜ì§‘
    batch_size = 512                  # ë” í° ë°°ì¹˜ (í° ë„¤íŠ¸ì›Œí¬ì— ì í•©)
    tau = 0.01                        # ë” ë¹ ë¥¸ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
    gamma = 0.98                      # ì•½ê°„ ë‚®ì€ í• ì¸ ì¸ìˆ˜
    train_freq = 4                    # ë” ìì£¼ í•™ìŠµ
    gradient_steps = 4                # ë” ë§ì€ gradient steps
    
    # ğŸ§  ë” í° ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° (ë³µì¡í•œ ë¡œë´‡ íƒœìŠ¤í¬ì— ìµœì í™”)
    policy_kwargs = {
        "net_arch": [256, 256, 128],  # ğŸ”¥ ì¶”í›„ 512, 512, 256ìœ¼ë¡œ ìˆ˜ì •í•˜ê¸°
        "activation_fn": torch.nn.ReLU,  # ğŸ”§ ë¬¸ìì—´ â†’ í•¨ìˆ˜ ê°ì²´ë¡œ ìˆ˜ì •
        "normalize_images": False     # ì´ë¯¸ì§€ ì •ê·œí™” ë¹„í™œì„±í™”
    }
    
    # ğŸ¯ ë” ê°•í•œ íƒí—˜ ë…¸ì´ì¦ˆ (í° ë„¤íŠ¸ì›Œí¬ì™€ í•¨ê»˜)
    action_noise_std = 0.2            # ë” í° ë…¸ì´ì¦ˆë¡œ íƒí—˜ ê°•í™”
    
    # íƒí—˜ ë…¸ì´ì¦ˆ
    action_noise_std = 0.2
    
    # í‰ê°€ ì„¤ì •
    eval_freq = 20_000                # ë” ìì£¼ í‰ê°€ (í° ë„¤íŠ¸ì›Œí¬ ëª¨ë‹ˆí„°ë§)
    n_eval_episodes = 20              # ë” ë§ì€ í‰ê°€ ì—í”¼ì†Œë“œ
    eval_deterministic = True
    
    # ì €ì¥ ì„¤ì •
    save_freq = 100_000
    video_freq = 50_000               # ë” ìì£¼ ë¹„ë””ì˜¤ ìƒì„±
    
    # ğŸ¥ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ì„¤ì •
    max_episodes_per_video = 5        # ë¹„ë””ì˜¤ë‹¹ ìµœëŒ€ 5 ì—í”¼ì†Œë“œ
    wait_time_after_episode = 1.0     # ì—í”¼ì†Œë“œ í›„ 1ì´ˆ ëŒ€ê¸°
    
    # ì‹œê°í™” ì„¤ì •
    enable_realtime_viz = True
    viz_duration = 45                 # ë” ê¸´ ì‹œê°í™” ì‹œê°„ (í° ë„¤íŠ¸ì›Œí¬ ê´€ì°°)
    
    # í™˜ê²½ ê°œì„  ì„¤ì •
    normalize_env = True              # ê´€ì°°ê°’ ì •ê·œí™” (í° ë„¤íŠ¸ì›Œí¬ì— ì¤‘ìš”)
    reward_scale = 0.1                # ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ìœ ì§€
    
    # ğŸ§ª ì¶”ê°€ í•™ìŠµ ê¸°ë²• (í° ë„¤íŠ¸ì›Œí¬ ì•ˆì •í™”)
    use_sde = True                    # State Dependent Exploration (í° ë„¤íŠ¸ì›Œí¬ì— ë„ì›€)
    sde_sample_freq = 4
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    base_dir = "data_smart_video"
    model_dir = os.path.join(base_dir, "models")
    results_dir = os.path.join(base_dir, "training_results")
    video_dir = os.path.join(base_dir, "videos")
    log_dir = os.path.join(base_dir, "logs")
    
    # ì‹¤í—˜ ì´ë¦„
    experiment_name = f"{env_name}_{algorithm}_smart_video_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

config = SmartConfig()

def create_directories():
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    dirs_to_create = [
        config.base_dir,
        config.model_dir,
        config.results_dir,
        config.video_dir,
        config.log_dir
    ]
    
    for dir_path in dirs_to_create:
        os.makedirs(dir_path, exist_ok=True)
        print(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {dir_path}")

class SmartTrainingCallback(BaseCallback):
    """
    ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ì½œë°± - 6ë‹¨ê³„ ì‹œê°í™” + íš¨ìœ¨ì  ë¹„ë””ì˜¤
    """
    def __init__(self, log_dir, verbose=0):
        super(SmartTrainingCallback, self).__init__(verbose)
        self.log_dir = log_dir
        self.episode_rewards = []
        self.episode_lengths = []
        self.success_count = 0
        self.episode_count = 0
        self.csv_file = os.path.join(log_dir, f"training_log_{config.experiment_name}.csv")
        
        # 6ë‹¨ê³„ ì‹œê°í™” ìŠ¤ì¼€ì¤„ (ì ì ˆí•œ ë¹ˆë„)
        self.total_timesteps = config.total_timesteps
        self.visualization_steps = [
                                            # 0% - í•™ìŠµ ì „ : train_model()ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ìƒì„±ë¨
            self.total_timesteps // 5,      # 10% -> 20%
            self.total_timesteps * 2 // 5,       # 25% -> 40%
            self.total_timesteps * 3 // 5,       # 50% -> 60%
            self.total_timesteps * 4 // 5,   # 75% -> 80%
            self.total_timesteps              # 100% - í•™ìŠµ ì™„ë£Œ
        ]
        self.completed_visualizations = set()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.best_reward = float('-inf')
        self.recent_rewards = []
        self.recent_success_rate = 0.0
        
        # CSV íŒŒì¼ ì´ˆê¸°í™”
        with open(self.csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Timestep', 'Episode', 'Reward', 'Length', 'Success', 'Success_Rate', 'Best_Reward'])
    
    def _on_step(self) -> bool:
        # 6ë‹¨ê³„ ì‹œê°í™” ì²´í¬
        for i, target_step in enumerate(self.visualization_steps):
            if (target_step not in self.completed_visualizations and 
                self.num_timesteps >= target_step):
                
                self.completed_visualizations.add(target_step)
                stage_names = [
                    "1_í•™ìŠµì§„í–‰_20í¼ì„¼íŠ¸",
                    "2_í•™ìŠµì§„í–‰_40í¼ì„¼íŠ¸",
                    "3_í•™ìŠµì§„í–‰_60í¼ì„¼íŠ¸",
                    "4_í•™ìŠµì§„í–‰_80í¼ì„¼íŠ¸",
                    "5_í•™ìŠµì™„ë£Œ_100í¼ì„¼íŠ¸"
                ]
                stage_name = stage_names[i]
                
                print(f"\nğŸ¬ [{stage_name}] ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë…¹í™”! (Step {self.num_timesteps})")
                if self.recent_rewards:
                    print(f"   í˜„ì¬ í‰ê·  ë³´ìƒ: {np.mean(self.recent_rewards[-50:]):.2f}")
                    print(f"   í˜„ì¬ ì„±ê³µë¥ : {self.recent_success_rate:.3f}")
                self.record_smart_video(stage_name)
        
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì‹œ ì²˜ë¦¬
        if len(self.locals.get('dones', [])) > 0 and self.locals.get('dones', [False])[0]:
            info = self.locals.get('infos', [{}])[0]
            
            if 'episode' in info:
                episode_reward = info['episode']['r']
                episode_length = info['episode']['l']
                is_success = info.get('is_success', False)
                
                self.episode_rewards.append(episode_reward)
                self.episode_lengths.append(episode_length)
                self.recent_rewards.append(episode_reward)
                self.episode_count += 1
                
                if is_success:
                    self.success_count += 1
                
                # ì„±ê³µë¥  ê³„ì‚°
                self.recent_success_rate = self.success_count / self.episode_count if self.episode_count > 0 else 0
                
                # ìµœê·¼ 100ê°œ ì—í”¼ì†Œë“œë§Œ ìœ ì§€
                if len(self.recent_rewards) > 100:
                    self.recent_rewards.pop(0)
                
                # ìµœê³  ë³´ìƒ ì—…ë°ì´íŠ¸
                if episode_reward > self.best_reward:
                    self.best_reward = episode_reward
                    print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ë³´ìƒ! {episode_reward:.2f} (ì„±ê³µë¥ : {self.recent_success_rate:.3f})")
                
                # ë¡œê¹…
                if self.episode_count % 10 == 0:
                    avg_reward = np.mean(self.recent_rewards[-50:]) if len(self.recent_rewards) >= 50 else np.mean(self.recent_rewards)
                    avg_length = np.mean(self.episode_lengths[-50:]) if len(self.episode_lengths) >= 50 else np.mean(self.episode_lengths)
                    
                    print(f"ğŸ“Š Episode {self.episode_count:4d} | "
                          f"Reward: {episode_reward:7.2f} | "
                          f"Avg: {avg_reward:7.2f} | "
                          f"Length: {episode_length:3.0f} | "
                          f"Success: {self.recent_success_rate:.3f}")
                
                # CSV ë¡œê·¸ ì €ì¥
                with open(self.csv_file, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        self.num_timesteps,
                        self.episode_count,
                        episode_reward,
                        episode_length,
                        is_success,
                        self.recent_success_rate,
                        self.best_reward
                    ])
        
        return True
    
    def record_smart_video(self, stage_name):
        """ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë…¹í™” - ì—í”¼ì†Œë“œ ê¸°ë°˜, 1ì´ˆ ëŒ€ê¸°"""
        try:
            print(f"ğŸ¥ [{stage_name}] ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë…¹í™” ì‹œì‘...")
            
            # ë¹„ë””ì˜¤ í™˜ê²½ ìƒì„±
            video_env = create_env(config.env_name, render_mode="rgb_array")
            
            # ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë ˆì½”ë” ìƒì„±
            stage_video_dir = os.path.join(config.video_dir, "training_stages")
            video_path = os.path.join(stage_video_dir, f"{stage_name}_step_{self.num_timesteps}")
            
            recorder = SmartVideoRecorder(
                env=video_env,
                video_path=video_path,
                max_episodes=config.max_episodes_per_video,
                wait_time=config.wait_time_after_episode
            )
            
            # ìŠ¤ë§ˆíŠ¸ ë…¹í™” ì‹¤í–‰
            stats = recorder.record_episodes(
                model=self.model if hasattr(self.model, 'predict') else None,
                deterministic=True
            )
            
            print(f"âœ… [{stage_name}] ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ì™„ë£Œ!")
            print(f"   ğŸ“Š ì—í”¼ì†Œë“œ: {stats['episodes']}")
            print(f"   ğŸ† ì„±ê³µë¥ : {stats['success_rate']:.3f}")
            print(f"   ğŸ’ í‰ê·  ë³´ìƒ: {stats['avg_reward']:.2f}")
            print(f"   ğŸ“¹ íš¨ìœ¨ì  í”„ë ˆì„ ìˆ˜: {stats['total_frames']}")
            
        except Exception as e:
            print(f"âš ï¸ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë…¹í™” ì˜¤ë¥˜: {e}")

def create_env(env_name, render_mode=None):
    """ê°œì„ ëœ í™˜ê²½ ìƒì„± (ë˜í¼ ì ìš©)"""
    env = gym.make(env_name, render_mode=render_mode)
    env = Monitor(env)
    
    # ë³´ìƒ ìŠ¤ì¼€ì¼ë§ ì ìš©
    if config.reward_scale != 1.0:
        env = RewardScalingWrapper(env, scale=config.reward_scale)
    
    # ì„±ê³µë¥  ì¶”ì  ì¶”ê°€
    env = SuccessTrackingWrapper(env)
    
    return env

def create_vec_env(env_name, n_envs=1, normalize=True):
    """ë²¡í„°í™”ëœ í™˜ê²½ ìƒì„±"""
    def make_env():
        env = create_env(env_name)
        return env
    
    vec_env = DummyVecEnv([make_env for _ in range(n_envs)])
    
    if normalize:
        vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)
    
    return vec_env

def create_model(env):
    """SAC ëª¨ë¸ ìƒì„±"""
    n_actions = env.action_space.shape[-1]
    
    # ê°œì„ ëœ SAC ëª¨ë¸
    action_noise = NormalActionNoise(
        mean=np.zeros(n_actions), 
        sigma=config.action_noise_std * np.ones(n_actions)
    )
    
    model = SAC(
        policy="MultiInputPolicy",
        env=env,
        learning_rate=config.learning_rate,
        buffer_size=config.buffer_size,
        learning_starts=config.learning_starts,
        batch_size=config.batch_size,
        tau=config.tau,
        gamma=config.gamma,
        train_freq=config.train_freq,
        gradient_steps=config.gradient_steps,
        action_noise=action_noise,
        policy_kwargs=config.policy_kwargs,
        use_sde=config.use_sde,
        sde_sample_freq=config.sde_sample_freq,
        verbose=1,
        tensorboard_log=config.log_dir,
        device="auto"
    )
    
    return model

def record_final_smart_video(model):
    """ìµœì¢… ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë…¹í™”"""
    print("\nğŸ¥ ìµœì¢… ì„±ëŠ¥ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë…¹í™”...")
    
    # ìµœì¢… ë¹„ë””ì˜¤ìš© í™˜ê²½
    video_env = create_env(config.env_name, render_mode="rgb_array")
    video_path = os.path.join(config.video_dir, f"final_performance_smart_{config.experiment_name}")
    
    # ë” ë§ì€ ì—í”¼ì†Œë“œë¡œ ìµœì¢… ì„±ëŠ¥ í‰ê°€
    recorder = SmartVideoRecorder(
        env=video_env,
        video_path=video_path,
        max_episodes=10,  # ìµœì¢… í‰ê°€ëŠ” ë” ë§ì€ ì—í”¼ì†Œë“œ
        wait_time=1.0
    )
    
    stats = recorder.record_episodes(model=model, deterministic=True)
    
    print(f"ğŸ¥ ìµœì¢… ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ì—í”¼ì†Œë“œ: {stats['episodes']}")
    print(f"ğŸ† ìµœì¢… ì„±ê³µë¥ : {stats['success_rate']:.3f}")
    print(f"ğŸ’ ìµœì¢… í‰ê·  ë³´ìƒ: {stats['avg_reward']:.2f}")
    print(f"ğŸ“¹ íš¨ìœ¨ì  ì´ í”„ë ˆì„: {stats['total_frames']}")
    
    return stats

def train_model():
    """ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ í•™ìŠµ"""
    print(f"ğŸ¯ í™˜ê²½: {config.env_name}")
    print(f"ğŸ§  ì•Œê³ ë¦¬ì¦˜: {config.algorithm}")
    print(f"ğŸ“Š ì´ í•™ìŠµ ìŠ¤í…: {config.total_timesteps:,}")
    print(f"ğŸ¥ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤: ì—í”¼ì†Œë“œë‹¹ {config.max_episodes_per_video}ê°œ, {config.wait_time_after_episode}ì´ˆ ëŒ€ê¸°")
    print(f"ğŸ”§ ë³´ìƒ ìŠ¤ì¼€ì¼ë§: {config.reward_scale}")
    print(f"ğŸ“ˆ 5ë‹¨ê³„ ì‹œê°í™” (20%, 40%, 60%, 80%, 100%) + í•™ìŠµì „ ìˆ˜ë™")
    print("-" * 60)
    
    create_directories()
    
    # í™˜ê²½ ìƒì„±
    print("ğŸ—ï¸  ê°œì„ ëœ í™˜ê²½ ìƒì„± ì¤‘...")
    env = create_vec_env(config.env_name, normalize=config.normalize_env)
    eval_env = create_vec_env(config.env_name, normalize=config.normalize_env)
    
    # ë¡œê±° ì„¤ì •
    logger_path = os.path.join(config.log_dir, config.experiment_name)
    new_logger = configure(logger_path, ["stdout", "csv", "tensorboard"])
    
    # ëª¨ë¸ ìƒì„±
    print(f"\nğŸ§  {config.algorithm} ëª¨ë¸ ì´ˆê¸°í™”...")
    model = create_model(env)
    model.set_logger(new_logger)
    
    # ì½œë°± ì„¤ì •
    callbacks = []
    
    # ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ëª¨ë‹ˆí„°ë§
    training_callback = SmartTrainingCallback(config.log_dir)
    training_callback.model = model
    callbacks.append(training_callback)
    
    # í•™ìŠµ ì „ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤
    print("\nğŸ¬ [í•™ìŠµ ì „] ë¬´ì‘ìœ„ í–‰ë™ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤!")
    training_callback.record_smart_video("0_í•™ìŠµì „_ë¬´ì‘ìœ„")
    
    # í‰ê°€ ì½œë°±
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=os.path.join(config.model_dir, f"best_model_{config.experiment_name}"),
        log_path=os.path.join(config.log_dir, "eval"),
        eval_freq=config.eval_freq,
        n_eval_episodes=config.n_eval_episodes,
        deterministic=config.eval_deterministic,
        render=False,
        verbose=1
    )
    callbacks.append(eval_callback)
    
    # ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_callback = CheckpointCallback(
        save_freq=config.save_freq,
        save_path=os.path.join(config.model_dir, "checkpoints"),
        name_prefix=f"{config.algorithm}_smart_{config.experiment_name}"
    )
    callbacks.append(checkpoint_callback)
    
    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸš€ ìŠ¤ë§ˆíŠ¸ {config.algorithm} í•™ìŠµ ì‹œì‘!")
    print("=" * 60)
    print("ğŸ’¡ 6ë‹¨ê³„ ìë™ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤!")
    print("ğŸ’¡ ì—í”¼ì†Œë“œ ê¸°ë°˜ íš¨ìœ¨ì  ë…¹í™”!")
    print("ğŸ’¡ 1ì´ˆ ëŒ€ê¸° í›„ ìë™ ì „í™˜!")
    print("ğŸ’¡ ë³´ìƒ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì•ˆì •ì  í•™ìŠµ!")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=config.total_timesteps,
            callback=callbacks,
            log_interval=10,
            progress_bar=True
        )
    except KeyboardInterrupt:
        print("\nâ¸ï¸  í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    end_time = time.time()
    training_time = end_time - start_time
    
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"â±ï¸  ì´ í•™ìŠµ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = os.path.join(config.model_dir, f"{config.algorithm}_smart_{config.experiment_name}_final")
    model.save(final_model_path)
    print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
    
    # ìµœì¢… í‰ê°€
    print("\nğŸ” ìµœì¢… í‰ê°€...")
    mean_reward, std_reward = evaluate_policy(
        model, eval_env, 
        n_eval_episodes=50,
        deterministic=True
    )
    print(f"ğŸ† ìµœì¢… í‰ê°€ ê²°ê³¼: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    return model, training_callback

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print(f"ğŸš€ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ê¸°ë°˜ Panda ë¡œë´‡ í•™ìŠµ ì‹œì‘!")
    print(f"ğŸ¯ ëª©í‘œ: FrankaSlideDense íƒœìŠ¤í¬ ë§ˆìŠ¤í„°í•˜ê¸°")
    print(f"ğŸ’¡ ì£¼ìš” íŠ¹ì§•:")
    print(f"   â€¢ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤: ì—í”¼ì†Œë“œ ê¸°ë°˜ íš¨ìœ¨ì  ë…¹í™”")
    print(f"   â€¢ 1ì´ˆ ëŒ€ê¸°: ì •ì§€ í™”ë©´ ìµœì†Œí™”")
    print(f"   â€¢ 6ë‹¨ê³„ ì‹œê°í™”: ì ì ˆí•œ ëª¨ë‹ˆí„°ë§")
    print(f"   â€¢ ë³´ìƒ ìŠ¤ì¼€ì¼ë§: {config.reward_scale} (ì•ˆì •ì  í•™ìŠµ)")
    print("=" * 60)
    
    # í•™ìŠµ ì‹¤í–‰
    model, training_callback = train_model()
    
    # ìµœì¢… ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ë…¹í™”
    final_stats = record_final_smart_video(model)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… ì„±ê³µë¥ : {training_callback.recent_success_rate:.3f}")
    print(f"ğŸ† ìµœê³  ë³´ìƒ: {training_callback.best_reward:.2f}")
    print(f"ğŸ¥ íš¨ìœ¨ì  ë¹„ë””ì˜¤ ì‹œìŠ¤í…œ ì ìš© ì™„ë£Œ!")
    print("=" * 60)
    
    return model, training_callback, final_stats

if __name__ == "__main__":
    model, training_callback, final_stats = main()