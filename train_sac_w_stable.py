#/home/minjun/panda_mujoco_gym/train_sac_w_stable.py
#!/usr/bin/env python3
"""
Stable-Baselines3 SAC ê¸°ë°˜ Panda ë¡œë´‡ í•™ìŠµ ì½”ë“œ
"""

import os
import sys
import time
import numpy as np
import matplotlib.pyplot as plt
import json
import csv
from datetime import datetime

import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback
from stable_baselines3.common.logger import configure

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ì‚¬ìš©ì ì •ì˜ í™˜ê²½ ë“±ë¡
import panda_mujoco_gym

print("ğŸ¤– Stable-Baselines3 SAC ê¸°ë°˜ Panda ë¡œë´‡ í•™ìŠµ!")
print("=" * 60)

# ì„¤ì • í´ë˜ìŠ¤
class Config:
    # í™˜ê²½ ì„¤ì •
    env_name = "FrankaSlideDense-v0"  # í•™ìŠµí•  í™˜ê²½
    total_timesteps = 200_000         # ì´ í•™ìŠµ ìŠ¤í…
    
    # SAC í•˜ì´í¼íŒŒë¼ë¯¸í„°
    learning_rate = 3e-4
    buffer_size = 1_000_000
    learning_starts = 1000
    batch_size = 256
    tau = 0.005
    gamma = 0.99
    train_freq = 1
    gradient_steps = 1
    
    # í‰ê°€ ì„¤ì •
    eval_freq = 10_000
    n_eval_episodes = 10
    eval_deterministic = True
    
    # ì €ì¥ ì„¤ì •
    save_freq = 50_000
    video_freq = 50_000
    video_length = 1000
    
    # ì‹œê°í™” ì„¤ì •
    enable_realtime_viz = True      # ì‹¤ì‹œê°„ MuJoCo ì°½ í‘œì‹œ
    auto_close_window = True        # 30ì´ˆ í›„ ìë™ ë‹«ê¸°
    viz_duration = 30               # ì‹œê°í™” ì§€ì† ì‹œê°„ (ì´ˆ)
    base_dir = "data"
    model_dir = os.path.join(base_dir, "models")
    results_dir = os.path.join(base_dir, "training_results")
    video_dir = os.path.join(base_dir, "videos")
    log_dir = os.path.join(base_dir, "logs")
    
    # ì‹¤í—˜ ì´ë¦„ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    experiment_name = f"{env_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

config = Config()

# ë””ë ‰í† ë¦¬ ìƒì„±
def create_directories():
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    dirs_to_create = [
        config.base_dir,
        config.model_dir,
        config.results_dir,
        config.video_dir,
        config.log_dir
    ]
    
    for dir_path in dirs_to_create:
        os.makedirs(dir_path, exist_ok=True)
        print(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {dir_path}")

# ì»¤ìŠ¤í…€ ì½œë°± í´ë˜ìŠ¤
class TrainingCallback(BaseCallback):
    """
    í•™ìŠµ ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ë¡œê·¸ë¥¼ ì €ì¥í•˜ëŠ” ì½œë°±
    """
    def __init__(self, log_dir, verbose=0):
        super(TrainingCallback, self).__init__(verbose)
        self.log_dir = log_dir
        self.episode_rewards = []
        self.episode_lengths = []
        self.csv_file = os.path.join(log_dir, f"training_log_{config.experiment_name}.csv")
        
        # 6ë‹¨ê³„ ì‹œê°í™”ë¥¼ ìœ„í•œ ì„¤ì •
        self.total_timesteps = config.total_timesteps
        self.visualization_steps = [
            0,  # í•™ìŠµ ì „ (ë¬´ì‘ìœ„)
            self.total_timesteps // 5,      # 1/5 ì§„í–‰
            self.total_timesteps * 2 // 5,  # 2/5 ì§„í–‰  
            self.total_timesteps * 3 // 5,  # 3/5 ì§„í–‰
            self.total_timesteps * 4 // 5,  # 4/5 ì§„í–‰
            self.total_timesteps             # í•™ìŠµ ì™„ë£Œ
        ]
        self.completed_visualizations = set()
        
        # CSV íŒŒì¼ ì´ˆê¸°í™”
        with open(self.csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Timestep', 'Episode', 'Reward', 'Length', 'FPS'])
    
    def _on_step(self) -> bool:
        # 6ë‹¨ê³„ ì‹œê°í™” ì²´í¬
        for i, target_step in enumerate(self.visualization_steps):
            if (target_step not in self.completed_visualizations and 
                self.num_timesteps >= target_step):
                
                self.completed_visualizations.add(target_step)
                stage_name = [
                    "0_í•™ìŠµì „_ë¬´ì‘ìœ„",
                    "1_í•™ìŠµì§„í–‰_20í¼ì„¼íŠ¸", 
                    "2_í•™ìŠµì§„í–‰_40í¼ì„¼íŠ¸",
                    "3_í•™ìŠµì§„í–‰_60í¼ì„¼íŠ¸", 
                    "4_í•™ìŠµì§„í–‰_80í¼ì„¼íŠ¸",
                    "5_í•™ìŠµì™„ë£Œ_100í¼ì„¼íŠ¸"
                ][i]
                
                print(f"\nğŸ¬ [{stage_name}] ì‹¤ì‹œê°„ ì‹œê°í™” ì‹œì‘! (Step {self.num_timesteps})")
                self.visualize_current_performance(stage_name)
        
        # ì—í”¼ì†Œë“œê°€ ëë‚¬ì„ ë•Œ
        if self.locals.get('dones', [False])[0]:
            info = self.locals.get('infos', [{}])[0]
            
            if 'episode' in info:
                episode_reward = info['episode']['r']
                episode_length = info['episode']['l']
                
                self.episode_rewards.append(episode_reward)
                self.episode_lengths.append(episode_length)
                
                # ì½˜ì†” ì¶œë ¥
                if len(self.episode_rewards) % 10 == 0:
                    avg_reward = np.mean(self.episode_rewards[-10:])
                    print(f"ğŸ“Š Episode {len(self.episode_rewards):4d} | "
                          f"Reward: {episode_reward:7.2f} | "
                          f"Avg Reward: {avg_reward:7.2f} | "
                          f"Length: {episode_length:3d}")
                
                # CSV ë¡œê·¸ ì €ì¥
                with open(self.csv_file, 'a', newline='') as f:
                    writer = csv.writer(f)
                    fps = self.locals.get('fps', 0)
                    writer.writerow([
                        self.num_timesteps,
                        len(self.episode_rewards),
                        episode_reward,
                        episode_length,
                        fps
                    ])
        
        return True
    
    def visualize_current_performance(self, stage_name):
        """í˜„ì¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹¤ì‹œê°„ ì‹œê°í™”í•©ë‹ˆë‹¤."""
        if not config.enable_realtime_viz:
            print(f"ğŸ¥ [{stage_name}] ë¹„ë””ì˜¤ë§Œ ë…¹í™” ì¤‘...")
            self.record_stage_video(stage_name)
            return
            
        try:
            if config.auto_close_window:
                print(f"ğŸ® MuJoCo í™˜ê²½ì—ì„œ ì‹¤ì‹œê°„ ì‹œì—° ì¤‘... ({config.viz_duration}ì´ˆ í›„ ìë™ ì§„í–‰)")
            else:
                print(f"ğŸ® MuJoCo í™˜ê²½ì—ì„œ ì‹¤ì‹œê°„ ì‹œì—° ì¤‘... (ì°½ì„ ë‹«ìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰)")
            
            # ì‹¤ì‹œê°„ ì‹œê°í™”ìš© í™˜ê²½ ìƒì„±
            vis_env = gym.make(config.env_name, render_mode="human")
            
            obs, _ = vis_env.reset()
            
            total_reward = 0
            steps = 0
            max_steps = 150
            
            start_time = time.time()
            
            for step in range(max_steps):
                # í˜„ì¬ ëª¨ë¸ë¡œ í–‰ë™ ì˜ˆì¸¡
                if hasattr(self.model, 'predict'):
                    action, _ = self.model.predict(obs, deterministic=True)
                else:
                    # í•™ìŠµ ì „ì´ë©´ ë¬´ì‘ìœ„ í–‰ë™
                    action = vis_env.action_space.sample()
                
                obs, reward, terminated, truncated, info = vis_env.step(action)
                total_reward += reward
                steps += 1
                
                # ì„±ê³µ ì—¬ë¶€ í‘œì‹œ
                if info.get('is_success', False):
                    print(f"ğŸ‰ ì„±ê³µ! (Step {steps}, Reward: {total_reward:.2f})")
                
                if terminated or truncated:
                    obs, _ = vis_env.reset()
                    if steps > 10:  # ë„ˆë¬´ ë¹¨ë¦¬ ëë‚˜ì§€ ì•Šì•˜ë‹¤ë©´ í†µê³„ ì¶œë ¥
                        print(f"ğŸ“Š ì—í”¼ì†Œë“œ ì™„ë£Œ: Steps={steps}, Total Reward={total_reward:.2f}")
                    total_reward = 0
                    steps = 0
                
                # ì ì ˆí•œ ì†ë„ë¡œ ë Œë”ë§
                time.sleep(0.02)  # 50 FPS
                
                # ìë™ ë‹«ê¸° ì˜µì…˜ í™•ì¸
                if config.auto_close_window and time.time() - start_time > config.viz_duration:
                    print(f"â° {config.viz_duration}ì´ˆ ê²½ê³¼ - ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰")
                    break
            
            vis_env.close()
            print(f"âœ… [{stage_name}] ì‹œê°í™” ì™„ë£Œ!")
            
            # ë¹„ë””ì˜¤ë„ í•¨ê»˜ ì €ì¥
            self.record_stage_video(stage_name)
            
        except Exception as e:
            print(f"âš ï¸ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ğŸ’¡ X11 ë””ìŠ¤í”Œë ˆì´ê°€ ì—†ê±°ë‚˜ GUI í™˜ê²½ì´ ì•„ë‹Œ ê²½ìš° ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("ğŸ¥ ë¹„ë””ì˜¤ë§Œ ë…¹í™”í•©ë‹ˆë‹¤...")
            self.record_stage_video(stage_name)
    
    def record_stage_video(self, stage_name):
        """ë‹¨ê³„ë³„ ë¹„ë””ì˜¤ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            print(f"ğŸ¥ [{stage_name}] ë¹„ë””ì˜¤ ë…¹í™” ì¤‘...")
            
            # ë¹„ë””ì˜¤ ì €ì¥ìš© í™˜ê²½
            video_env = create_env(config.env_name, render_mode="rgb_array")
            video_env = DummyVecEnv([lambda: video_env])
            
            # ë‹¨ê³„ë³„ ë¹„ë””ì˜¤ ê²½ë¡œ
            stage_video_dir = os.path.join(config.video_dir, "training_stages")
            os.makedirs(stage_video_dir, exist_ok=True)
            
            video_path = os.path.join(stage_video_dir, f"{stage_name}_step_{self.num_timesteps}")
            video_env = VecVideoRecorder(
                video_env,
                video_path,
                record_video_trigger=lambda x: x == 0,
                video_length=200,  # ì§§ì€ ë¹„ë””ì˜¤
                name_prefix=stage_name
            )
            
            # ë¹„ë””ì˜¤ ë…¹í™”
            obs = video_env.reset()
            for i in range(200):
                if hasattr(self.model, 'predict'):
                    action, _ = self.model.predict(obs, deterministic=True)
                else:
                    action = [video_env.action_space.sample()]
                
                obs, _, dones, _ = video_env.step(action)
                if dones[0]:
                    obs = video_env.reset()
            
            video_env.close()
            print(f"ğŸ’¾ [{stage_name}] ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {video_path}")
            
        except Exception as e:
            print(f"âš ï¸ ë¹„ë””ì˜¤ ë…¹í™” ì¤‘ ì˜¤ë¥˜: {e}")

def create_env(env_name, render_mode=None):
    """í™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    env = gym.make(env_name, render_mode=render_mode)
    env = Monitor(env)
    return env

def train_model():
    """SAC ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤."""
    print(f"ğŸ¯ í™˜ê²½: {config.env_name}")
    print(f"ğŸ“Š ì´ í•™ìŠµ ìŠ¤í…: {config.total_timesteps:,}")
    print(f"ğŸ§  ì•Œê³ ë¦¬ì¦˜: SAC (Stable-Baselines3)")
    print(f"ğŸ¬ ì‹¤ì‹œê°„ ì‹œê°í™”: 6ë‹¨ê³„ (0%, 20%, 40%, 60%, 80%, 100%)")
    print("-" * 60)
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    create_directories()
    
    # í™˜ê²½ ìƒì„±
    print("ğŸ—ï¸  í™˜ê²½ ìƒì„± ì¤‘...")
    env = create_env(config.env_name)
    eval_env = create_env(config.env_name)
    
    # í™˜ê²½ ì •ë³´ ì¶œë ¥
    print(f"ğŸ“ ê´€ì°° ê³µê°„: {env.observation_space}")
    print(f"ğŸ® í–‰ë™ ê³µê°„: {env.action_space}")
    print(f"ğŸ¯ í–‰ë™ ë²”ìœ„: [{env.action_space.low[0]:.2f}, {env.action_space.high[0]:.2f}]")
    
    # ë¡œê±° ì„¤ì •
    logger_path = os.path.join(config.log_dir, config.experiment_name)
    new_logger = configure(logger_path, ["stdout", "csv", "tensorboard"])
    
    # SAC ëª¨ë¸ ìƒì„±
    print("\nğŸ§  SAC ëª¨ë¸ ì´ˆê¸°í™”...")
    model = SAC(
        policy="MultiInputPolicy",  # Dict ê´€ì°° ê³µê°„ìš© ì •ì±… âœ…
        env=env,
        learning_rate=config.learning_rate,
        buffer_size=config.buffer_size,
        learning_starts=config.learning_starts,
        batch_size=config.batch_size,
        tau=config.tau,
        gamma=config.gamma,
        train_freq=config.train_freq,
        gradient_steps=config.gradient_steps,
        verbose=1,
        tensorboard_log=config.log_dir,
        device="auto"  # GPU ìë™ ê°ì§€
    )
    
    # ë¡œê±° ì„¤ì •
    model.set_logger(new_logger)
    
    # ì½œë°± ì„¤ì •
    callbacks = []
    
    # 1. í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì½œë°± (ì‹œê°í™” ê¸°ëŠ¥ í¬í•¨)
    training_callback = TrainingCallback(config.log_dir)
    training_callback.model = model  # ëª¨ë¸ ì°¸ì¡° ì¶”ê°€
    callbacks.append(training_callback)
    
    # í•™ìŠµ ì „ ë¬´ì‘ìœ„ í–‰ë™ ì‹œê°í™”
    print("\nğŸ¬ [í•™ìŠµ ì „] ë¬´ì‘ìœ„ í–‰ë™ ì‹œì—°ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    training_callback.visualize_current_performance("0_í•™ìŠµì „_ë¬´ì‘ìœ„")
    
    # 2. í‰ê°€ ì½œë°±
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=os.path.join(config.model_dir, f"best_model_{config.experiment_name}"),
        log_path=os.path.join(config.log_dir, "eval"),
        eval_freq=config.eval_freq,
        n_eval_episodes=config.n_eval_episodes,
        deterministic=config.eval_deterministic,
        render=False,
        verbose=1
    )
    callbacks.append(eval_callback)
    
    # 3. ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_callback = CheckpointCallback(
        save_freq=config.save_freq,
        save_path=os.path.join(config.model_dir, "checkpoints"),
        name_prefix=f"sac_{config.experiment_name}"
    )
    callbacks.append(checkpoint_callback)
    
    # í•™ìŠµ ì‹œì‘
    print("\nğŸš€ í•™ìŠµ ì‹œì‘!")
    print("=" * 60)
    print("ğŸ’¡ í•™ìŠµ ì¤‘ 20%, 40%, 60%, 80%, 100% ì§€ì ì—ì„œ ìë™ìœ¼ë¡œ ì‹¤ì‹œê°„ ì‹œê°í™”ê°€ ì§„í–‰ë©ë‹ˆë‹¤!")
    print("ğŸ’¡ MuJoCo ì°½ì´ ë‚˜íƒ€ë‚˜ë©´ ë¡œë´‡ì˜ ì„±ëŠ¥ ë³€í™”ë¥¼ ê´€ì°°í•´ë³´ì„¸ìš”!")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=config.total_timesteps,
            callback=callbacks,
            log_interval=10,
            progress_bar=True
        )
    except KeyboardInterrupt:
        print("\nâ¸ï¸  í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    end_time = time.time()
    training_time = end_time - start_time
    
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"â±ï¸  ì´ í•™ìŠµ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = os.path.join(config.model_dir, f"sac_{config.experiment_name}_final")
    model.save(final_model_path)
    print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
    
    # ìµœì¢… í‰ê°€
    print("\nğŸ” ìµœì¢… í‰ê°€ ì§„í–‰...")
    mean_reward, std_reward = evaluate_policy(
        model, eval_env, 
        n_eval_episodes=20, 
        deterministic=True
    )
    print(f"ğŸ† ìµœì¢… í‰ê°€ ê²°ê³¼: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    return model, training_callback

def record_videos(model):
    """í•™ìŠµëœ ëª¨ë¸ì˜ ë™ì˜ìƒì„ ë…¹í™”í•©ë‹ˆë‹¤."""
    print("\nğŸ¥ ë™ì˜ìƒ ë…¹í™” ì‹œì‘...")
    
    # ë¹„ë””ì˜¤ ë…¹í™”ìš© í™˜ê²½ ì„¤ì •
    video_env = create_env(config.env_name, render_mode="rgb_array")
    video_env = DummyVecEnv([lambda: video_env])
    
    video_path = os.path.join(config.video_dir, f"final_performance_{config.experiment_name}")
    video_env = VecVideoRecorder(
        video_env,
        video_path,
        record_video_trigger=lambda x: x == 0,
        video_length=config.video_length,
        name_prefix=f"sac_{config.experiment_name}"
    )
    
    # ë™ì˜ìƒ ë…¹í™”
    obs = video_env.reset()
    for i in range(config.video_length):
        action, _ = model.predict(obs, deterministic=True)
        obs, _, dones, _ = video_env.step(action)
        if dones[0]:
            obs = video_env.reset()
    
    video_env.close()
    print(f"ğŸ¥ ë™ì˜ìƒ ì €ì¥ ì™„ë£Œ: {video_path}")

def save_training_stats(training_callback):
    """í•™ìŠµ í†µê³„ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    print("\nğŸ“Š í•™ìŠµ í†µê³„ ì €ì¥...")
    
    stats = {
        'experiment_name': config.experiment_name,
        'env_name': config.env_name,
        'total_timesteps': config.total_timesteps,
        'total_episodes': len(training_callback.episode_rewards),
        'final_avg_reward': np.mean(training_callback.episode_rewards[-100:]) if len(training_callback.episode_rewards) >= 100 else np.mean(training_callback.episode_rewards),
        'best_reward': np.max(training_callback.episode_rewards) if training_callback.episode_rewards else 0,
        'config': {
            'learning_rate': config.learning_rate,
            'buffer_size': config.buffer_size,
            'batch_size': config.batch_size,
            'tau': config.tau,
            'gamma': config.gamma
        },
        'episode_rewards': training_callback.episode_rewards,
        'episode_lengths': training_callback.episode_lengths
    }
    
    # JSONìœ¼ë¡œ ì €ì¥
    stats_path = os.path.join(config.results_dir, f"training_stats_{config.experiment_name}.json")
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"ğŸ“‹ í†µê³„ ì €ì¥ ì™„ë£Œ: {stats_path}")
    return stats

def plot_training_results(stats):
    """í•™ìŠµ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    print("\nğŸ“ˆ í•™ìŠµ ê²°ê³¼ ì‹œê°í™”...")
    
    episode_rewards = stats['episode_rewards']
    episode_lengths = stats['episode_lengths']
    
    if not episode_rewards:
        print("âš ï¸  ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle(f'Training Results - {config.experiment_name}', fontsize=16)
    
    # 1. ì—í”¼ì†Œë“œë³„ ë³´ìƒ
    axes[0, 0].plot(episode_rewards, alpha=0.7)
    axes[0, 0].set_title('Episode Rewards')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Reward')
    axes[0, 0].grid(True)
    
    # 2. ì´ë™ í‰ê·  ë³´ìƒ
    if len(episode_rewards) > 50:
        window = min(50, len(episode_rewards) // 4)
        moving_avg = []
        for i in range(window, len(episode_rewards)):
            moving_avg.append(np.mean(episode_rewards[i-window:i]))
        
        axes[0, 1].plot(moving_avg)
        axes[0, 1].set_title(f'Moving Average Rewards (window={window})')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Average Reward')
        axes[0, 1].grid(True)
    
    # 3. ì—í”¼ì†Œë“œ ê¸¸ì´
    axes[0, 2].plot(episode_lengths, color='orange', alpha=0.7)
    axes[0, 2].set_title('Episode Lengths')
    axes[0, 2].set_xlabel('Episode')
    axes[0, 2].set_ylabel('Steps')
    axes[0, 2].grid(True)
    
    # 4. ë³´ìƒ ë¶„í¬
    axes[1, 0].hist(episode_rewards, bins=50, alpha=0.7, color='green')
    axes[1, 0].set_title('Reward Distribution')
    axes[1, 0].set_xlabel('Reward')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].grid(True)
    
    # 5. ê¸¸ì´ ë¶„í¬
    axes[1, 1].hist(episode_lengths, bins=50, alpha=0.7, color='red')
    axes[1, 1].set_title('Episode Length Distribution')
    axes[1, 1].set_xlabel('Steps')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].grid(True)
    
    # 6. í•™ìŠµ ì§„í–‰ë¥ 
    if len(episode_rewards) > 100:
        progress_rewards = []
        chunk_size = len(episode_rewards) // 10
        for i in range(0, len(episode_rewards), chunk_size):
            chunk = episode_rewards[i:i+chunk_size]
            if chunk:
                progress_rewards.append(np.mean(chunk))
        
        axes[1, 2].plot(progress_rewards, marker='o')
        axes[1, 2].set_title('Learning Progress (10 chunks)')
        axes[1, 2].set_xlabel('Training Phase')
        axes[1, 2].set_ylabel('Average Reward')
        axes[1, 2].grid(True)
    
    plt.tight_layout()
    
    # ê·¸ë˜í”„ ì €ì¥
    plot_path = os.path.join(config.results_dir, f'training_results_{config.experiment_name}.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“ˆ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {plot_path}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Stable-Baselines3 SAC í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
    
    try:
        # 1. ëª¨ë¸ í•™ìŠµ
        model, training_callback = train_model()
        
        # 2. ë™ì˜ìƒ ë…¹í™”
        record_videos(model)
        
        # 3. í†µê³„ ì €ì¥
        stats = save_training_stats(training_callback)
        
        # 4. ê²°ê³¼ ì‹œê°í™”
        plot_training_results(stats)
        
        # 5. ê²°ê³¼ ìš”ì•½
        print("\n" + "="*60)
        print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("="*60)
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:")
        print(f"   ğŸ§  ëª¨ë¸: {config.model_dir}/")
        print(f"   ğŸ“Š ê²°ê³¼: {config.results_dir}/")
        print(f"   ğŸ¥ ë™ì˜ìƒ: {config.video_dir}/")
        print(f"   ğŸ¬ ë‹¨ê³„ë³„ ì˜ìƒ: {config.video_dir}/training_stages/")
        print(f"   ğŸ“‹ ë¡œê·¸: {config.log_dir}/")
        print(f"   ğŸ“ˆ TensorBoard: tensorboard --logdir {config.log_dir}")
        
        if training_callback.episode_rewards:
            print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
            print(f"   ğŸ¯ ì´ ì—í”¼ì†Œë“œ: {len(training_callback.episode_rewards)}")
            print(f"   ğŸ† ìµœê³  ë³´ìƒ: {np.max(training_callback.episode_rewards):.2f}")
            print(f"   ğŸ“ˆ ìµœì¢… í‰ê· : {np.mean(training_callback.episode_rewards[-100:]):.2f}")
            
        print(f"\nğŸ¬ ìƒì„±ëœ ë‹¨ê³„ë³„ ì‹œê°í™”:")
        stage_video_dir = os.path.join(config.video_dir, "training_stages")
        if os.path.exists(stage_video_dir):
            stage_files = os.listdir(stage_video_dir)
            for i, stage in enumerate(["í•™ìŠµì „", "20%", "40%", "60%", "80%", "100%"]):
                stage_videos = [f for f in stage_files if f.startswith(f"{i}_")]
                if stage_videos:
                    print(f"   ğŸ“¹ {stage}: âœ…")
                else:
                    print(f"   ğŸ“¹ {stage}: âŒ")
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()