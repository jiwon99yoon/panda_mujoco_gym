#!/usr/bin/env python3
"""
Stable-Baselines3 SAC ê¸°ë°˜ Panda ë¡œë´‡ í•™ìŠµ ì½”ë“œ
"""

import os
import sys
import time
import numpy as np
import matplotlib.pyplot as plt
import json
import csv
from datetime import datetime

import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback
from stable_baselines3.common.logger import configure

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ì‚¬ìš©ì ì •ì˜ í™˜ê²½ ë“±ë¡
import panda_mujoco_gym

print("ğŸ¤– Stable-Baselines3 SAC ê¸°ë°˜ Panda ë¡œë´‡ í•™ìŠµ!")
print("=" * 60)

# ì„¤ì • í´ë˜ìŠ¤
class Config:
    # í™˜ê²½ ì„¤ì •
    env_name = "FrankaSlideDense-v0"  # í•™ìŠµí•  í™˜ê²½
    total_timesteps = 200_000         # ì´ í•™ìŠµ ìŠ¤í…
    
    # SAC í•˜ì´í¼íŒŒë¼ë¯¸í„°
    learning_rate = 3e-4
    buffer_size = 1_000_000
    learning_starts = 1000
    batch_size = 256
    tau = 0.005
    gamma = 0.99
    train_freq = 1
    gradient_steps = 1
    
    # í‰ê°€ ì„¤ì •
    eval_freq = 10_000
    n_eval_episodes = 10
    eval_deterministic = True
    
    # ì €ì¥ ì„¤ì •
    save_freq = 50_000
    video_freq = 50_000
    video_length = 1000
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    base_dir = "data"
    model_dir = os.path.join(base_dir, "models")
    results_dir = os.path.join(base_dir, "training_results")
    video_dir = os.path.join(base_dir, "videos")
    log_dir = os.path.join(base_dir, "logs")
    
    # ì‹¤í—˜ ì´ë¦„ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    experiment_name = f"{env_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

config = Config()

# ë””ë ‰í† ë¦¬ ìƒì„±
def create_directories():
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    dirs_to_create = [
        config.base_dir,
        config.model_dir,
        config.results_dir,
        config.video_dir,
        config.log_dir
    ]
    
    for dir_path in dirs_to_create:
        os.makedirs(dir_path, exist_ok=True)
        print(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {dir_path}")

# ì»¤ìŠ¤í…€ ì½œë°± í´ë˜ìŠ¤
class TrainingCallback(BaseCallback):
    """
    í•™ìŠµ ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ë¡œê·¸ë¥¼ ì €ì¥í•˜ëŠ” ì½œë°±
    """
    def __init__(self, log_dir, verbose=0):
        super(TrainingCallback, self).__init__(verbose)
        self.log_dir = log_dir
        self.episode_rewards = []
        self.episode_lengths = []
        self.csv_file = os.path.join(log_dir, f"training_log_{config.experiment_name}.csv")
        
        # CSV íŒŒì¼ ì´ˆê¸°í™”
        with open(self.csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Timestep', 'Episode', 'Reward', 'Length', 'FPS'])
    
    def _on_step(self) -> bool:
        # ì—í”¼ì†Œë“œê°€ ëë‚¬ì„ ë•Œ
        if self.locals.get('dones', [False])[0]:
            info = self.locals.get('infos', [{}])[0]
            
            if 'episode' in info:
                episode_reward = info['episode']['r']
                episode_length = info['episode']['l']
                
                self.episode_rewards.append(episode_reward)
                self.episode_lengths.append(episode_length)
                
                # ì½˜ì†” ì¶œë ¥
                if len(self.episode_rewards) % 10 == 0:
                    avg_reward = np.mean(self.episode_rewards[-10:])
                    print(f"ğŸ“Š Episode {len(self.episode_rewards):4d} | "
                          f"Reward: {episode_reward:7.2f} | "
                          f"Avg Reward: {avg_reward:7.2f} | "
                          f"Length: {episode_length:3d}")
                
                # CSV ë¡œê·¸ ì €ì¥
                with open(self.csv_file, 'a', newline='') as f:
                    writer = csv.writer(f)
                    fps = self.locals.get('fps', 0)
                    writer.writerow([
                        self.num_timesteps,
                        len(self.episode_rewards),
                        episode_reward,
                        episode_length,
                        fps
                    ])
        
        return True

def create_env(env_name, render_mode=None):
    """í™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    env = gym.make(env_name, render_mode=render_mode)
    env = Monitor(env)
    return env

def train_model():
    """SAC ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤."""
    print(f"ğŸ¯ í™˜ê²½: {config.env_name}")
    print(f"ğŸ“Š ì´ í•™ìŠµ ìŠ¤í…: {config.total_timesteps:,}")
    print(f"ğŸ§  ì•Œê³ ë¦¬ì¦˜: SAC (Stable-Baselines3)")
    print("-" * 60)
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    create_directories()
    
    # í™˜ê²½ ìƒì„±
    print("ğŸ—ï¸  í™˜ê²½ ìƒì„± ì¤‘...")
    env = create_env(config.env_name)
    eval_env = create_env(config.env_name)
    
    # í™˜ê²½ ì •ë³´ ì¶œë ¥
    print(f"ğŸ“ ê´€ì°° ê³µê°„: {env.observation_space}")
    print(f"ğŸ® í–‰ë™ ê³µê°„: {env.action_space}")
    print(f"ğŸ¯ í–‰ë™ ë²”ìœ„: [{env.action_space.low[0]:.2f}, {env.action_space.high[0]:.2f}]")
    
    # ë¡œê±° ì„¤ì •
    logger_path = os.path.join(config.log_dir, config.experiment_name)
    new_logger = configure(logger_path, ["stdout", "csv", "tensorboard"])
    
    # SAC ëª¨ë¸ ìƒì„±
    print("\nğŸ§  SAC ëª¨ë¸ ì´ˆê¸°í™”...")
    model = SAC(
        policy="MlpPolicy",
        env=env,
        learning_rate=config.learning_rate,
        buffer_size=config.buffer_size,
        learning_starts=config.learning_starts,
        batch_size=config.batch_size,
        tau=config.tau,
        gamma=config.gamma,
        train_freq=config.train_freq,
        gradient_steps=config.gradient_steps,
        verbose=1,
        tensorboard_log=config.log_dir,
        device="auto"  # GPU ìë™ ê°ì§€
    )
    
    # ë¡œê±° ì„¤ì •
    model.set_logger(new_logger)
    
    # ì½œë°± ì„¤ì •
    callbacks = []
    
    # 1. í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì½œë°±
    training_callback = TrainingCallback(config.log_dir)
    callbacks.append(training_callback)
    
    # 2. í‰ê°€ ì½œë°±
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=os.path.join(config.model_dir, f"best_model_{config.experiment_name}"),
        log_path=os.path.join(config.log_dir, "eval"),
        eval_freq=config.eval_freq,
        n_eval_episodes=config.n_eval_episodes,
        deterministic=config.eval_deterministic,
        render=False,
        verbose=1
    )
    callbacks.append(eval_callback)
    
    # 3. ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_callback = CheckpointCallback(
        save_freq=config.save_freq,
        save_path=os.path.join(config.model_dir, "checkpoints"),
        name_prefix=f"sac_{config.experiment_name}"
    )
    callbacks.append(checkpoint_callback)
    
    # í•™ìŠµ ì‹œì‘
    print("\nğŸš€ í•™ìŠµ ì‹œì‘!")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=config.total_timesteps,
            callback=callbacks,
            log_interval=10,
            progress_bar=True
        )
    except KeyboardInterrupt:
        print("\nâ¸ï¸  í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    end_time = time.time()
    training_time = end_time - start_time
    
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"â±ï¸  ì´ í•™ìŠµ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = os.path.join(config.model_dir, f"sac_{config.experiment_name}_final")
    model.save(final_model_path)
    print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
    
    # ìµœì¢… í‰ê°€
    print("\nğŸ” ìµœì¢… í‰ê°€ ì§„í–‰...")
    mean_reward, std_reward = evaluate_policy(
        model, eval_env, 
        n_eval_episodes=20, 
        deterministic=True
    )
    print(f"ğŸ† ìµœì¢… í‰ê°€ ê²°ê³¼: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    return model, training_callback

def record_videos(model):
    """í•™ìŠµëœ ëª¨ë¸ì˜ ë™ì˜ìƒì„ ë…¹í™”í•©ë‹ˆë‹¤."""
    print("\nğŸ¥ ë™ì˜ìƒ ë…¹í™” ì‹œì‘...")
    
    # ë¹„ë””ì˜¤ ë…¹í™”ìš© í™˜ê²½ ì„¤ì •
    video_env = create_env(config.env_name, render_mode="rgb_array")
    video_env = DummyVecEnv([lambda: video_env])
    
    video_path = os.path.join(config.video_dir, f"final_performance_{config.experiment_name}")
    video_env = VecVideoRecorder(
        video_env,
        video_path,
        record_video_trigger=lambda x: x == 0,
        video_length=config.video_length,
        name_prefix=f"sac_{config.experiment_name}"
    )
    
    # ë™ì˜ìƒ ë…¹í™”
    obs = video_env.reset()
    for i in range(config.video_length):
        action, _ = model.predict(obs, deterministic=True)
        obs, _, dones, _ = video_env.step(action)
        if dones[0]:
            obs = video_env.reset()
    
    video_env.close()
    print(f"ğŸ¥ ë™ì˜ìƒ ì €ì¥ ì™„ë£Œ: {video_path}")

def save_training_stats(training_callback):
    """í•™ìŠµ í†µê³„ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    print("\nğŸ“Š í•™ìŠµ í†µê³„ ì €ì¥...")
    
    stats = {
        'experiment_name': config.experiment_name,
        'env_name': config.env_name,
        'total_timesteps': config.total_timesteps,
        'total_episodes': len(training_callback.episode_rewards),
        'final_avg_reward': np.mean(training_callback.episode_rewards[-100:]) if len(training_callback.episode_rewards) >= 100 else np.mean(training_callback.episode_rewards),
        'best_reward': np.max(training_callback.episode_rewards) if training_callback.episode_rewards else 0,
        'config': {
            'learning_rate': config.learning_rate,
            'buffer_size': config.buffer_size,
            'batch_size': config.batch_size,
            'tau': config.tau,
            'gamma': config.gamma
        },
        'episode_rewards': training_callback.episode_rewards,
        'episode_lengths': training_callback.episode_lengths
    }
    
    # JSONìœ¼ë¡œ ì €ì¥
    stats_path = os.path.join(config.results_dir, f"training_stats_{config.experiment_name}.json")
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"ğŸ“‹ í†µê³„ ì €ì¥ ì™„ë£Œ: {stats_path}")
    return stats

def plot_training_results(stats):
    """í•™ìŠµ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    print("\nğŸ“ˆ í•™ìŠµ ê²°ê³¼ ì‹œê°í™”...")
    
    episode_rewards = stats['episode_rewards']
    episode_lengths = stats['episode_lengths']
    
    if not episode_rewards:
        print("âš ï¸  ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle(f'Training Results - {config.experiment_name}', fontsize=16)
    
    # 1. ì—í”¼ì†Œë“œë³„ ë³´ìƒ
    axes[0, 0].plot(episode_rewards, alpha=0.7)
    axes[0, 0].set_title('Episode Rewards')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Reward')
    axes[0, 0].grid(True)
    
    # 2. ì´ë™ í‰ê·  ë³´ìƒ
    if len(episode_rewards) > 50:
        window = min(50, len(episode_rewards) // 4)
        moving_avg = []
        for i in range(window, len(episode_rewards)):
            moving_avg.append(np.mean(episode_rewards[i-window:i]))
        
        axes[0, 1].plot(moving_avg)
        axes[0, 1].set_title(f'Moving Average Rewards (window={window})')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Average Reward')
        axes[0, 1].grid(True)
    
    # 3. ì—í”¼ì†Œë“œ ê¸¸ì´
    axes[0, 2].plot(episode_lengths, color='orange', alpha=0.7)
    axes[0, 2].set_title('Episode Lengths')
    axes[0, 2].set_xlabel('Episode')
    axes[0, 2].set_ylabel('Steps')
    axes[0, 2].grid(True)
    
    # 4. ë³´ìƒ ë¶„í¬
    axes[1, 0].hist(episode_rewards, bins=50, alpha=0.7, color='green')
    axes[1, 0].set_title('Reward Distribution')
    axes[1, 0].set_xlabel('Reward')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].grid(True)
    
    # 5. ê¸¸ì´ ë¶„í¬
    axes[1, 1].hist(episode_lengths, bins=50, alpha=0.7, color='red')
    axes[1, 1].set_title('Episode Length Distribution')
    axes[1, 1].set_xlabel('Steps')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].grid(True)
    
    # 6. í•™ìŠµ ì§„í–‰ë¥ 
    if len(episode_rewards) > 100:
        progress_rewards = []
        chunk_size = len(episode_rewards) // 10
        for i in range(0, len(episode_rewards), chunk_size):
            chunk = episode_rewards[i:i+chunk_size]
            if chunk:
                progress_rewards.append(np.mean(chunk))
        
        axes[1, 2].plot(progress_rewards, marker='o')
        axes[1, 2].set_title('Learning Progress (10 chunks)')
        axes[1, 2].set_xlabel('Training Phase')
        axes[1, 2].set_ylabel('Average Reward')
        axes[1, 2].grid(True)
    
    plt.tight_layout()
    
    # ê·¸ë˜í”„ ì €ì¥
    plot_path = os.path.join(config.results_dir, f'training_results_{config.experiment_name}.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“ˆ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {plot_path}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Stable-Baselines3 SAC í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
    
    try:
        # 1. ëª¨ë¸ í•™ìŠµ
        model, training_callback = train_model()
        
        # 2. ë™ì˜ìƒ ë…¹í™”
        record_videos(model)
        
        # 3. í†µê³„ ì €ì¥
        stats = save_training_stats(training_callback)
        
        # 4. ê²°ê³¼ ì‹œê°í™”
        plot_training_results(stats)
        
        # 5. ê²°ê³¼ ìš”ì•½
        print("\n" + "="*60)
        print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("="*60)
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:")
        print(f"   ğŸ§  ëª¨ë¸: {config.model_dir}/")
        print(f"   ğŸ“Š ê²°ê³¼: {config.results_dir}/")
        print(f"   ğŸ¥ ë™ì˜ìƒ: {config.video_dir}/")
        print(f"   ğŸ“‹ ë¡œê·¸: {config.log_dir}/")
        print(f"   ğŸ“ˆ TensorBoard: tensorboard --logdir {config.log_dir}")
        
        if training_callback.episode_rewards:
            print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
            print(f"   ğŸ¯ ì´ ì—í”¼ì†Œë“œ: {len(training_callback.episode_rewards)}")
            print(f"   ğŸ† ìµœê³  ë³´ìƒ: {np.max(training_callback.episode_rewards):.2f}")
            print(f"   ğŸ“ˆ ìµœì¢… í‰ê· : {np.mean(training_callback.episode_rewards[-100:]):.2f}")
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
